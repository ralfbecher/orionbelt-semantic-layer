{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#orionbelt-semantic-layer","title":"OrionBelt Semantic Layer","text":"<p>Compile YAML semantic models into analytical SQL across multiple database dialects.</p> <p>OrionBelt is a semantic layer engine that transforms declarative YAML model definitions into optimized SQL for Postgres, Snowflake, ClickHouse, Dremio, and Databricks. Query using business concepts \u2014 dimensions, measures, and metrics \u2014 instead of raw SQL.</p>"},{"location":"#why-orionbelt","title":"Why OrionBelt?","text":"<ul> <li>One model, many dialects \u2014 Define your semantic model once in YAML, compile to SQL for any supported warehouse</li> <li>Safe by construction \u2014 AST-based SQL generation prevents injection and ensures syntactic correctness</li> <li>Precise error reporting \u2014 Validation errors include line and column numbers from your YAML source</li> <li>Automatic join resolution \u2014 Declare relationships between data objects; OrionBelt finds optimal join paths using graph algorithms</li> <li>Multi-fact support \u2014 Composite Fact Layer (CFL) planning handles queries spanning multiple fact tables with UNION ALL and CTE-based aggregation</li> <li>Session management \u2014 TTL-scoped sessions isolate model state per client, enabling iterative development workflows</li> <li>AI-native \u2014 MCP server with 10 tools and 3 prompts lets AI assistants load, validate, and query models interactively</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"Feature Description 5 SQL Dialects Postgres, Snowflake, ClickHouse, Dremio, Databricks SQL OrionBelt ML (OBML) YAML-based data objects, dimensions, measures, metrics, joins Star Schema &amp; CFL Automatic fact selection and join path resolution Session Management TTL-scoped per-client sessions for REST API and MCP REST API FastAPI endpoints for session-based model management, validation, and compilation MCP Server 10 tools + 3 prompts for Claude Desktop, Cursor, and other MCP clients Gradio UI Interactive web interface for model editing, query testing, SQL compilation, ER diagrams, and OSI import/export Custom Extensions Vendor-specific metadata at all model levels (model, data object, column, dimension, measure, metric) Plugin Architecture Extensible dialect system with capability flags Source Tracking Error messages with YAML line/column positions"},{"location":"#quick-example","title":"Quick Example","text":"<p>Define a semantic model in YAML:</p> <pre><code># yaml-language-server: $schema=schema/obml-schema.json\nversion: 1.0\n\ndataObjects:\n  Customers:\n    code: CUSTOMERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Country:\n        code: COUNTRY\n        abstractType: string\n\n  Orders:\n    code: ORDERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Price:\n        code: PRICE\n        abstractType: float\n      Quantity:\n        code: QUANTITY\n        abstractType: int\n    joins:\n      - joinType: many-to-one\n        joinTo: Customers\n        columnsFrom:\n          - Customer ID\n        columnsTo:\n          - Customer ID\n\ndimensions:\n  Country:\n    dataObject: Customers\n    column: Country\n    resultType: string\n\nmeasures:\n  Revenue:\n    resultType: float\n    aggregation: sum\n    expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n</code></pre> <p>Compile a query to SQL:</p> <pre><code>result = pipeline.compile(query, model, \"postgres\")\n</code></pre> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nGROUP BY \"Customers\".\"COUNTRY\"\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Start with Installation and then follow the Quick Start tutorial.</p> <p>    Copyright 2025 RALFORION d.o.o. \u2014 Licensed under Apache 2.0 </p>"},{"location":"api/endpoints/","title":"API Endpoints","text":"<p>Complete reference for all OrionBelt REST API endpoints.</p>"},{"location":"api/endpoints/#health-check","title":"Health Check","text":""},{"location":"api/endpoints/#get-health","title":"<code>GET /health</code>","text":"<p>Returns the service status and version.</p> <p>Response:</p> <pre><code>{\n  \"status\": \"ok\",\n  \"version\": \"0.1.0\"\n}\n</code></pre>"},{"location":"api/endpoints/#sessions","title":"Sessions","text":""},{"location":"api/endpoints/#post-sessions","title":"<code>POST /sessions</code>","text":"<p>Create a new session. Each session has its own model store.</p> <p>Request (optional):</p> <pre><code>{\n  \"metadata\": {\n    \"user\": \"alice\",\n    \"purpose\": \"revenue analysis\"\n  }\n}\n</code></pre> <p>Response (201):</p> <pre><code>{\n  \"session_id\": \"a1b2c3d4e5f6\",\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"last_accessed_at\": \"2025-01-15T10:30:00Z\",\n  \"model_count\": 0,\n  \"metadata\": {\n    \"user\": \"alice\",\n    \"purpose\": \"revenue analysis\"\n  }\n}\n</code></pre>"},{"location":"api/endpoints/#get-sessions","title":"<code>GET /sessions</code>","text":"<p>List all active sessions.</p> <p>Response (200):</p> <pre><code>{\n  \"sessions\": [\n    {\n      \"session_id\": \"a1b2c3d4e5f6\",\n      \"created_at\": \"2025-01-15T10:30:00Z\",\n      \"last_accessed_at\": \"2025-01-15T10:35:00Z\",\n      \"model_count\": 2,\n      \"metadata\": {}\n    }\n  ]\n}\n</code></pre>"},{"location":"api/endpoints/#get-sessionssession_id","title":"<code>GET /sessions/{session_id}</code>","text":"<p>Get info for a specific session. Also refreshes the session's last-accessed time.</p> <p>Response (200): Same as single session in list response.</p> <p>Error (404): Session not found or expired.</p>"},{"location":"api/endpoints/#delete-sessionssession_id","title":"<code>DELETE /sessions/{session_id}</code>","text":"<p>Close a session and release its resources.</p> <p>Response (204): No content.</p> <p>Error (404): Session not found.</p>"},{"location":"api/endpoints/#session-models","title":"Session Models","text":""},{"location":"api/endpoints/#post-sessionssession_idmodels","title":"<code>POST /sessions/{session_id}/models</code>","text":"<p>Load an OBML semantic model into a session. The model is parsed, validated, and stored.</p> <p>Request:</p> <pre><code>{\n  \"model_yaml\": \"version: 1.0\\ndataObjects:\\n  Orders:\\n    code: ORDERS\\n    ...\"\n}\n</code></pre> <p>Response (201):</p> <pre><code>{\n  \"model_id\": \"abcd1234\",\n  \"data_objects\": 2,\n  \"dimensions\": 3,\n  \"measures\": 2,\n  \"metrics\": 1,\n  \"warnings\": []\n}\n</code></pre> <p>Error (422): Model has validation errors.</p> <p>Error (404): Session not found.</p>"},{"location":"api/endpoints/#get-sessionssession_idmodels","title":"<code>GET /sessions/{session_id}/models</code>","text":"<p>List all models loaded in a session.</p> <p>Response (200):</p> <pre><code>[\n  {\n    \"model_id\": \"abcd1234\",\n    \"data_objects\": 2,\n    \"dimensions\": 3,\n    \"measures\": 2,\n    \"metrics\": 1\n  }\n]\n</code></pre>"},{"location":"api/endpoints/#get-sessionssession_idmodelsmodel_id","title":"<code>GET /sessions/{session_id}/models/{model_id}</code>","text":"<p>Describe a model's contents \u2014 data objects (with fields and joins), dimensions, measures, and metrics.</p> <p>Response (200):</p> <pre><code>{\n  \"model_id\": \"abcd1234\",\n  \"data_objects\": [\n    {\n      \"label\": \"Orders\",\n      \"code\": \"WAREHOUSE.PUBLIC.ORDERS\",\n      \"columns\": [\"Order ID\", \"Price\", \"Quantity\"],\n      \"join_targets\": [\"Customers\"]\n    }\n  ],\n  \"dimensions\": [\n    {\n      \"name\": \"Country\",\n      \"result_type\": \"string\",\n      \"data_object\": \"Customers\",\n      \"column\": \"Country\",\n      \"time_grain\": null\n    }\n  ],\n  \"measures\": [...],\n  \"metrics\": [...]\n}\n</code></pre> <p>Error (404): Model or session not found.</p>"},{"location":"api/endpoints/#delete-sessionssession_idmodelsmodel_id","title":"<code>DELETE /sessions/{session_id}/models/{model_id}</code>","text":"<p>Remove a model from a session.</p> <p>Response (204): No content.</p> <p>Error (404): Model or session not found.</p>"},{"location":"api/endpoints/#session-validation","title":"Session Validation","text":""},{"location":"api/endpoints/#post-sessionssession_idvalidate","title":"<code>POST /sessions/{session_id}/validate</code>","text":"<p>Validate OBML YAML within a session context. Does not store the model.</p> <p>Request:</p> <pre><code>{\n  \"model_yaml\": \"version: 1.0\\ndataObjects:\\n  ...\"\n}\n</code></pre> <p>Response (200):</p> <pre><code>{\n  \"valid\": true,\n  \"errors\": [],\n  \"warnings\": []\n}\n</code></pre> <p>Validation failure:</p> <pre><code>{\n  \"valid\": false,\n  \"errors\": [\n    {\n      \"code\": \"UNKNOWN_DATA_OBJECT\",\n      \"message\": \"Data object 'Unknown' not found\",\n      \"path\": \"dimensions.Bad.dataObject\"\n    }\n  ],\n  \"warnings\": []\n}\n</code></pre>"},{"location":"api/endpoints/#session-query-compilation","title":"Session Query Compilation","text":""},{"location":"api/endpoints/#post-sessionssession_idquerysql","title":"<code>POST /sessions/{session_id}/query/sql</code>","text":"<p>Compile a semantic query against a model loaded in the session.</p> <p>Request:</p> <pre><code>{\n  \"model_id\": \"abcd1234\",\n  \"query\": {\n    \"select\": {\n      \"dimensions\": [\"Customer Country\"],\n      \"measures\": [\"Revenue\"]\n    },\n    \"where\": [\n      {\n        \"field\": \"Customer Segment\",\n        \"op\": \"in\",\n        \"value\": [\"SMB\", \"MidMarket\"]\n      }\n    ],\n    \"order_by\": [\n      { \"field\": \"Revenue\", \"direction\": \"desc\" }\n    ],\n    \"limit\": 1000\n  },\n  \"dialect\": \"postgres\"\n}\n</code></pre> <p>Response (200):</p> <pre><code>{\n  \"sql\": \"SELECT ...\",\n  \"dialect\": \"postgres\",\n  \"resolved\": {\n    \"fact_tables\": [\"Orders\"],\n    \"dimensions\": [\"Customer Country\"],\n    \"measures\": [\"Revenue\"]\n  },\n  \"warnings\": []\n}\n</code></pre> <p>Error responses:</p> Status Cause 400 Unsupported dialect 404 Model or session not found 422 Resolution error"},{"location":"api/endpoints/#dialects","title":"Dialects","text":""},{"location":"api/endpoints/#get-dialects","title":"<code>GET /dialects</code>","text":"<p>List all available SQL dialects and their capability flags.</p> <p>Response (200):</p> <pre><code>{\n  \"dialects\": [\n    {\n      \"name\": \"postgres\",\n      \"capabilities\": {\n        \"supports_cte\": true,\n        \"supports_qualify\": false,\n        \"supports_arrays\": true,\n        \"supports_window_filters\": false,\n        \"supports_ilike\": true,\n        \"supports_time_travel\": false,\n        \"supports_semi_structured\": false\n      }\n    },\n    {\n      \"name\": \"snowflake\",\n      \"capabilities\": { \"...\" : true }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/mcp/","title":"MCP Server","text":"<p>OrionBelt includes a Model Context Protocol (MCP) server that exposes the semantic layer as tools for AI assistants like Claude Desktop, Cursor, and other MCP-compatible clients.</p>"},{"location":"api/mcp/#starting-the-server","title":"Starting the Server","text":"stdio (default)HTTPSSE (legacy) <pre><code>uv run orionbelt-mcp\n</code></pre> <p>Used by Claude Desktop and Cursor. Single-user, default session created automatically.</p> <pre><code>MCP_TRANSPORT=\"http\" uv run orionbelt-mcp\n</code></pre> <p>Streamable HTTP on port 9000. Multi-client \u2014 each client creates its own session.</p> <pre><code>MCP_TRANSPORT=\"sse\" uv run orionbelt-mcp\n</code></pre> <p>Server-Sent Events on port 9000. Legacy transport, prefer HTTP for new integrations.</p>"},{"location":"api/mcp/#configuration","title":"Configuration","text":"<p>The MCP server reads from the same <code>.env</code> file as the REST API:</p> Variable Default Description <code>MCP_TRANSPORT</code> <code>stdio</code> Transport mode (<code>stdio</code>, <code>http</code>, <code>sse</code>) <code>MCP_SERVER_HOST</code> <code>localhost</code> Bind host (http/sse only) <code>MCP_SERVER_PORT</code> <code>9000</code> Bind port (http/sse only) <code>SESSION_TTL_SECONDS</code> <code>1800</code> Session inactivity timeout <code>SESSION_CLEANUP_INTERVAL</code> <code>60</code> Cleanup sweep interval"},{"location":"api/mcp/#session-model","title":"Session Model","text":"<p>In stdio mode (single-user), a default session is created automatically. All tools work without passing a <code>session_id</code>.</p> <p>In HTTP/SSE mode (multi-client), clients must create sessions explicitly using <code>create_session</code> and pass the <code>session_id</code> to subsequent tool calls.</p>"},{"location":"api/mcp/#tools-10","title":"Tools (10)","text":""},{"location":"api/mcp/#reference","title":"Reference","text":""},{"location":"api/mcp/#get_obml_reference","title":"<code>get_obml_reference</code>","text":"<p>Get the full OBML format specification with examples. Call this tool before composing any OBML YAML to understand the correct syntax for data objects, dimensions, measures, metrics, joins, and expressions.</p> <p>No parameters required.</p> <p>Returns: The OBML format reference as text.</p>"},{"location":"api/mcp/#session-management","title":"Session Management","text":""},{"location":"api/mcp/#create_session","title":"<code>create_session</code>","text":"<p>Create a new session with its own model store.</p> Parameter Type Required Description <code>metadata_json</code> string No JSON object with metadata key-value pairs <p>Returns: Session ID and creation timestamp.</p>"},{"location":"api/mcp/#close_session","title":"<code>close_session</code>","text":"<p>Close a session and release its resources.</p> Parameter Type Required Description <code>session_id</code> string Yes Session to close"},{"location":"api/mcp/#list_sessions","title":"<code>list_sessions</code>","text":"<p>List all active sessions with their model counts and last-accessed times.</p>"},{"location":"api/mcp/#model-management","title":"Model Management","text":""},{"location":"api/mcp/#load_model","title":"<code>load_model</code>","text":"<p>Parse, validate, and store an OBML semantic model. Returns a <code>model_id</code> for use with other tools.</p> Parameter Type Required Description <code>model_yaml</code> string Yes Complete OBML YAML content <code>session_id</code> string No Target session (optional in stdio mode)"},{"location":"api/mcp/#validate_model","title":"<code>validate_model</code>","text":"<p>Validate an OBML model without storing it. Returns structured errors and warnings.</p> Parameter Type Required Description <code>model_yaml</code> string Yes Complete OBML YAML content <code>session_id</code> string No Session context (optional in stdio mode)"},{"location":"api/mcp/#describe_model","title":"<code>describe_model</code>","text":"<p>Show the contents of a loaded model: data objects, columns, joins, dimensions, measures, and metrics.</p> Parameter Type Required Description <code>model_id</code> string Yes ID returned by <code>load_model</code> <code>session_id</code> string No Session holding the model (optional in stdio mode)"},{"location":"api/mcp/#list_models","title":"<code>list_models</code>","text":"<p>List all models loaded in a session.</p> Parameter Type Required Description <code>session_id</code> string No Session to list (optional in stdio mode)"},{"location":"api/mcp/#query-compilation","title":"Query Compilation","text":""},{"location":"api/mcp/#compile_query","title":"<code>compile_query</code>","text":"<p>Compile a semantic query to SQL. Supports two modes:</p> <p>Simple mode \u2014 pass dimension and measure names directly:</p> <pre><code>compile_query(model_id=\"abc12345\", dimensions=[\"Country\"], measures=[\"Revenue\"])\n</code></pre> <p>Full mode \u2014 pass a complete query as JSON:</p> <pre><code>compile_query(model_id=\"abc12345\", query_json='{\"select\": {\"dimensions\": [\"Country\"], \"measures\": [\"Revenue\"]}, \"limit\": 10}')\n</code></pre> Parameter Type Required Description <code>model_id</code> string Yes ID returned by <code>load_model</code> <code>dialect</code> string No Target dialect (default: <code>postgres</code>) <code>dimensions</code> list[string] No Dimension names (simple mode) <code>measures</code> list[string] No Measure names (simple mode) <code>query_json</code> string No Full query as JSON (full mode) <code>session_id</code> string No Session holding the model (optional in stdio mode) <code>use_path_names</code> list[object] No Secondary join overrides (simple mode): <code>[{source, target, pathName}]</code>"},{"location":"api/mcp/#information","title":"Information","text":""},{"location":"api/mcp/#list_dialects","title":"<code>list_dialects</code>","text":"<p>List available SQL dialects and their capabilities. Stateless \u2014 no session required.</p>"},{"location":"api/mcp/#prompts-3","title":"Prompts (3)","text":""},{"location":"api/mcp/#write_obml_model","title":"<code>write_obml_model</code>","text":"<p>OBML syntax reference \u2014 how to write a semantic model in YAML. Includes the complete format specification, key rules, and a recommended workflow.</p>"},{"location":"api/mcp/#write_query","title":"<code>write_query</code>","text":"<p>How to use the <code>compile_query</code> tool \u2014 covers simple mode, full mode with filters/ordering/limits, filter operators, and tips.</p>"},{"location":"api/mcp/#debug_validation","title":"<code>debug_validation</code>","text":"<p>All OBML validation error codes with causes and fixes. Organized by category: parse errors, reference errors, semantic errors, and resolution errors.</p>"},{"location":"api/mcp/#claude-desktop-integration","title":"Claude Desktop Integration","text":"<p>Add to <code>claude_desktop_config.json</code>:</p> <pre><code>{\n  \"mcpServers\": {\n    \"orionbelt-semantic-layer\": {\n      \"command\": \"uv\",\n      \"args\": [\n        \"run\",\n        \"--directory\",\n        \"/path/to/orionbelt-semantic-layer\",\n        \"orionbelt-mcp\"\n      ]\n    }\n  }\n}\n</code></pre>"},{"location":"api/mcp/#typical-workflow","title":"Typical Workflow","text":"<ol> <li>Ask Claude to use the <code>write_obml_model</code> prompt for OBML syntax reference</li> <li>Compose a model YAML interactively</li> <li>Use <code>validate_model</code> to check for errors</li> <li>Use <code>load_model</code> to load the validated model</li> <li>Use <code>describe_model</code> to explore its contents</li> <li>Use <code>compile_query</code> to generate SQL for different dialects</li> </ol>"},{"location":"api/openapi/","title":"OpenAPI / Swagger","text":"<p>OrionBelt auto-generates interactive API documentation from its FastAPI route definitions and Pydantic schemas.</p>"},{"location":"api/openapi/#swagger-ui","title":"Swagger UI","text":"<p>Available at:</p> <pre><code>http://127.0.0.1:8000/docs\n</code></pre> <p>Swagger UI provides an interactive interface to explore and test all API endpoints. You can:</p> <ul> <li>Browse all available endpoints grouped by tags</li> <li>See request/response schemas with examples</li> <li>Execute requests directly from the browser</li> <li>View detailed parameter descriptions and validation rules</li> </ul>"},{"location":"api/openapi/#redoc","title":"ReDoc","text":"<p>Available at:</p> <pre><code>http://127.0.0.1:8000/redoc\n</code></pre> <p>ReDoc provides a clean, readable API reference with:</p> <ul> <li>Three-panel layout (navigation, documentation, request/response examples)</li> <li>Nested schema visualization</li> <li>Search functionality</li> </ul>"},{"location":"api/openapi/#openapi-json-schema","title":"OpenAPI JSON Schema","text":"<p>The raw OpenAPI 3.1 specification is available at:</p> <pre><code>http://127.0.0.1:8000/openapi.json\n</code></pre> <p>You can use this with any OpenAPI-compatible tool for:</p> <ul> <li>Client code generation (e.g., <code>openapi-generator</code>)</li> <li>API testing tools (e.g., Postman, Insomnia)</li> <li>Documentation hosting (e.g., Stoplight, ReadMe)</li> </ul>"},{"location":"api/openapi/#starting-the-server","title":"Starting the Server","text":"<pre><code>uv run orionbelt-api\n# or with reload:\nuv run uvicorn orionbelt.api.app:create_app --factory --reload\n</code></pre> <p>The <code>--reload</code> flag enables auto-reload during development. The <code>--factory</code> flag tells uvicorn to call <code>create_app()</code> to get the FastAPI instance.</p>"},{"location":"api/openapi/#api-tags","title":"API Tags","text":"<p>Endpoints are grouped by the following tags in the OpenAPI spec:</p> Tag Endpoints <code>sessions</code> <code>/sessions</code>, <code>/sessions/{id}</code>, <code>/sessions/{id}/models</code>, <code>/sessions/{id}/models/{mid}</code>, <code>/sessions/{id}/validate</code>, <code>/sessions/{id}/query/sql</code> <code>dialects</code> <code>/dialects</code> <code>health</code> <code>/health</code>"},{"location":"api/overview/","title":"API Overview","text":"<p>OrionBelt exposes two server interfaces:</p> <ol> <li>REST API \u2014 FastAPI-powered HTTP endpoints for session-based model management, validation, and query compilation</li> <li>MCP Server \u2014 Model Context Protocol server with 10 tools and 3 prompts for AI assistant integration</li> </ol>"},{"location":"api/overview/#rest-api","title":"REST API","text":""},{"location":"api/overview/#base-url","title":"Base URL","text":"<pre><code>http://127.0.0.1:8000\n</code></pre>"},{"location":"api/overview/#content-type","title":"Content Type","text":"<p>All endpoints accept and return JSON:</p> <pre><code>Content-Type: application/json\n</code></pre>"},{"location":"api/overview/#routers","title":"Routers","text":"Prefix Tag Description <code>/sessions</code> sessions Session-scoped model management, validation, and query compilation <code>/dialects</code> dialects Available SQL dialect info <code>/health</code> health Health check"},{"location":"api/overview/#session-based-workflow","title":"Session-Based Workflow","text":"<p>The primary API workflow uses sessions to manage model state:</p> <ol> <li>Create a session \u2014 <code>POST /sessions</code> returns a <code>session_id</code></li> <li>Load models \u2014 <code>POST /sessions/{id}/models</code> with OBML YAML</li> <li>Query \u2014 <code>POST /sessions/{id}/query/sql</code> against loaded models</li> <li>Close \u2014 <code>DELETE /sessions/{id}</code> when done (or let TTL expire)</li> </ol> <p>Sessions automatically expire after 30 minutes of inactivity (configurable via <code>SESSION_TTL_SECONDS</code>).</p>"},{"location":"api/overview/#mcp-server","title":"MCP Server","text":"<p>The MCP server exposes OrionBelt as tools for AI assistants (Claude Desktop, Cursor, etc.):</p> Category Tools Reference <code>get_obml_reference</code> Session <code>create_session</code>, <code>close_session</code>, <code>list_sessions</code> Model <code>load_model</code>, <code>validate_model</code>, <code>describe_model</code>, <code>list_models</code> Query <code>compile_query</code> Info <code>list_dialects</code> <p>See MCP Server for the full tool and prompt reference.</p>"},{"location":"api/overview/#error-responses","title":"Error Responses","text":"<p>All errors follow a consistent format:</p> <pre><code>{\n  \"detail\": \"Session 'abc123' not found\"\n}\n</code></pre>"},{"location":"api/overview/#status-codes","title":"Status Codes","text":"Code Meaning When 200 OK Successful GET or compilation 201 Created Session or model created (POST) 204 No Content Session or model deleted 400 Bad Request Invalid YAML, unknown dialect, bad operator 404 Not Found Session expired/missing, model not found 422 Unprocessable Entity Model validation failure, resolution error"},{"location":"api/overview/#middleware","title":"Middleware","text":""},{"location":"api/overview/#request-timing","title":"Request Timing","text":"<p>The <code>RequestTimingMiddleware</code> adds <code>X-Request-Duration-Ms</code> headers for performance monitoring.</p>"},{"location":"api/overview/#interactive-documentation","title":"Interactive Documentation","text":"<p>When the server is running, interactive API docs are available at:</p> <ul> <li>Swagger UI: <code>http://127.0.0.1:8000/docs</code></li> <li>ReDoc: <code>http://127.0.0.1:8000/redoc</code></li> </ul> <p>Both are auto-generated from the FastAPI route definitions and Pydantic schemas.</p>"},{"location":"examples/multi-dialect/","title":"Multi-Dialect Output","text":"<p>This example shows how the same semantic model and query produce different SQL for each of the five supported dialects.</p>"},{"location":"examples/multi-dialect/#the-model","title":"The Model","text":"<p>Using the Sales Model with Customers, Products, and Orders data objects.</p>"},{"location":"examples/multi-dialect/#the-query","title":"The Query","text":"<p>Revenue by customer country, filtered to SMB/MidMarket segments, ordered by revenue descending, limited to 1000 rows.</p> <pre><code>from orionbelt.models.query import *\n\nquery = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Revenue\"],\n    ),\n    where=[\n        QueryFilter(field=\"Customer Segment\", op=FilterOperator.IN, value=[\"SMB\", \"MidMarket\"]),\n    ],\n    order_by=[QueryOrderBy(field=\"Revenue\", direction=SortDirection.DESC)],\n    limit=1000,\n)\n</code></pre>"},{"location":"examples/multi-dialect/#generated-sql-by-dialect","title":"Generated SQL by Dialect","text":"PostgreSQLSnowflakeClickHouseDremioDatabricks <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nWHERE (\"Customers\".\"SEGMENT\" IN ('SMB', 'MidMarket'))\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Revenue\" DESC\nLIMIT 1000\n</code></pre> <p>Key traits: Double-quoted identifiers, <code>date_trunc()</code> for time grains, <code>ILIKE</code> for case-insensitive matching.</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nWHERE (\"Customers\".\"SEGMENT\" IN ('SMB', 'MidMarket'))\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Revenue\" DESC\nLIMIT 1000\n</code></pre> <p>Key traits: Double-quoted identifiers (case-sensitive), <code>DATE_TRUNC()</code> (uppercase), <code>CONTAINS()</code> for string matching, supports <code>QUALIFY</code> and window filters.</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nWHERE (\"Customers\".\"SEGMENT\" IN ('SMB', 'MidMarket'))\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Revenue\" DESC\nLIMIT 1000\n</code></pre> <p>Key traits: Double-quoted identifiers, custom time functions (<code>toStartOfMonth()</code>, <code>toStartOfYear()</code>), native type conversion (<code>toInt64()</code>, <code>toFloat64()</code>).</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nWHERE (\"Customers\".\"SEGMENT\" IN ('SMB', 'MidMarket'))\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Revenue\" DESC\nLIMIT 1000\n</code></pre> <p>Key traits: Double-quoted identifiers, <code>DATE_TRUNC()</code>, no <code>ILIKE</code> support (uses <code>LOWER()</code> + <code>LIKE</code> workaround), minimal capability set.</p> <pre><code>SELECT\n  `Customers`.`COUNTRY` AS `Customer Country`,\n  SUM(`Orders`.`PRICE` * `Orders`.`QUANTITY`) AS `Revenue`\nFROM WAREHOUSE.PUBLIC.ORDERS AS `Orders`\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS `Customers`\n  ON `Orders`.`CUSTOMER_ID` = `Customers`.`CUSTOMER_ID`\nWHERE (`Customers`.`SEGMENT` IN ('SMB', 'MidMarket'))\nGROUP BY `Customers`.`COUNTRY`\nORDER BY `Revenue` DESC\nLIMIT 1000\n</code></pre> <p>Key traits: Backtick-quoted identifiers (Spark SQL), <code>date_trunc()</code> for time grains, <code>lower()</code> + <code>LIKE</code> for case-insensitive matching.</p>"},{"location":"examples/multi-dialect/#key-differences","title":"Key Differences","text":""},{"location":"examples/multi-dialect/#identifier-quoting","title":"Identifier Quoting","text":"Dialect Style Example Postgres, Snowflake, ClickHouse, Dremio Double quotes <code>\"column\"</code> Databricks Backticks <code>`column`</code>"},{"location":"examples/multi-dialect/#time-grain-monthly-aggregation","title":"Time Grain: Monthly Aggregation","text":"<p>If the query included <code>\"Order Date:month\"</code> as a dimension:</p> PostgresSnowflakeClickHouseDremioDatabricks <pre><code>date_trunc('month', \"Orders\".\"ORDER_DATE\") AS \"Order Date\"\n</code></pre> <pre><code>DATE_TRUNC('month', \"Orders\".\"ORDER_DATE\") AS \"Order Date\"\n</code></pre> <pre><code>toStartOfMonth(\"Orders\".\"ORDER_DATE\") AS \"Order Date\"\n</code></pre> <pre><code>DATE_TRUNC('month', \"Orders\".\"ORDER_DATE\") AS \"Order Date\"\n</code></pre> <pre><code>date_trunc('month', `Orders`.`ORDER_DATE`) AS `Order Date`\n</code></pre>"},{"location":"examples/multi-dialect/#string-contains-filter","title":"String Contains Filter","text":"<p>If the query filtered with <code>{ field: \"Customer Country\", op: \"contains\", value: \"United\" }</code>:</p> PostgresSnowflakeClickHouseDremioDatabricks <pre><code>\"Customers\".\"COUNTRY\" ILIKE '%' || 'United' || '%'\n</code></pre> <pre><code>CONTAINS(\"Customers\".\"COUNTRY\", 'United')\n</code></pre> <pre><code>\"Customers\".\"COUNTRY\" ILIKE '%' || 'United' || '%'\n</code></pre> <pre><code>LOWER(\"Customers\".\"COUNTRY\") LIKE '%' || LOWER('United') || '%'\n</code></pre> <pre><code>lower(`Customers`.`COUNTRY`) LIKE '%' || lower('United') || '%'\n</code></pre>"},{"location":"examples/multi-dialect/#compiling-for-all-dialects","title":"Compiling for All Dialects","text":"<pre><code>from orionbelt.compiler.pipeline import CompilationPipeline\n\npipeline = CompilationPipeline()\n\nfor dialect in [\"postgres\", \"snowflake\", \"clickhouse\", \"dremio\", \"databricks\"]:\n    result = pipeline.compile(query, model, dialect)\n    print(f\"--- {dialect} ---\")\n    print(result.sql)\n    print()\n</code></pre>"},{"location":"examples/multi-fact/","title":"Multi-Fact: Sales &amp; Returns","text":"<p>This example demonstrates the Composite Fact Layer (CFL) planner with a multi-fact scenario. When a query combines measures from different fact tables, OrionBelt uses a CTE with <code>UNION ALL</code> to stitch the fact sets together (with NULL padding), then aggregates in the outer query.</p>"},{"location":"examples/multi-fact/#scenario","title":"Scenario","text":"<p>An e-commerce company tracks store sales and store returns as separate fact tables, both sharing conformed dimensions: <code>Customers</code> and <code>Date Dim</code>.</p> <pre><code>graph LR\n  SS[\"&lt;b&gt;Store Sales&lt;/b&gt;&lt;br/&gt;&lt;i&gt;fact&lt;/i&gt;&lt;br/&gt;Sale ID&lt;br/&gt;Customer ID&lt;br/&gt;Date Key&lt;br/&gt;Amount\"]\n  SR[\"&lt;b&gt;Store Returns&lt;/b&gt;&lt;br/&gt;&lt;i&gt;fact&lt;/i&gt;&lt;br/&gt;Return ID&lt;br/&gt;Customer ID&lt;br/&gt;Date Key&lt;br/&gt;Refund Amt\"]\n  C[\"&lt;b&gt;Customers&lt;/b&gt;&lt;br/&gt;Customer ID&lt;br/&gt;Name&lt;br/&gt;Country\"]\n  D[\"&lt;b&gt;Date Dim&lt;/b&gt;&lt;br/&gt;Date Key&lt;br/&gt;Full Date&lt;br/&gt;Year&lt;br/&gt;Quarter\"]\n\n  SS --&gt;|many-to-one| C\n  SS --&gt;|many-to-one| D\n  SR --&gt;|many-to-one| C\n  SR --&gt;|many-to-one| D</code></pre> <p>Both fact tables join to the same dimension tables \u2014 these are conformed dimensions.</p>"},{"location":"examples/multi-fact/#the-model","title":"The Model","text":"<pre><code># yaml-language-server: $schema=schema/obml-schema.json\nversion: 1.0\n\ndataObjects:\n  Customers:\n    code: CUSTOMERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Customer Name:\n        code: NAME\n        abstractType: string\n      Country:\n        code: COUNTRY\n        abstractType: string\n\n  Date Dim:\n    code: DATE_DIM\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Date Key:\n        code: DATE_KEY\n        abstractType: int\n      Full Date:\n        code: FULL_DATE\n        abstractType: date\n      Year:\n        code: YEAR_NUM\n        abstractType: int\n      Quarter:\n        code: QUARTER_NUM\n        abstractType: int\n\n  Store Sales:\n    code: STORE_SALES\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Sale ID:\n        code: SALE_ID\n        abstractType: string\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Date Key:\n        code: DATE_KEY\n        abstractType: int\n      Amount:\n        code: AMOUNT\n        abstractType: float\n    joins:\n      - joinType: many-to-one\n        joinTo: Customers\n        columnsFrom:\n          - Customer ID\n        columnsTo:\n          - Customer ID\n      - joinType: many-to-one\n        joinTo: Date Dim\n        columnsFrom:\n          - Date Key\n        columnsTo:\n          - Date Key\n\n  Store Returns:\n    code: STORE_RETURNS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Return ID:\n        code: RETURN_ID\n        abstractType: string\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Date Key:\n        code: DATE_KEY\n        abstractType: int\n      Refund Amount:\n        code: REFUND_AMT\n        abstractType: float\n    joins:\n      - joinType: many-to-one\n        joinTo: Customers\n        columnsFrom:\n          - Customer ID\n        columnsTo:\n          - Customer ID\n      - joinType: many-to-one\n        joinTo: Date Dim\n        columnsFrom:\n          - Date Key\n        columnsTo:\n          - Date Key\n\ndimensions:\n  Customer Country:\n    dataObject: Customers\n    column: Country\n    resultType: string\n\n  Year:\n    dataObject: Date Dim\n    column: Year\n    resultType: int\n\nmeasures:\n  Total Revenue:\n    columns:\n      - dataObject: Store Sales\n        column: Amount\n    resultType: float\n    aggregation: sum\n\n  Total Returns:\n    columns:\n      - dataObject: Store Returns\n        column: Refund Amount\n    resultType: float\n    aggregation: sum\n\nmetrics:\n  Net Revenue:\n    expression: '{[Total Revenue]} - {[Total Returns]}'\n</code></pre>"},{"location":"examples/multi-fact/#query-1-single-fact-star-schema","title":"Query 1: Single-Fact (Star Schema)","text":"<p>When a query only uses measures from one fact table, OrionBelt uses the standard star schema planner \u2014 no CTEs needed.</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Total Revenue\"],\n    ),\n    order_by=[QueryOrderBy(field=\"Total Revenue\", direction=SortDirection.DESC)],\n    limit=100,\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Store Sales\".\"AMOUNT\") AS \"Total Revenue\"\nFROM WAREHOUSE.PUBLIC.STORE_SALES AS \"Store Sales\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Store Sales\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Total Revenue\" DESC\nLIMIT 100\n</code></pre> <p>This is a straightforward star schema query: one fact table (<code>Store Sales</code>) joined to one dimension table (<code>Customers</code>).</p>"},{"location":"examples/multi-fact/#query-2-multi-fact-cfl-with-union-all","title":"Query 2: Multi-Fact (CFL with UNION ALL)","text":"<p>When measures come from different fact tables, OrionBelt detects this and switches to the CFL planner. It stitches the facts together with <code>UNION ALL</code> inside a CTE, then aggregates in the outer query.</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Total Revenue\", \"Total Returns\"],\n    ),\n    order_by=[QueryOrderBy(field=\"Total Revenue\", direction=SortDirection.DESC)],\n    limit=100,\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>WITH composite_01 AS (\n  SELECT\n    \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n    \"Store Sales\".\"AMOUNT\" AS \"Total Revenue\",\n    NULL AS \"Total Returns\"\n  FROM WAREHOUSE.PUBLIC.STORE_SALES AS \"Store Sales\"\n  LEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n    ON \"Store Sales\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\n\n  UNION ALL\n\n  SELECT\n    \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n    NULL AS \"Total Revenue\",\n    \"Store Returns\".\"REFUND_AMT\" AS \"Total Returns\"\n  FROM WAREHOUSE.PUBLIC.STORE_RETURNS AS \"Store Returns\"\n  LEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n    ON \"Store Returns\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\n)\nSELECT\n  \"Customer Country\",\n  SUM(\"Total Revenue\") AS \"Total Revenue\",\n  SUM(\"Total Returns\") AS \"Total Returns\"\nFROM composite_01\nGROUP BY \"Customer Country\"\nORDER BY \"Total Revenue\" DESC\nLIMIT 100\n</code></pre>"},{"location":"examples/multi-fact/#what-happened","title":"What Happened","text":"<ol> <li>Measure grouping \u2014 The resolver detected that <code>Total Revenue</code> comes from <code>Store Sales</code> and <code>Total Returns</code> comes from <code>Store Returns</code> \u2014 two different fact tables</li> <li>UNION ALL with NULL padding \u2014 Each fact leg selects the conformed dimensions plus its own measure columns, filling the other fact's measures with <code>NULL</code>. This produces a single row set at the raw grain</li> <li>Outer aggregation \u2014 The outer query groups by the conformed dimensions and applies <code>SUM</code> over the unioned rows. Because each row only has a value for one fact's measures (NULLs are ignored by <code>SUM</code>), the results are correct</li> <li>No fanout \u2014 The facts are never joined directly to each other, so no many-to-many explosion occurs</li> </ol> <p>Snowflake: UNION BY NAME</p> <p>On Snowflake, OrionBelt uses <code>UNION BY NAME</code> instead of <code>UNION ALL</code>. This matches columns by name rather than position, so the NULL padding columns can be omitted \u2014 each fact leg only selects its own measures.</p>"},{"location":"examples/multi-fact/#why-cfl-is-necessary","title":"Why CFL is Necessary","text":"<p>Without the Composite Fact Layer, combining measures from different fact tables in a single query would cause fanout \u2014 a many-to-many join that inflates aggregation results:</p> <pre><code>graph LR\n  SS[\"Store Sales&lt;br/&gt;10 rows\"] --- C[\"Customers\"]\n  SR[\"Store Returns&lt;br/&gt;5 rows\"] --- C\n\n  style C fill:#f96,stroke:#333</code></pre> <p>Direct join: 10 x 5 = 50 rows \u2014 wrong SUM results!</p> <p>By stacking the facts with <code>UNION ALL</code> and aggregating outside, CFL eliminates fanout entirely. Each row in the union contributes to exactly one fact's measures.</p> <p>When CFL Activates</p> <p>The compiler automatically selects CFL when <code>requires_cfl = True</code> in the resolved query. This happens when the query's measures reference fields from multiple distinct fact tables. You do not need to opt in \u2014 OrionBelt detects it for you.</p>"},{"location":"examples/multi-fact/#query-3-net-revenue-metric","title":"Query 3: Net Revenue Metric","text":"<p>Metrics that combine measures from different fact tables also use CFL:</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\", \"Year\"],\n        measures=[\"Net Revenue\"],\n    ),\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>WITH composite_01 AS (\n  SELECT\n    \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n    \"Date Dim\".\"YEAR_NUM\" AS \"Year\",\n    \"Store Sales\".\"AMOUNT\" AS \"Total Revenue\",\n    NULL AS \"Total Returns\"\n  FROM WAREHOUSE.PUBLIC.STORE_SALES AS \"Store Sales\"\n  LEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n    ON \"Store Sales\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\n  LEFT JOIN WAREHOUSE.PUBLIC.DATE_DIM AS \"Date Dim\"\n    ON \"Store Sales\".\"DATE_KEY\" = \"Date Dim\".\"DATE_KEY\"\n\n  UNION ALL\n\n  SELECT\n    \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n    \"Date Dim\".\"YEAR_NUM\" AS \"Year\",\n    NULL AS \"Total Revenue\",\n    \"Store Returns\".\"REFUND_AMT\" AS \"Total Returns\"\n  FROM WAREHOUSE.PUBLIC.STORE_RETURNS AS \"Store Returns\"\n  LEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n    ON \"Store Returns\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\n  LEFT JOIN WAREHOUSE.PUBLIC.DATE_DIM AS \"Date Dim\"\n    ON \"Store Returns\".\"DATE_KEY\" = \"Date Dim\".\"DATE_KEY\"\n)\nSELECT\n  \"Customer Country\",\n  \"Year\",\n  (SUM(\"Total Revenue\") - SUM(\"Total Returns\")) AS \"Net Revenue\"\nFROM composite_01\nGROUP BY \"Customer Country\", \"Year\"\n</code></pre> <p>The metric expression <code>{[Total Revenue]} - {[Total Returns]}</code> is applied to the aggregated sums in the outer SELECT. Both measures are aggregated from the same <code>UNION ALL</code> result set, with NULLs naturally ignored by <code>SUM</code>.</p> <p>Multiple Conformed Dimensions</p> <p>When there are multiple conformed dimensions (here: <code>Customer Country</code> and <code>Year</code>), both appear in the <code>GROUP BY</code> of the outer query. Each UNION leg joins its own dimension tables independently, so the grain is preserved correctly.</p>"},{"location":"examples/sales-model/","title":"Sales Model Walkthrough","text":"<p>This example walks through a complete semantic model for a sales analytics use case with customers, products, and orders.</p>"},{"location":"examples/sales-model/#the-model","title":"The Model","text":"<pre><code># yaml-language-server: $schema=schema/obml-schema.json\nversion: 1.0\n\ndataObjects:\n  Customers:\n    code: CUSTOMERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Customer Name:\n        code: NAME\n        abstractType: string\n      Country:\n        code: COUNTRY\n        abstractType: string\n      Segment:\n        code: SEGMENT\n        abstractType: string\n\n  Products:\n    code: PRODUCTS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Product ID:\n        code: PRODUCT_ID\n        abstractType: string\n      Product Name:\n        code: NAME\n        abstractType: string\n      Category:\n        code: CATEGORY\n        abstractType: string\n\n  Orders:\n    code: ORDERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Order ID:\n        code: ORDER_ID\n        abstractType: string\n      Order Date:\n        code: ORDER_DATE\n        abstractType: date\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Product ID:\n        code: PRODUCT_ID\n        abstractType: string\n      Quantity:\n        code: QUANTITY\n        abstractType: int\n      Price:\n        code: PRICE\n        abstractType: float\n    joins:\n      - joinType: many-to-one\n        joinTo: Customers\n        columnsFrom:\n          - Customer ID\n        columnsTo:\n          - Customer ID\n      - joinType: many-to-one\n        joinTo: Products\n        columnsFrom:\n          - Product ID\n        columnsTo:\n          - Product ID\n\ndimensions:\n  Customer Country:\n    dataObject: Customers\n    column: Country\n    resultType: string\n\n  Customer Segment:\n    dataObject: Customers\n    column: Segment\n    resultType: string\n\n  Product Name:\n    dataObject: Products\n    column: Product Name\n    resultType: string\n\n  Product Category:\n    dataObject: Products\n    column: Category\n    resultType: string\n\n  Order Date:\n    dataObject: Orders\n    column: Order Date\n    resultType: date\n    timeGrain: month\n\nmeasures:\n  Revenue:\n    resultType: float\n    aggregation: sum\n    expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n\n  Order Count:\n    columns:\n      - dataObject: Orders\n        column: Order ID\n    resultType: int\n    aggregation: count\n\n  Average Order Value:\n    resultType: float\n    aggregation: avg\n    expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n\nmetrics:\n  Revenue per Order:\n    expression: '{[Revenue]} / {[Order Count]}'\n</code></pre>"},{"location":"examples/sales-model/#data-objects-explained","title":"Data Objects Explained","text":""},{"location":"examples/sales-model/#star-schema-structure","title":"Star Schema Structure","text":"<p>This model follows a classic star schema:</p> <pre><code>graph LR\n  O[\"&lt;b&gt;Orders&lt;/b&gt;&lt;br/&gt;&lt;i&gt;fact&lt;/i&gt;&lt;br/&gt;Order ID&lt;br/&gt;Order Date&lt;br/&gt;Customer ID&lt;br/&gt;Product ID&lt;br/&gt;Quantity&lt;br/&gt;Price\"]\n  C[\"&lt;b&gt;Customers&lt;/b&gt;&lt;br/&gt;Customer ID&lt;br/&gt;Name&lt;br/&gt;Country&lt;br/&gt;Segment\"]\n  P[\"&lt;b&gt;Products&lt;/b&gt;&lt;br/&gt;Product ID&lt;br/&gt;Name&lt;br/&gt;Category\"]\n\n  O --&gt;|many-to-one| C\n  O --&gt;|many-to-one| P</code></pre> <p>Orders is the fact table \u2014 it declares joins to both dimension tables. The compiler automatically identifies it as the base object because it has join definitions.</p>"},{"location":"examples/sales-model/#column-mapping","title":"Column Mapping","text":"<p>Each column maps a business name to a physical column:</p> Data Object Business Name Physical Column Customers Customer ID <code>CUSTOMER_ID</code> Customers Country <code>COUNTRY</code> Products Product Name <code>NAME</code> Orders Price <code>PRICE</code> <p>The physical column name appears in generated SQL, while the business name is used in dimensions, measures, and queries.</p>"},{"location":"examples/sales-model/#dimensions-explained","title":"Dimensions Explained","text":""},{"location":"examples/sales-model/#standard-dimension","title":"Standard Dimension","text":"<pre><code>Customer Country:\n  dataObject: Customers\n  column: Country\n  resultType: string\n</code></pre> <p>References the <code>Country</code> column in the <code>Customers</code> data object. When queried, generates <code>\"Customers\".\"COUNTRY\"</code> with a GROUP BY.</p>"},{"location":"examples/sales-model/#time-dimension","title":"Time Dimension","text":"<pre><code>Order Date:\n  dataObject: Orders\n  column: Order Date\n  resultType: date\n  timeGrain: month\n</code></pre> <p>The <code>timeGrain: month</code> means queries using this dimension will apply <code>date_trunc('month', ...)</code> by default.</p>"},{"location":"examples/sales-model/#measures-explained","title":"Measures Explained","text":""},{"location":"examples/sales-model/#expression-measure-revenue","title":"Expression Measure \u2014 Revenue","text":"<pre><code>Revenue:\n  resultType: float\n  aggregation: sum\n  expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n</code></pre> <p>Compiles to: <code>SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\")</code></p> <p>The expression references columns using <code>{[DataObject].[Column]}</code> syntax. The <code>sum</code> aggregation wraps the entire expression.</p>"},{"location":"examples/sales-model/#simple-measure-order-count","title":"Simple Measure \u2014 Order Count","text":"<pre><code>Order Count:\n  columns:\n    - dataObject: Orders\n      column: Order ID\n  resultType: int\n  aggregation: count\n</code></pre> <p>Compiles to: <code>COUNT(\"Orders\".\"ORDER_ID\")</code></p>"},{"location":"examples/sales-model/#expression-measure-average-order-value","title":"Expression Measure \u2014 Average Order Value","text":"<pre><code>Average Order Value:\n  resultType: float\n  aggregation: avg\n  expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n</code></pre> <p>Compiles to: <code>AVG(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\")</code></p> <p>Uses <code>{[DataObject].[Column]}</code> syntax to reference columns directly in the expression \u2014 no <code>columns</code> array needed.</p>"},{"location":"examples/sales-model/#metrics-explained","title":"Metrics Explained","text":"<pre><code>Revenue per Order:\n  expression: '{[Revenue]} / {[Order Count]}'\n</code></pre> <p>This divides Revenue by Order Count. The <code>{[Revenue]}</code> and <code>{[Order Count]}</code> placeholders reference the measures defined above by name. Each measure is computed independently and then combined in the metric expression.</p>"},{"location":"examples/sales-model/#example-queries","title":"Example Queries","text":""},{"location":"examples/sales-model/#revenue-by-country","title":"Revenue by Country","text":"<pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Revenue\", \"Order Count\"],\n    ),\n    order_by=[QueryOrderBy(field=\"Revenue\", direction=SortDirection.DESC)],\n    limit=1000,\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\",\n  COUNT(\"Orders\".\"ORDER_ID\") AS \"Order Count\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Revenue\" DESC\nLIMIT 1000\n</code></pre>"},{"location":"examples/sales-model/#filtered-query-smb-customers","title":"Filtered Query \u2014 SMB Customers","text":"<pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Revenue\"],\n    ),\n    where=[\n        QueryFilter(field=\"Customer Segment\", op=FilterOperator.IN, value=[\"SMB\", \"MidMarket\"]),\n    ],\n)\n</code></pre> <p>Generated SQL:</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nWHERE (\"Customers\".\"SEGMENT\" IN ('SMB', 'MidMarket'))\nGROUP BY \"Customers\".\"COUNTRY\"\n</code></pre>"},{"location":"examples/sales-model/#multi-dimension-query","title":"Multi-Dimension Query","text":"<pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\", \"Product Category\"],\n        measures=[\"Revenue\"],\n    ),\n)\n</code></pre> <p>Generated SQL:</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  \"Products\".\"CATEGORY\" AS \"Product Category\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nLEFT JOIN WAREHOUSE.PUBLIC.PRODUCTS AS \"Products\"\n  ON \"Orders\".\"PRODUCT_ID\" = \"Products\".\"PRODUCT_ID\"\nGROUP BY \"Customers\".\"COUNTRY\", \"Products\".\"CATEGORY\"\n</code></pre> <p>Notice how the compiler automatically adds the <code>Products</code> join because the <code>Product Category</code> dimension requires it.</p>"},{"location":"examples/tpcds/","title":"TPC-DS Benchmark","text":"<p>This example shows how to model a subset of the TPC-DS benchmark \u2014 an industry-standard decision support benchmark \u2014 using OrionBelt's OBML format.</p> <p>TPC-DS models a retail company with multiple sales channels (store, catalog, web), returns, inventory, and a rich set of dimension tables. We focus on store sales and store returns with key dimensions to demonstrate both star schema and CFL planning.</p>"},{"location":"examples/tpcds/#the-model","title":"The Model","text":"<pre><code># yaml-language-server: $schema=schema/obml-schema.json\nversion: 1.0\n\ndataObjects:\n  store_sales:\n    code: STORE_SALES\n    database: TPCDS\n    schema: PUBLIC\n    columns:\n      ss_sold_date_sk:\n        code: SS_SOLD_DATE_SK\n        abstractType: int\n      ss_customer_sk:\n        code: SS_CUSTOMER_SK\n        abstractType: int\n      ss_item_sk:\n        code: SS_ITEM_SK\n        abstractType: int\n      ss_store_sk:\n        code: SS_STORE_SK\n        abstractType: int\n      ss_quantity:\n        code: SS_QUANTITY\n        abstractType: int\n      ss_sales_price:\n        code: SS_SALES_PRICE\n        abstractType: float\n      ss_net_profit:\n        code: SS_NET_PROFIT\n        abstractType: float\n    joins:\n      - joinType: many-to-one\n        joinTo: date_dim\n        columnsFrom:\n          - ss_sold_date_sk\n        columnsTo:\n          - d_date_sk\n      - joinType: many-to-one\n        joinTo: customer\n        columnsFrom:\n          - ss_customer_sk\n        columnsTo:\n          - c_customer_sk\n      - joinType: many-to-one\n        joinTo: item\n        columnsFrom:\n          - ss_item_sk\n        columnsTo:\n          - i_item_sk\n      - joinType: many-to-one\n        joinTo: store\n        columnsFrom:\n          - ss_store_sk\n        columnsTo:\n          - s_store_sk\n\n  store_returns:\n    code: STORE_RETURNS\n    database: TPCDS\n    schema: PUBLIC\n    columns:\n      sr_returned_date_sk:\n        code: SR_RETURNED_DATE_SK\n        abstractType: int\n      sr_customer_sk:\n        code: SR_CUSTOMER_SK\n        abstractType: int\n      sr_item_sk:\n        code: SR_ITEM_SK\n        abstractType: int\n      sr_store_sk:\n        code: SR_STORE_SK\n        abstractType: int\n      sr_return_quantity:\n        code: SR_RETURN_QUANTITY\n        abstractType: int\n      sr_return_amt:\n        code: SR_RETURN_AMT\n        abstractType: float\n    joins:\n      - joinType: many-to-one\n        joinTo: date_dim\n        columnsFrom:\n          - sr_returned_date_sk\n        columnsTo:\n          - d_date_sk\n      - joinType: many-to-one\n        joinTo: customer\n        columnsFrom:\n          - sr_customer_sk\n        columnsTo:\n          - c_customer_sk\n      - joinType: many-to-one\n        joinTo: item\n        columnsFrom:\n          - sr_item_sk\n        columnsTo:\n          - i_item_sk\n      - joinType: many-to-one\n        joinTo: store\n        columnsFrom:\n          - sr_store_sk\n        columnsTo:\n          - s_store_sk\n\n  date_dim:\n    code: DATE_DIM\n    database: TPCDS\n    schema: PUBLIC\n    columns:\n      d_date_sk:\n        code: D_DATE_SK\n        abstractType: int\n      d_date:\n        code: D_DATE\n        abstractType: date\n      d_year:\n        code: D_YEAR\n        abstractType: int\n      d_qoy:\n        code: D_QOY\n        abstractType: int\n\n  customer:\n    code: CUSTOMER\n    database: TPCDS\n    schema: PUBLIC\n    columns:\n      c_customer_sk:\n        code: C_CUSTOMER_SK\n        abstractType: int\n      c_first_name:\n        code: C_FIRST_NAME\n        abstractType: string\n      c_last_name:\n        code: C_LAST_NAME\n        abstractType: string\n      c_birth_country:\n        code: C_BIRTH_COUNTRY\n        abstractType: string\n\n  item:\n    code: ITEM\n    database: TPCDS\n    schema: PUBLIC\n    columns:\n      i_item_sk:\n        code: I_ITEM_SK\n        abstractType: int\n      i_item_id:\n        code: I_ITEM_ID\n        abstractType: string\n      i_category:\n        code: I_CATEGORY\n        abstractType: string\n      i_class:\n        code: I_CLASS\n        abstractType: string\n\n  store:\n    code: STORE\n    database: TPCDS\n    schema: PUBLIC\n    columns:\n      s_store_sk:\n        code: S_STORE_SK\n        abstractType: int\n      s_store_name:\n        code: S_STORE_NAME\n        abstractType: string\n      s_state:\n        code: S_STATE\n        abstractType: string\n\ndimensions:\n  Year:\n    dataObject: date_dim\n    column: d_year\n    resultType: int\n\n  Quarter:\n    dataObject: date_dim\n    column: d_qoy\n    resultType: int\n\n  Sale Date:\n    dataObject: date_dim\n    column: d_date\n    resultType: date\n\n  Customer Country:\n    dataObject: customer\n    column: c_birth_country\n    resultType: string\n\n  Item Category:\n    dataObject: item\n    column: i_category\n    resultType: string\n\n  Item Class:\n    dataObject: item\n    column: i_class\n    resultType: string\n\n  Store Name:\n    dataObject: store\n    column: s_store_name\n    resultType: string\n\n  Store State:\n    dataObject: store\n    column: s_state\n    resultType: string\n\nmeasures:\n  Sales Amount:\n    resultType: float\n    aggregation: sum\n    expression: '{[store_sales].[ss_sales_price]} * {[store_sales].[ss_quantity]}'\n\n  Net Profit:\n    columns:\n      - dataObject: store_sales\n        column: ss_net_profit\n    resultType: float\n    aggregation: sum\n\n  Quantity Sold:\n    columns:\n      - dataObject: store_sales\n        column: ss_quantity\n    resultType: int\n    aggregation: sum\n\n  Return Amount:\n    columns:\n      - dataObject: store_returns\n        column: sr_return_amt\n    resultType: float\n    aggregation: sum\n\n  Quantity Returned:\n    columns:\n      - dataObject: store_returns\n        column: sr_return_quantity\n    resultType: int\n    aggregation: sum\n\nmetrics:\n  Return Rate:\n    expression: '{[Quantity Returned]} / {[Quantity Sold]}'\n\n  Net Revenue:\n    expression: '{[Sales Amount]} - {[Return Amount]}'\n</code></pre>"},{"location":"examples/tpcds/#schema-diagram","title":"Schema Diagram","text":"<pre><code>graph LR\n  SS[\"&lt;b&gt;store_sales&lt;/b&gt;&lt;br/&gt;&lt;i&gt;fact&lt;/i&gt;&lt;br/&gt;ss_quantity&lt;br/&gt;ss_sales_price&lt;br/&gt;ss_net_profit\"]\n  SR[\"&lt;b&gt;store_returns&lt;/b&gt;&lt;br/&gt;&lt;i&gt;fact&lt;/i&gt;&lt;br/&gt;sr_return_quantity&lt;br/&gt;sr_return_amt\"]\n  D[\"&lt;b&gt;date_dim&lt;/b&gt;&lt;br/&gt;d_date_sk&lt;br/&gt;d_year&lt;br/&gt;d_qoy&lt;br/&gt;d_date\"]\n  C[\"&lt;b&gt;customer&lt;/b&gt;&lt;br/&gt;c_customer_sk&lt;br/&gt;c_first_name&lt;br/&gt;c_birth_country\"]\n  I[\"&lt;b&gt;item&lt;/b&gt;&lt;br/&gt;i_item_sk&lt;br/&gt;i_category&lt;br/&gt;i_class\"]\n  S[\"&lt;b&gt;store&lt;/b&gt;&lt;br/&gt;s_store_sk&lt;br/&gt;s_store_name&lt;br/&gt;s_state\"]\n\n  SS --&gt;|many-to-one| D\n  SS --&gt;|many-to-one| C\n  SS --&gt;|many-to-one| I\n  SS --&gt;|many-to-one| S\n  SR --&gt;|many-to-one| D\n  SR --&gt;|many-to-one| C\n  SR --&gt;|many-to-one| I\n  SR --&gt;|many-to-one| S</code></pre> <p>Both <code>store_sales</code> and <code>store_returns</code> share the same four dimension tables \u2014 classic conformed dimensions in TPC-DS.</p>"},{"location":"examples/tpcds/#query-1-sales-by-year-and-item-category-star-schema","title":"Query 1: Sales by Year and Item Category (Star Schema)","text":"<p>A single-fact query using only <code>store_sales</code> measures:</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Year\", \"Item Category\"],\n        measures=[\"Sales Amount\", \"Net Profit\"],\n    ),\n    order_by=[QueryOrderBy(field=\"Sales Amount\", direction=SortDirection.DESC)],\n    limit=1000,\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>SELECT\n  \"date_dim\".\"D_YEAR\" AS \"Year\",\n  \"item\".\"I_CATEGORY\" AS \"Item Category\",\n  SUM(\"store_sales\".\"SS_SALES_PRICE\" * \"store_sales\".\"SS_QUANTITY\") AS \"Sales Amount\",\n  SUM(\"store_sales\".\"SS_NET_PROFIT\") AS \"Net Profit\"\nFROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\nLEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n  ON \"store_sales\".\"SS_SOLD_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\nLEFT JOIN TPCDS.PUBLIC.ITEM AS \"item\"\n  ON \"store_sales\".\"SS_ITEM_SK\" = \"item\".\"I_ITEM_SK\"\nGROUP BY \"date_dim\".\"D_YEAR\", \"item\".\"I_CATEGORY\"\nORDER BY \"Sales Amount\" DESC\nLIMIT 1000\n</code></pre> <p>All measures come from <code>store_sales</code>, so the star schema planner handles this with a simple SELECT + JOINs.</p>"},{"location":"examples/tpcds/#query-2-sales-vs-returns-by-customer-country-cfl","title":"Query 2: Sales vs Returns by Customer Country (CFL)","text":"<p>When we combine measures from both <code>store_sales</code> and <code>store_returns</code>, the CFL planner kicks in:</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Sales Amount\", \"Return Amount\"],\n    ),\n    order_by=[QueryOrderBy(field=\"Sales Amount\", direction=SortDirection.DESC)],\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>WITH composite_01 AS (\n  SELECT\n    \"customer\".\"C_BIRTH_COUNTRY\" AS \"Customer Country\",\n    \"store_sales\".\"SS_SALES_PRICE\" * \"store_sales\".\"SS_QUANTITY\" AS \"Sales Amount\",\n    NULL AS \"Return Amount\"\n  FROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\n  LEFT JOIN TPCDS.PUBLIC.CUSTOMER AS \"customer\"\n    ON \"store_sales\".\"SS_CUSTOMER_SK\" = \"customer\".\"C_CUSTOMER_SK\"\n\n  UNION ALL\n\n  SELECT\n    \"customer\".\"C_BIRTH_COUNTRY\" AS \"Customer Country\",\n    NULL AS \"Sales Amount\",\n    \"store_returns\".\"SR_RETURN_AMT\" AS \"Return Amount\"\n  FROM TPCDS.PUBLIC.STORE_RETURNS AS \"store_returns\"\n  LEFT JOIN TPCDS.PUBLIC.CUSTOMER AS \"customer\"\n    ON \"store_returns\".\"SR_CUSTOMER_SK\" = \"customer\".\"C_CUSTOMER_SK\"\n)\nSELECT\n  \"Customer Country\",\n  SUM(\"Sales Amount\") AS \"Sales Amount\",\n  SUM(\"Return Amount\") AS \"Return Amount\"\nFROM composite_01\nGROUP BY \"Customer Country\"\nORDER BY \"Sales Amount\" DESC\n</code></pre> <p>The two fact legs are stacked with <code>UNION ALL</code> \u2014 each leg selects the conformed dimension plus its own measure, with <code>NULL</code> for the other fact's measure. The outer query aggregates over the union, and <code>SUM</code> naturally ignores the NULLs.</p>"},{"location":"examples/tpcds/#query-3-quarterly-net-profit-with-time-grain","title":"Query 3: Quarterly Net Profit with Time Grain","text":"<p>Using a time grain on the <code>Sale Date</code> dimension:</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Sale Date:quarter\"],\n        measures=[\"Net Profit\"],\n    ),\n    order_by=[QueryOrderBy(field=\"Sale Date\", direction=SortDirection.ASC)],\n)\n</code></pre>"},{"location":"examples/tpcds/#multi-dialect-output","title":"Multi-Dialect Output","text":"PostgreSQLSnowflakeClickHouseDremioDatabricks <pre><code>SELECT\n  date_trunc('quarter', \"date_dim\".\"D_DATE\") AS \"Sale Date\",\n  SUM(\"store_sales\".\"SS_NET_PROFIT\") AS \"Net Profit\"\nFROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\nLEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n  ON \"store_sales\".\"SS_SOLD_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\nGROUP BY date_trunc('quarter', \"date_dim\".\"D_DATE\")\nORDER BY \"Sale Date\" ASC\n</code></pre> <p>Key traits: <code>date_trunc('quarter', ...)</code> for time grain truncation.</p> <pre><code>SELECT\n  DATE_TRUNC('quarter', \"date_dim\".\"D_DATE\") AS \"Sale Date\",\n  SUM(\"store_sales\".\"SS_NET_PROFIT\") AS \"Net Profit\"\nFROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\nLEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n  ON \"store_sales\".\"SS_SOLD_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\nGROUP BY DATE_TRUNC('quarter', \"date_dim\".\"D_DATE\")\nORDER BY \"Sale Date\" ASC\n</code></pre> <p>Key traits: <code>DATE_TRUNC()</code> in uppercase, double-quoted identifiers.</p> <pre><code>SELECT\n  toStartOfQuarter(\"date_dim\".\"D_DATE\") AS \"Sale Date\",\n  SUM(\"store_sales\".\"SS_NET_PROFIT\") AS \"Net Profit\"\nFROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\nLEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n  ON \"store_sales\".\"SS_SOLD_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\nGROUP BY toStartOfQuarter(\"date_dim\".\"D_DATE\")\nORDER BY \"Sale Date\" ASC\n</code></pre> <p>Key traits: ClickHouse uses native <code>toStartOfQuarter()</code> instead of <code>date_trunc</code>.</p> <pre><code>SELECT\n  DATE_TRUNC('quarter', \"date_dim\".\"D_DATE\") AS \"Sale Date\",\n  SUM(\"store_sales\".\"SS_NET_PROFIT\") AS \"Net Profit\"\nFROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\nLEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n  ON \"store_sales\".\"SS_SOLD_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\nGROUP BY DATE_TRUNC('quarter', \"date_dim\".\"D_DATE\")\nORDER BY \"Sale Date\" ASC\n</code></pre> <p>Key traits: <code>DATE_TRUNC()</code> uppercase, same quoting as Postgres/Snowflake.</p> <pre><code>SELECT\n  date_trunc('quarter', `date_dim`.`D_DATE`) AS `Sale Date`,\n  SUM(`store_sales`.`SS_NET_PROFIT`) AS `Net Profit`\nFROM TPCDS.PUBLIC.STORE_SALES AS `store_sales`\nLEFT JOIN TPCDS.PUBLIC.DATE_DIM AS `date_dim`\n  ON `store_sales`.`SS_SOLD_DATE_SK` = `date_dim`.`D_DATE_SK`\nGROUP BY date_trunc('quarter', `date_dim`.`D_DATE`)\nORDER BY `Sale Date` ASC\n</code></pre> <p>Key traits: Backtick-quoted identifiers (Spark SQL), lowercase <code>date_trunc</code>.</p>"},{"location":"examples/tpcds/#query-4-return-rate-metric-cfl","title":"Query 4: Return Rate Metric (CFL)","text":"<p>The <code>Return Rate</code> metric combines measures from both fact tables:</p> <pre><code>query = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Year\", \"Store State\"],\n        measures=[\"Return Rate\"],\n    ),\n)\n</code></pre> <p>Generated SQL (Postgres):</p> <pre><code>WITH composite_01 AS (\n  SELECT\n    \"date_dim\".\"D_YEAR\" AS \"Year\",\n    \"store\".\"S_STATE\" AS \"Store State\",\n    \"store_returns\".\"SR_RETURN_QUANTITY\" AS \"Quantity Returned\",\n    NULL AS \"Quantity Sold\"\n  FROM TPCDS.PUBLIC.STORE_RETURNS AS \"store_returns\"\n  LEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n    ON \"store_returns\".\"SR_RETURNED_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\n  LEFT JOIN TPCDS.PUBLIC.STORE AS \"store\"\n    ON \"store_returns\".\"SR_STORE_SK\" = \"store\".\"S_STORE_SK\"\n\n  UNION ALL\n\n  SELECT\n    \"date_dim\".\"D_YEAR\" AS \"Year\",\n    \"store\".\"S_STATE\" AS \"Store State\",\n    NULL AS \"Quantity Returned\",\n    \"store_sales\".\"SS_QUANTITY\" AS \"Quantity Sold\"\n  FROM TPCDS.PUBLIC.STORE_SALES AS \"store_sales\"\n  LEFT JOIN TPCDS.PUBLIC.DATE_DIM AS \"date_dim\"\n    ON \"store_sales\".\"SS_SOLD_DATE_SK\" = \"date_dim\".\"D_DATE_SK\"\n  LEFT JOIN TPCDS.PUBLIC.STORE AS \"store\"\n    ON \"store_sales\".\"SS_STORE_SK\" = \"store\".\"S_STORE_SK\"\n)\nSELECT\n  \"Year\",\n  \"Store State\",\n  (SUM(\"Quantity Returned\") / SUM(\"Quantity Sold\")) AS \"Return Rate\"\nFROM composite_01\nGROUP BY \"Year\", \"Store State\"\n</code></pre> <p>The metric expression <code>{[Quantity Returned]} / {[Quantity Sold]}</code> is applied to the aggregated sums in the outer SELECT.</p>"},{"location":"examples/tpcds/#compiling-for-all-dialects","title":"Compiling for All Dialects","text":"<pre><code>from orionbelt.compiler.pipeline import CompilationPipeline\n\npipeline = CompilationPipeline()\n\nfor dialect in [\"postgres\", \"snowflake\", \"clickhouse\", \"dremio\", \"databricks\"]:\n    result = pipeline.compile(query, model, dialect)\n    print(f\"--- {dialect} ---\")\n    print(result.sql)\n    print()\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>uv \u2014 fast Python package manager (recommended)</li> </ul>"},{"location":"getting-started/installation/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/ralfbecher/orionbelt-semantic-layer.git\ncd orionbelt-semantic-layer\n</code></pre>"},{"location":"getting-started/installation/#install-dependencies","title":"Install Dependencies","text":"<pre><code>uv sync\n</code></pre> <p>This installs all runtime dependencies:</p> Package Purpose <code>fastapi</code> REST API framework <code>uvicorn</code> ASGI server <code>pydantic</code> Model validation (v2) <code>pydantic-settings</code> Configuration from environment / .env <code>ruamel.yaml</code> YAML parsing with source positions <code>networkx</code> Join graph algorithms <code>fastmcp</code> MCP server framework <code>sqlalchemy</code> + <code>alembic</code> Database persistence <code>structlog</code> Structured logging <code>opentelemetry-api</code> Observability"},{"location":"getting-started/installation/#development-dependencies","title":"Development Dependencies","text":"<pre><code>uv sync --group dev\n</code></pre> <p>Adds <code>pytest</code>, <code>ruff</code>, <code>mypy</code>, <code>httpx</code>, <code>pre-commit</code>, and type stubs.</p>"},{"location":"getting-started/installation/#documentation-dependencies","title":"Documentation Dependencies","text":"<pre><code>uv sync --extra docs\n</code></pre> <p>Adds <code>mkdocs-material</code>, <code>mkdocs-autorefs</code>, and <code>mkdocstrings[python]</code>.</p>"},{"location":"getting-started/installation/#verify-the-installation","title":"Verify the Installation","text":"<pre><code># Run the test suite\nuv run pytest\n\n# Type check\nuv run mypy src/\n\n# Lint\nuv run ruff check src/\n</code></pre>"},{"location":"getting-started/installation/#configuration","title":"Configuration","text":"<p>OrionBelt reads configuration from environment variables and a <code>.env</code> file. Copy the example:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Key settings:</p> Variable Default Description <code>LOG_LEVEL</code> <code>INFO</code> Logging level <code>API_SERVER_HOST</code> <code>localhost</code> REST API bind host <code>API_SERVER_PORT</code> <code>8000</code> REST API bind port <code>MCP_TRANSPORT</code> <code>stdio</code> MCP transport (<code>stdio</code>, <code>http</code>, <code>sse</code>) <code>MCP_SERVER_HOST</code> <code>localhost</code> MCP server host (http/sse only) <code>MCP_SERVER_PORT</code> <code>9000</code> MCP server port (http/sse only) <code>SESSION_TTL_SECONDS</code> <code>1800</code> Session inactivity timeout (30 min) <code>SESSION_CLEANUP_INTERVAL</code> <code>60</code> Cleanup sweep interval (seconds)"},{"location":"getting-started/installation/#start-the-servers","title":"Start the Servers","text":""},{"location":"getting-started/installation/#rest-api","title":"REST API","text":"<pre><code>uv run orionbelt-api\n# or with reload:\nuv run uvicorn orionbelt.api.app:create_app --factory --reload\n</code></pre> <p>The API is available at:</p> <ul> <li><code>http://127.0.0.1:8000</code> \u2014 API root</li> <li><code>http://127.0.0.1:8000/docs</code> \u2014 Swagger UI</li> <li><code>http://127.0.0.1:8000/redoc</code> \u2014 ReDoc</li> <li><code>http://127.0.0.1:8000/health</code> \u2014 Health check</li> </ul>"},{"location":"getting-started/installation/#mcp-server","title":"MCP Server","text":"<pre><code># stdio (default, for Claude Desktop / Cursor)\nuv run orionbelt-mcp\n\n# HTTP transport (for multi-client use)\nMCP_TRANSPORT=http uv run orionbelt-mcp\n</code></pre>"},{"location":"getting-started/installation/#project-structure","title":"Project Structure","text":"<pre><code>orionbelt-semantic-layer/\n\u251c\u2500\u2500 src/orionbelt/\n\u2502   \u251c\u2500\u2500 api/            # FastAPI app, routers, schemas, deps, middleware\n\u2502   \u2502   \u2514\u2500\u2500 routers/    # sessions, validate, query, dialects\n\u2502   \u251c\u2500\u2500 ast/            # SQL AST nodes, builder, visitor\n\u2502   \u251c\u2500\u2500 compiler/       # Resolution, planning (star/CFL), codegen pipeline\n\u2502   \u251c\u2500\u2500 dialect/        # 5 SQL dialect implementations\n\u2502   \u251c\u2500\u2500 mcp/            # MCP server (10 tools, 3 prompts)\n\u2502   \u251c\u2500\u2500 models/         # Pydantic models (semantic, query, errors)\n\u2502   \u251c\u2500\u2500 parser/         # YAML loader, reference resolver, validator\n\u2502   \u251c\u2500\u2500 service/        # ModelStore, SessionManager\n\u2502   \u2514\u2500\u2500 settings.py     # Shared configuration\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/           # Unit tests for each module\n\u2502   \u251c\u2500\u2500 integration/    # End-to-end compilation and API tests\n\u2502   \u2514\u2500\u2500 fixtures/       # Sample models and queries\n\u251c\u2500\u2500 examples/           # Model examples and JSON Schema\n\u251c\u2500\u2500 schema/             # OBML JSON Schema\n\u251c\u2500\u2500 docs/               # MkDocs documentation source\n\u251c\u2500\u2500 mkdocs.yml          # MkDocs configuration\n\u2514\u2500\u2500 pyproject.toml      # Project metadata and dependencies\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This walkthrough takes you from a YAML semantic model to compiled SQL in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#step-1-define-a-semantic-model","title":"Step 1: Define a Semantic Model","text":"<p>Create a file called <code>model.yaml</code>:</p> <pre><code># yaml-language-server: $schema=schema/obml-schema.json\nversion: 1.0\n\ndataObjects:\n  Customers:\n    code: CUSTOMERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Country:\n        code: COUNTRY\n        abstractType: string\n      Segment:\n        code: SEGMENT\n        abstractType: string\n\n  Orders:\n    code: ORDERS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Order ID:\n        code: ORDER_ID\n        abstractType: string\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Price:\n        code: PRICE\n        abstractType: float\n      Quantity:\n        code: QUANTITY\n        abstractType: int\n    joins:\n      - joinType: many-to-one\n        joinTo: Customers\n        columnsFrom:\n          - Customer ID\n        columnsTo:\n          - Customer ID\n\ndimensions:\n  Customer Country:\n    dataObject: Customers\n    column: Country\n    resultType: string\n  Customer Segment:\n    dataObject: Customers\n    column: Segment\n    resultType: string\n\nmeasures:\n  Revenue:\n    resultType: float\n    aggregation: sum\n    expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n\n  Order Count:\n    columns:\n      - dataObject: Orders\n        column: Order ID\n    resultType: int\n    aggregation: count\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-load-and-validate","title":"Step 2: Load and Validate","text":"<pre><code>from orionbelt.parser.loader import TrackedLoader\nfrom orionbelt.parser.resolver import ReferenceResolver\nfrom orionbelt.parser.validator import SemanticValidator\n\n# Load YAML with source position tracking\nloader = TrackedLoader()\nraw, source_map = loader.load(\"model.yaml\")\n\n# Resolve references into typed Pydantic models\nresolver = ReferenceResolver()\nmodel, result = resolver.resolve(raw, source_map)\n\nif not result.valid:\n    for error in result.errors:\n        print(f\"  {error.code}: {error.message}\")\nelse:\n    print(\"Model is valid!\")\n\n# Run semantic validation (cycle check, reference resolution, etc.)\nvalidator = SemanticValidator()\nerrors = validator.validate(model)\nfor error in errors:\n    print(f\"  {error.code}: {error.message}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-compile-a-query","title":"Step 3: Compile a Query","text":"<pre><code>from orionbelt.compiler.pipeline import CompilationPipeline\nfrom orionbelt.models.query import (\n    QueryObject,\n    QuerySelect,\n    QueryFilter,\n    FilterOperator,\n    QueryOrderBy,\n    SortDirection,\n)\n\n# Define a query: Revenue by country for SMB/MidMarket customers\nquery = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\"],\n        measures=[\"Revenue\", \"Order Count\"],\n    ),\n    where=[\n        QueryFilter(\n            field=\"Customer Segment\",\n            op=FilterOperator.IN,\n            value=[\"SMB\", \"MidMarket\"],\n        ),\n    ],\n    order_by=[QueryOrderBy(field=\"Revenue\", direction=SortDirection.DESC)],\n    limit=1000,\n)\n\n# Compile to Postgres SQL\npipeline = CompilationPipeline()\nresult = pipeline.compile(query, model, \"postgres\")\nprint(result.sql)\n</code></pre> <p>Output:</p> <pre><code>SELECT\n  \"Customers\".\"COUNTRY\" AS \"Customer Country\",\n  SUM(\"Orders\".\"PRICE\" * \"Orders\".\"QUANTITY\") AS \"Revenue\",\n  COUNT(\"Orders\".\"ORDER_ID\") AS \"Order Count\"\nFROM WAREHOUSE.PUBLIC.ORDERS AS \"Orders\"\nLEFT JOIN WAREHOUSE.PUBLIC.CUSTOMERS AS \"Customers\"\n  ON \"Orders\".\"CUSTOMER_ID\" = \"Customers\".\"CUSTOMER_ID\"\nWHERE (\"Customers\".\"SEGMENT\" IN ('SMB', 'MidMarket'))\nGROUP BY \"Customers\".\"COUNTRY\"\nORDER BY \"Revenue\" DESC\nLIMIT 1000\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-try-a-different-dialect","title":"Step 4: Try a Different Dialect","text":"<p>Simply change the dialect parameter:</p> <pre><code># Snowflake\nresult = pipeline.compile(query, model, \"snowflake\")\n\n# ClickHouse\nresult = pipeline.compile(query, model, \"clickhouse\")\n\n# Databricks\nresult = pipeline.compile(query, model, \"databricks\")\n</code></pre> <p>Each dialect applies its own identifier quoting, function names, and SQL syntax. See SQL Dialects for details.</p>"},{"location":"getting-started/quickstart/#step-5-use-the-rest-api-with-sessions","title":"Step 5: Use the REST API with Sessions","text":"<p>Start the server:</p> <pre><code>uv run orionbelt-api\n</code></pre> <p>Create a session and load a model:</p> <pre><code># Create a session\nSESSION_ID=$(curl -s -X POST http://127.0.0.1:8000/sessions | jq -r .session_id)\n\n# Load a model into the session\nMODEL_ID=$(curl -s -X POST \"http://127.0.0.1:8000/sessions/$SESSION_ID/models\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"model_yaml\\\": \\\"$(cat model.yaml)\\\"}\" | jq -r .model_id)\n\n# Compile a query\ncurl -s -X POST \"http://127.0.0.1:8000/sessions/$SESSION_ID/query/sql\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\n    \\\"model_id\\\": \\\"$MODEL_ID\\\",\n    \\\"query\\\": {\n      \\\"select\\\": {\n        \\\"dimensions\\\": [\\\"Customer Country\\\"],\n        \\\"measures\\\": [\\\"Revenue\\\"]\n      }\n    },\n    \\\"dialect\\\": \\\"postgres\\\"\n  }\" | jq .sql\n\n# Clean up\ncurl -s -X DELETE \"http://127.0.0.1:8000/sessions/$SESSION_ID\"\n</code></pre>"},{"location":"getting-started/quickstart/#step-6-use-with-claude-desktop-mcp","title":"Step 6: Use with Claude Desktop (MCP)","text":"<p>Add OrionBelt to your Claude Desktop config:</p> <pre><code>{\n  \"mcpServers\": {\n    \"orionbelt-semantic-layer\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"--directory\", \"/path/to/orionbelt-semantic-layer\", \"orionbelt-mcp\"]\n    }\n  }\n}\n</code></pre> <p>Then in Claude Desktop, you can ask:</p> <p>Load this OBML model and compile a query for Revenue by Country using Snowflake dialect.</p> <p>Claude will use the <code>load_model</code>, <code>describe_model</code>, and <code>compile_query</code> tools to complete the workflow.</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>OBML Model Format \u2014 Complete OrionBelt ML specification</li> <li>Query Language \u2014 Filters, operators, time grains</li> <li>SQL Dialects \u2014 Dialect capabilities and differences</li> <li>API Endpoints \u2014 Full REST API documentation</li> <li>MCP Server \u2014 MCP tools and prompts reference</li> </ul>"},{"location":"guide/compilation/","title":"Compilation Pipeline","text":"<p>OrionBelt compiles semantic queries into SQL through a three-phase pipeline: Resolution, Planning, and Code Generation. Each phase transforms the query into a progressively more concrete representation.</p> <pre><code>QueryObject + SemanticModel\n        |\n        v\n+-----------------+\n|  Phase 1:       |\n|  Resolution     |  -&gt; ResolvedQuery\n+--------+--------+\n         |\n         v\n+-----------------+\n|  Phase 2:       |\n|  Planning       |  -&gt; QueryPlan (SQL AST)\n|  (Star or CFL)  |\n+--------+--------+\n         |\n         v\n+-----------------+\n|  Phase 3:       |\n|  Code Generation|  -&gt; SQL string\n|  (Dialect)      |\n+-----------------+\n</code></pre>"},{"location":"guide/compilation/#phase-1-resolution","title":"Phase 1: Resolution","text":"<p>Module: <code>orionbelt.compiler.resolution</code></p> <p>The resolver transforms a high-level <code>QueryObject</code> (business names) into a <code>ResolvedQuery</code> (concrete column references and expressions).</p>"},{"location":"guide/compilation/#what-resolution-does","title":"What Resolution Does","text":"<ol> <li>Resolve dimensions \u2014 Look up each dimension name in the model, find the source data object and column, apply time grain if requested</li> <li>Resolve measures \u2014 Expand expression placeholders (<code>{[DataObject].[Column]}</code>) into column references, wrap in aggregation functions</li> <li>Resolve metrics \u2014 Expand measure references (<code>{[Measure Name]}</code>), compose expressions</li> <li>Select base object \u2014 Choose the primary fact table (prefers data objects with joins defined)</li> <li>Find join paths \u2014 Use the join graph to find the minimal set of joins connecting all required objects</li> <li>Classify filters \u2014 Dimension filters -&gt; WHERE, measure filters -&gt; HAVING</li> <li>Resolve ORDER BY \u2014 Map field names to dimension or measure expressions</li> </ol>"},{"location":"guide/compilation/#resolvedquery","title":"ResolvedQuery","text":"<p>The output of resolution contains everything the planner needs:</p> Field Type Description <code>dimensions</code> <code>list[ResolvedDimension]</code> Resolved column refs with data object/field/source <code>measures</code> <code>list[ResolvedMeasure]</code> AST expressions with aggregation <code>base_object</code> <code>str</code> Selected fact table name <code>required_objects</code> <code>set[str]</code> All data objects needed by the query <code>join_steps</code> <code>list[JoinStep]</code> Ordered join sequence <code>where_filters</code> <code>list[ResolvedFilter]</code> Dimension filter expressions <code>having_filters</code> <code>list[ResolvedFilter]</code> Measure filter expressions <code>order_by_exprs</code> <code>list[tuple[Expr, bool]]</code> (expression, is_descending) pairs <code>limit</code> <code>int | None</code> Row limit <code>requires_cfl</code> <code>bool</code> Whether multi-fact CFL planning is needed <code>use_path_names</code> <code>list[UsePathName]</code> Secondary join overrides from the query"},{"location":"guide/compilation/#join-graph","title":"Join Graph","text":"<p>Module: <code>orionbelt.compiler.graph</code></p> <p>The <code>JoinGraph</code> uses networkx to model data object relationships:</p> <ul> <li>Undirected graph for finding shortest paths between data objects</li> <li>Directed graph for cycle detection, reachability checks, and common root computation</li> <li><code>find_join_path(from_objects, to_objects)</code> returns the minimal <code>JoinStep</code> sequence</li> <li><code>descendants(node)</code> returns all nodes reachable via directed join paths from the given node</li> <li><code>find_common_root(required_objects)</code> finds the deepest directed ancestor that can reach all required objects \u2014 used by the CFL planner to select the FROM base for each UNION ALL leg</li> <li><code>build_join_condition(step)</code> generates equality conditions from field mappings</li> <li>Accepts optional <code>use_path_names</code> to activate secondary joins \u2014 when a secondary override is active for a <code>(source, target)</code> pair, the primary join is replaced by the matching secondary join</li> </ul> <pre><code># Example: Orders -&gt; Customers join\nJoinStep(\n    from_object=\"Orders\",\n    to_object=\"Customers\",\n    from_columns=[\"Customer ID\"],\n    to_columns=[\"Customer ID\"],\n    join_type=JoinType.LEFT,\n    cardinality=Cardinality.MANY_TO_ONE,\n)\n</code></pre>"},{"location":"guide/compilation/#phase-2-planning","title":"Phase 2: Planning","text":"<p>The planner converts a <code>ResolvedQuery</code> into a <code>QueryPlan</code> containing an SQL AST (<code>Select</code> node).</p>"},{"location":"guide/compilation/#star-schema-planner","title":"Star Schema Planner","text":"<p>Module: <code>orionbelt.compiler.star</code></p> <p>Used for single-fact queries (most common case). Builds a straightforward SELECT with joins:</p> <pre><code>SELECT  dimension_columns, aggregate_expressions\nFROM    base_fact_table\nJOIN    dimension_table ON condition\nWHERE   dimension_filters\nGROUP BY dimension_columns\nHAVING  measure_filters\nORDER BY ...\nLIMIT   ...\n</code></pre> <p>The planner uses the <code>QueryBuilder</code> fluent API to construct the AST:</p> <pre><code>builder = QueryBuilder()\nbuilder.select(...)           # dimensions + measures\nbuilder.from_(fact_table)     # base fact\nbuilder.join(dim_table, on=condition)  # each join step\nbuilder.where(filter_expr)    # WHERE conditions\nbuilder.group_by(dim_cols)    # GROUP BY\nbuilder.having(having_expr)   # HAVING conditions\nbuilder.order_by(expr, desc=True)\nbuilder.limit(1000)\nplan = QueryPlan(ast=builder.build())\n</code></pre>"},{"location":"guide/compilation/#cfl-planner-composite-fact-layer","title":"CFL Planner (Composite Fact Layer)","text":"<p>Module: <code>orionbelt.compiler.cfl</code></p> <p>Used for multi-fact queries \u2014 when measures come from truly independent fact tables that are not reachable from each other via directed join paths. The CFL planner uses a UNION ALL strategy:</p> <ol> <li>Groups measures by source data object \u2014 Identifies which measures belong to which fact table</li> <li>Finds common root per leg \u2014 Each leg uses <code>JoinGraph.find_common_root()</code> to find the deepest directed ancestor covering all required objects (dimension objects + measure source) for that leg</li> <li>Validates fanout \u2014 Ensures dimensions are compatible across facts</li> <li>Builds UNION ALL legs \u2014 Each fact leg starts FROM the common root, JOINs to reach all required objects, SELECTs conformed dimensions + its own measures (with NULL for the other facts' measures)</li> <li>Combines into a CTE \u2014 The legs are combined with <code>UNION ALL</code> into a single <code>composite_01</code> CTE</li> <li>Outer aggregation \u2014 The outer query aggregates over the union, grouping by conformed dimensions</li> </ol> <p>CFL trigger</p> <p>CFL is only activated when measure source objects are truly unreachable from the base object via directed join paths. If all measure sources are reachable from a single fact table, the star schema planner is used instead \u2014 even when measures reference columns from different data objects.</p> <pre><code>WITH composite_01 AS (\n  SELECT country, price * quantity AS revenue, NULL AS return_count\n  FROM orders JOIN customers ON ...\n  UNION ALL\n  SELECT country, NULL AS revenue, 1 AS return_count\n  FROM returns JOIN customers ON ...\n)\nSELECT\n  country,\n  SUM(revenue) AS revenue,\n  COUNT(return_count) AS return_count\nFROM composite_01\nGROUP BY country\n</code></pre> <p>On Snowflake, <code>UNION ALL BY NAME</code> is used instead, so each leg only selects its own measures (no NULL padding needed).</p> <p>If there is only one fact table, the CFL planner delegates to the Star Schema planner.</p>"},{"location":"guide/compilation/#phase-3-code-generation","title":"Phase 3: Code Generation","text":"<p>Module: <code>orionbelt.compiler.codegen</code></p> <p>The code generator walks the SQL AST and produces a dialect-specific SQL string. It delegates entirely to the dialect's <code>compile()</code> method.</p> <pre><code>class CodeGenerator:\n    def __init__(self, dialect: Dialect) -&gt; None:\n        self._dialect = dialect\n\n    def generate(self, ast: Select) -&gt; str:\n        return self._dialect.compile(ast)\n</code></pre> <p>The dialect's <code>compile()</code> method recursively visits each AST node:</p> <ul> <li><code>Select</code> -&gt; <code>SELECT ... FROM ... JOIN ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... LIMIT ...</code></li> <li><code>ColumnRef</code> -&gt; <code>\"table\".\"column\"</code> (or <code>`table`.`column`</code> for Databricks)</li> <li><code>FunctionCall</code> -&gt; <code>SUM(\"col\")</code>, <code>COUNT(DISTINCT \"col\")</code>, etc.</li> <li><code>BinaryOp</code> -&gt; <code>(left op right)</code></li> <li><code>Literal</code> -&gt; <code>'string'</code>, <code>42</code>, <code>NULL</code>, <code>TRUE</code></li> <li><code>CTE</code> -&gt; <code>WITH name AS (SELECT ...)</code></li> </ul>"},{"location":"guide/compilation/#sql-ast","title":"SQL AST","text":"<p>Module: <code>orionbelt.ast.nodes</code></p> <p>All SQL is generated from an immutable AST \u2014 never by string concatenation. The AST nodes are frozen dataclasses:</p>"},{"location":"guide/compilation/#expression-nodes","title":"Expression Nodes","text":"Node Description Example <code>Literal</code> Constant value <code>'hello'</code>, <code>42</code>, <code>NULL</code> <code>ColumnRef</code> Column reference <code>\"table\".\"col\"</code> <code>Star</code> Wildcard <code>*</code>, <code>\"table\".*</code> <code>AliasedExpr</code> Aliased expression <code>expr AS \"alias\"</code> <code>FunctionCall</code> Function call <code>SUM(\"col\")</code> <code>BinaryOp</code> Binary operator <code>(a + b)</code>, <code>(x AND y)</code> <code>UnaryOp</code> Unary operator <code>NOT x</code> <code>IsNull</code> NULL check <code>x IS NULL</code>, <code>x IS NOT NULL</code> <code>InList</code> IN list <code>x IN (1, 2, 3)</code> <code>Between</code> Range check <code>x BETWEEN 1 AND 10</code> <code>CaseExpr</code> CASE expression <code>CASE WHEN ... THEN ... END</code> <code>Cast</code> Type cast <code>CAST(x AS INTEGER)</code> <code>SubqueryExpr</code> Subquery <code>(SELECT ...)</code> <code>RawSQL</code> Escape hatch Raw SQL string"},{"location":"guide/compilation/#statement-nodes","title":"Statement Nodes","text":"Node Description <code>Select</code> Full SELECT statement with columns, from, joins, where, group_by, having, order_by, limit, ctes <code>From</code> FROM clause (table or subquery with alias) <code>Join</code> JOIN clause (type, source, alias, on condition) <code>OrderByItem</code> ORDER BY item (expression, direction, nulls handling) <code>CTE</code> Common Table Expression (name + SELECT or UNION ALL query) <code>UnionAll</code> UNION ALL of multiple SELECT statements"},{"location":"guide/compilation/#querybuilder","title":"QueryBuilder","text":"<p>Module: <code>orionbelt.ast.builder</code></p> <p>Fluent API for constructing AST nodes:</p> <pre><code>from orionbelt.ast.builder import QueryBuilder, col, func, lit, alias, eq, and_\n\nquery = (\n    QueryBuilder()\n    .select(alias(col(\"COUNTRY\", \"Customers\"), \"Country\"))\n    .select(alias(func(\"SUM\", col(\"PRICE\", \"Orders\")), \"Revenue\"))\n    .from_(\"WAREHOUSE.PUBLIC.ORDERS\", alias=\"Orders\")\n    .join(\"WAREHOUSE.PUBLIC.CUSTOMERS\", on=eq(col(\"CUSTOMER_ID\", \"Orders\"), col(\"CUSTOMER_ID\", \"Customers\")), alias=\"Customers\")\n    .where(col(\"SEGMENT\", \"Customers\"))\n    .group_by(col(\"COUNTRY\", \"Customers\"))\n    .order_by(col(\"Revenue\"), desc=True)\n    .limit(100)\n    .build()\n)\n</code></pre>"},{"location":"guide/compilation/#pipeline-orchestration","title":"Pipeline Orchestration","text":"<p>Module: <code>orionbelt.compiler.pipeline</code></p> <p>The <code>CompilationPipeline</code> ties all phases together:</p> <pre><code>class CompilationPipeline:\n    def compile(self, query: QueryObject, model: SemanticModel, dialect_name: str) -&gt; CompilationResult:\n        # Phase 1: Resolution\n        resolved = QueryResolver().resolve(query, model)\n\n        # Phase 2: Planning\n        if resolved.requires_cfl:\n            plan = CFLPlanner.plan(resolved, model)\n        else:\n            plan = StarSchemaPlanner.plan(resolved, model)\n\n        # Phase 3: Code Generation\n        dialect = DialectRegistry.get(dialect_name)\n        sql = CodeGenerator(dialect).generate(plan.ast)\n\n        return CompilationResult(sql=sql, dialect=dialect_name, resolved=..., warnings=...)\n</code></pre> <p>The <code>CompilationResult</code> includes:</p> Field Type Description <code>sql</code> <code>str</code> Generated SQL string <code>dialect</code> <code>str</code> Dialect name used <code>resolved</code> <code>ResolvedInfo</code> Fact tables, dimensions, measures used <code>warnings</code> <code>list[str]</code> Non-fatal warnings"},{"location":"guide/dialects/","title":"SQL Dialects","text":"<p>OrionBelt compiles semantic queries into SQL for five database dialects. Each dialect has its own identifier quoting, function names, and SQL syntax. The plugin architecture allows adding new dialects without modifying the core compiler.</p>"},{"location":"guide/dialects/#supported-dialects","title":"Supported Dialects","text":"Dialect Identifier Description PostgreSQL <code>postgres</code> Standard PostgreSQL with strict GROUP BY Snowflake <code>snowflake</code> Cloud data warehouse with QUALIFY, semi-structured types ClickHouse <code>clickhouse</code> Column-oriented OLAP with custom date/aggregation functions Dremio <code>dremio</code> Data lakehouse with reduced function surface Databricks SQL <code>databricks</code> Spark SQL semantics with backtick identifiers"},{"location":"guide/dialects/#capabilities-matrix","title":"Capabilities Matrix","text":"<p>Each dialect declares capability flags that the compiler uses to choose SQL generation strategies.</p> Capability Postgres Snowflake ClickHouse Dremio Databricks <code>supports_cte</code> Yes Yes Yes Yes Yes <code>supports_qualify</code> No Yes No No No <code>supports_arrays</code> Yes Yes Yes No Yes <code>supports_window_filters</code> No Yes No No No <code>supports_ilike</code> Yes Yes Yes No No <code>supports_time_travel</code> No Yes No No No <code>supports_semi_structured</code> No Yes No No No"},{"location":"guide/dialects/#identifier-quoting","title":"Identifier Quoting","text":"Dialect Style Example Postgres Double quotes <code>\"column_name\"</code> Snowflake Double quotes <code>\"column_name\"</code> ClickHouse Double quotes <code>\"column_name\"</code> Dremio Double quotes <code>\"column_name\"</code> Databricks Backticks <code>`column_name`</code>"},{"location":"guide/dialects/#time-grain-functions","title":"Time Grain Functions","text":"<p>The <code>timeGrain</code> is rendered differently per dialect:</p> PostgresSnowflakeClickHouseDremioDatabricks <pre><code>date_trunc('month', \"order_date\")\ndate_trunc('year', \"order_date\")\ndate_trunc('quarter', \"order_date\")\n</code></pre> <pre><code>DATE_TRUNC('month', \"order_date\")\nDATE_TRUNC('year', \"order_date\")\nDATE_TRUNC('quarter', \"order_date\")\n</code></pre> <pre><code>toStartOfMonth(\"order_date\")\ntoStartOfYear(\"order_date\")\ntoStartOfQuarter(\"order_date\")\ntoMonday(\"order_date\")        -- week\ntoDate(\"order_date\")          -- day\ntoStartOfHour(\"order_date\")\ntoStartOfMinute(\"order_date\")\ntoStartOfSecond(\"order_date\")\n</code></pre> <pre><code>DATE_TRUNC('month', \"order_date\")\nDATE_TRUNC('year', \"order_date\")\n</code></pre> <pre><code>date_trunc('month', `order_date`)\ndate_trunc('year', `order_date`)\n</code></pre>"},{"location":"guide/dialects/#string-contains","title":"String Contains","text":"<p>The <code>contains</code> filter operator is rendered per dialect:</p> PostgresSnowflakeClickHouseDremioDatabricks <pre><code>\"column\" ILIKE '%' || 'search' || '%'\n</code></pre> <pre><code>CONTAINS(\"column\", 'search')\n</code></pre> <pre><code>\"column\" ILIKE '%' || 'search' || '%'\n</code></pre> <pre><code>LOWER(\"column\") LIKE '%' || LOWER('search') || '%'\n</code></pre> <pre><code>lower(`column`) LIKE '%' || lower('search') || '%'\n</code></pre>"},{"location":"guide/dialects/#cast-handling","title":"CAST Handling","text":"Postgres / Snowflake / Dremio / DatabricksClickHouse <pre><code>CAST(expr AS INTEGER)\nCAST(expr AS VARCHAR)\nCAST(expr AS DATE)\n</code></pre> <p>ClickHouse uses native conversion functions:</p> <pre><code>toInt64(expr)      -- int / integer\ntoFloat64(expr)    -- float / double\ntoString(expr)     -- string / varchar\ntoDate(expr)       -- date\n-- Other types fall back to CAST\nCAST(expr AS DateTime)\n</code></pre>"},{"location":"guide/dialects/#aggregation-functions","title":"Aggregation Functions","text":"<p>Most aggregations (<code>SUM</code>, <code>COUNT</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>) compile identically across dialects. The following aggregations require dialect-specific rendering:</p>"},{"location":"guide/dialects/#any_value","title":"ANY_VALUE","text":"Dialect SQL Postgres <code>ANY_VALUE(col)</code> Snowflake <code>ANY_VALUE(col)</code> ClickHouse <code>any(col)</code> Dremio <code>ANY_VALUE(col)</code> Databricks <code>ANY_VALUE(col)</code>"},{"location":"guide/dialects/#median","title":"MEDIAN","text":"Dialect SQL Postgres <code>PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY col)</code> Snowflake <code>MEDIAN(col)</code> ClickHouse <code>MEDIAN(col)</code> Dremio <code>MEDIAN(col)</code> Databricks <code>MEDIAN(col)</code>"},{"location":"guide/dialects/#mode","title":"MODE","text":"Dialect SQL Postgres <code>MODE() WITHIN GROUP (ORDER BY col)</code> Snowflake <code>MODE(col)</code> ClickHouse <code>topK(1)(col)[1]</code> Dremio Not supported Databricks <code>MODE(col)</code>"},{"location":"guide/dialects/#listagg","title":"LISTAGG","text":"Dialect Base + DISTINCT + ORDER BY Postgres <code>STRING_AGG(col, sep)</code> <code>STRING_AGG(DISTINCT col, sep)</code> <code>STRING_AGG(col, sep ORDER BY col)</code> Snowflake <code>LISTAGG(col, sep)</code> <code>LISTAGG(DISTINCT col, sep)</code> <code>LISTAGG(col, sep) WITHIN GROUP (ORDER BY col)</code> ClickHouse <code>arrayStringConcat(groupArray(col), sep)</code> <code>arrayStringConcat(groupUniqArray(col), sep)</code> <code>arrayStringConcat(arraySort(groupArray(col)), sep)</code> Dremio <code>LISTAGG(col, sep)</code> <code>LISTAGG(DISTINCT col, sep)</code> <code>LISTAGG(col, sep) WITHIN GROUP (ORDER BY col)</code> Databricks <code>ARRAY_JOIN(COLLECT_LIST(col), sep)</code> <code>ARRAY_JOIN(COLLECT_SET(col), sep)</code> <code>ARRAY_JOIN(SORT_ARRAY(COLLECT_LIST(col)), sep)</code> <p>LISTAGG ordering limitations</p> <p>ClickHouse and Databricks only support self-ordering (sorting by the aggregated column). Ordering by a different column raises an error at compile time.</p> <p>Total not supported</p> <p><code>MEDIAN</code>, <code>MODE</code>, <code>LISTAGG</code>, and <code>ANY_VALUE</code> do not support <code>total: true</code> because they cannot be meaningfully re-aggregated via window functions.</p>"},{"location":"guide/dialects/#dialect-plugin-architecture","title":"Dialect Plugin Architecture","text":"<p>Each dialect implements the abstract <code>Dialect</code> base class:</p> <pre><code>class Dialect(ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str: ...\n\n    @property\n    @abstractmethod\n    def capabilities(self) -&gt; DialectCapabilities: ...\n\n    @abstractmethod\n    def quote_identifier(self, name: str) -&gt; str: ...\n\n    @abstractmethod\n    def render_time_grain(self, column: Expr, grain: TimeGrain) -&gt; Expr: ...\n\n    @abstractmethod\n    def render_cast(self, expr: Expr, target_type: str) -&gt; Expr: ...\n\n    def render_string_contains(self, column: Expr, pattern: Expr) -&gt; Expr: ...\n\n    def compile(self, ast: Select) -&gt; str: ...\n</code></pre> <p>Dialects register themselves via the <code>@DialectRegistry.register</code> decorator:</p> <pre><code>@DialectRegistry.register\nclass PostgresDialect(Dialect):\n    @property\n    def name(self) -&gt; str:\n        return \"postgres\"\n    ...\n</code></pre> <p>The registry provides lookup by name:</p> <pre><code>from orionbelt.dialect.registry import DialectRegistry\n\ndialect = DialectRegistry.get(\"snowflake\")\nsql = dialect.compile(ast)\n</code></pre>"},{"location":"guide/dialects/#adding-a-new-dialect","title":"Adding a New Dialect","text":"<ol> <li>Create <code>src/orionbelt/dialect/my_dialect.py</code></li> <li>Subclass <code>Dialect</code> and implement all abstract methods</li> <li>Decorate with <code>@DialectRegistry.register</code></li> <li>The dialect is automatically available via <code>DialectRegistry.get(\"my_dialect\")</code></li> </ol>"},{"location":"guide/dialects/#querying-dialect-info-via-api","title":"Querying Dialect Info via API","text":"<pre><code>curl http://127.0.0.1:8000/dialects\n</code></pre> <pre><code>{\n  \"dialects\": [\n    {\n      \"name\": \"postgres\",\n      \"capabilities\": {\n        \"supports_cte\": true,\n        \"supports_qualify\": false,\n        \"supports_arrays\": true,\n        \"supports_window_filters\": false,\n        \"supports_ilike\": true,\n        \"supports_time_travel\": false,\n        \"supports_semi_structured\": false\n      }\n    },\n    ...\n  ]\n}\n</code></pre>"},{"location":"guide/model-format/","title":"OrionBelt ML (OBML) Model Format","text":"<p>OrionBelt ML (OBML) is the YAML-based format for defining semantic models in OrionBelt. A model describes your data warehouse tables (data objects), business dimensions, aggregate measures, and composite metrics.</p>"},{"location":"guide/model-format/#top-level-structure","title":"Top-Level Structure","text":"<pre><code># yaml-language-server: $schema=schema/obml-schema.json\nversion: 1.0\n\ndataObjects:  # Database tables/views with columns and joins\n  ...\n\ndimensions:   # Named dimensions referencing data object columns\n  ...\n\nmeasures:     # Aggregations with expressions\n  ...\n\nmetrics:      # Composite metrics combining measures\n  ...\n</code></pre> <p>All four sections are dictionaries keyed by name.</p>"},{"location":"guide/model-format/#data-objects","title":"Data Objects","text":"<p>A data object maps to a database table or custom SQL statement. Each data object declares its columns and optional join relationships.</p> <pre><code>dataObjects:\n  Orders:\n    code: ORDERS              # Table name or custom SQL\n    database: WAREHOUSE         # Database/catalog\n    schema: PUBLIC              # Schema\n    columns:\n      Order ID:\n        code: ORDER_ID        # Physical column name\n        abstractType: string\n      Order Date:\n        code: ORDER_DATE\n        abstractType: date\n      Customer ID:\n        code: CUSTOMER_ID\n        abstractType: string\n      Price:\n        code: PRICE\n        abstractType: float\n    joins:\n      - joinType: many-to-one\n        joinTo: Customers\n        columnsFrom:\n          - Customer ID\n        columnsTo:\n          - Customer ID\n</code></pre>"},{"location":"guide/model-format/#data-object-properties","title":"Data Object Properties","text":"Property Type Required Description <code>code</code> string Yes Table name or SQL statement <code>database</code> string Yes Database/catalog name <code>schema</code> string Yes Schema name <code>columns</code> map Yes Dictionary of column definitions <code>joins</code> list No Join relationships to other data objects <code>comment</code> string No Documentation"},{"location":"guide/model-format/#columns","title":"Columns","text":"Property Type Required Description <code>code</code> string Yes Physical column name in the database <code>abstractType</code> enum Yes <code>string</code>, <code>int</code>, <code>float</code>, <code>date</code>, <code>time</code>, <code>time_tz</code>, <code>timestamp</code>, <code>timestamp_tz</code>, <code>boolean</code>, <code>json</code> <code>sqlType</code> string No Informational: SQL data type (e.g. <code>VARCHAR</code>, <code>INTEGER</code>, <code>NUMERIC(10,2)</code>) <code>sqlPrecision</code> int No Informational: numeric precision <code>sqlScale</code> int No Informational: numeric scale <code>comment</code> string No Documentation"},{"location":"guide/model-format/#joins","title":"Joins","text":"<p>Joins define relationships between data objects. The data object that declares the join is the \"from\" side.</p> Property Type Required Description <code>joinType</code> enum Yes <code>many-to-one</code>, <code>one-to-one</code>, <code>many-to-many</code> <code>joinTo</code> string Yes Target data object name <code>columnsFrom</code> list Yes Column names in this data object (join keys) <code>columnsTo</code> list Yes Column names in the target data object (join keys) <code>secondary</code> bool No Mark as a secondary (alternative) join path (default: <code>false</code>) <code>pathName</code> string No Unique name for this join path (required when <code>secondary: true</code>) <p>Fact tables declare joins</p> <p>By convention, fact tables (e.g., <code>Orders</code>) declare joins to dimension tables (e.g., <code>Customers</code>, <code>Products</code>). The compiler uses this to identify fact tables \u2014 data objects with joins are preferred as base objects during query resolution.</p>"},{"location":"guide/model-format/#secondary-joins","title":"Secondary Joins","text":"<p>When a data object has multiple relationships to the same target (e.g., a <code>Flights</code> table joining to <code>Airports</code> via both departure and arrival), mark the additional joins as <code>secondary</code> with a unique <code>pathName</code>:</p> <pre><code>dataObjects:\n  Flights:\n    code: FLIGHTS\n    database: WAREHOUSE\n    schema: PUBLIC\n    columns:\n      Departure Airport:\n        code: DEP_AIRPORT\n        abstractType: string\n      Arrival Airport:\n        code: ARR_AIRPORT\n        abstractType: string\n    joins:\n      - joinType: many-to-one\n        joinTo: Airports\n        columnsFrom:\n          - Departure Airport\n        columnsTo:\n          - Airport ID\n      - joinType: many-to-one\n        joinTo: Airports\n        secondary: true\n        pathName: arrival\n        columnsFrom:\n          - Arrival Airport\n        columnsTo:\n          - Airport ID\n</code></pre> <p>Rules:</p> <ul> <li>Every secondary join must have a <code>pathName</code></li> <li><code>pathName</code> must be unique per <code>(source, target)</code> pair (not globally)</li> <li>Secondary joins are excluded from cycle detection and multipath validation</li> <li>Queries use <code>usePathNames</code> to select a secondary join instead of the default primary \u2014 see Query Language</li> </ul>"},{"location":"guide/model-format/#column-references","title":"Column References","text":"<p>Columns are referenced using the <code>dataObject</code> + <code>column</code> pair throughout the model:</p> <pre><code>dimensions:\n  Product Name:\n    dataObject: Products\n    column: Product Name\n    resultType: string\n</code></pre> <p>Column names must be unique within each data object. Dimensions, measures, and metrics must have unique names across the whole model.</p>"},{"location":"guide/model-format/#dimensions","title":"Dimensions","text":"<p>A dimension defines a business attribute used for grouping (GROUP BY) in queries.</p> <pre><code>dimensions:\n  Customer Country:\n    dataObject: Customers\n    column: Country\n    resultType: string\n\n  Order Date:\n    dataObject: Orders\n    column: Order Date\n    resultType: date\n    timeGrain: month\n</code></pre>"},{"location":"guide/model-format/#dimension-properties","title":"Dimension Properties","text":"Property Type Required Description <code>dataObject</code> string Yes Source data object name <code>column</code> string Yes Column name in the data object <code>resultType</code> enum Yes Data type of the result (informative only, not used for SQL generation) <code>label</code> string No Display label <code>timeGrain</code> enum No Time grain: <code>year</code>, <code>quarter</code>, <code>month</code>, <code>week</code>, <code>day</code>, <code>hour</code>, <code>minute</code>, <code>second</code> <code>format</code> string No Display format"},{"location":"guide/model-format/#time-dimensions","title":"Time Dimensions","text":"<p>Set <code>timeGrain</code> to apply time grain truncation:</p> <pre><code>dimensions:\n  Order Month:\n    dataObject: Orders\n    column: Order Date\n    resultType: date\n    timeGrain: month\n</code></pre> <p>This generates <code>date_trunc('month', col)</code> in Postgres/Snowflake or <code>toStartOfMonth(col)</code> in ClickHouse.</p> <p>You can also apply time grain at query time using the <code>\"dimension:grain\"</code> syntax \u2014 see Query Language.</p>"},{"location":"guide/model-format/#measures","title":"Measures","text":"<p>A measure defines an aggregate computation over data object columns.</p>"},{"location":"guide/model-format/#simple-measure-single-column","title":"Simple Measure (single column)","text":"<pre><code>measures:\n  Order Count:\n    columns:\n      - dataObject: Orders\n        column: Order ID\n    resultType: int\n    aggregation: count\n</code></pre>"},{"location":"guide/model-format/#expression-measure-computed-from-columns","title":"Expression Measure (computed from columns)","text":"<p>Reference columns directly in the expression using <code>{[DataObject].[Column]}</code>:</p> <pre><code>measures:\n  Revenue:\n    resultType: float\n    aggregation: sum\n    expression: '{[Orders].[Price]} * {[Orders].[Quantity]}'\n</code></pre> <pre><code>measures:\n  Profit:\n    resultType: float\n    aggregation: sum\n    expression: '{[Sales].[Salesamount]} - {[Sales].[Salescosts]}'\n    total: true\n</code></pre>"},{"location":"guide/model-format/#measure-properties","title":"Measure Properties","text":"Property Type Required Description <code>columns</code> list No List of column references (<code>dataObject</code>+<code>column</code>) for simple single-column measures <code>resultType</code> enum Yes Data type of the result (informative only, not used for SQL generation) <code>aggregation</code> enum Yes <code>sum</code>, <code>count</code>, <code>count_distinct</code>, <code>avg</code>, <code>min</code>, <code>max</code>, <code>listagg</code> <code>expression</code> string No Expression with <code>{[DataObject].[Column]}</code> placeholders <code>distinct</code> bool No Apply DISTINCT to aggregation <code>total</code> bool No Use the total (unfiltered) value when referenced in a metric <code>delimiter</code> string No Separator for <code>listagg</code> aggregation (default: <code>\",\"</code>) <code>withinGroup</code> object No Ordering clause for <code>listagg</code> \u2014 specifies <code>column</code> and <code>order</code> (<code>ASC</code>/<code>DESC</code>) <code>filter</code> object No Conditional filter applied to this measure <code>allowFanOut</code> bool No Allow fan-out joins (default: false)"},{"location":"guide/model-format/#aggregation-types","title":"Aggregation Types","text":"Type SQL Example <code>sum</code> <code>SUM(expr)</code> Total revenue <code>count</code> <code>COUNT(expr)</code> Number of orders <code>count_distinct</code> <code>COUNT(DISTINCT expr)</code> Unique customers <code>avg</code> <code>AVG(expr)</code> Average price <code>min</code> <code>MIN(expr)</code> Earliest date <code>max</code> <code>MAX(expr)</code> Latest date <code>any_value</code> <code>ANY_VALUE(expr)</code> Any single value from the group (<code>any()</code> in ClickHouse) <code>median</code> <code>MEDIAN(expr)</code> Median value (<code>PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY ...)</code> in Postgres) <code>mode</code> <code>MODE(expr)</code> Most frequent value (<code>MODE() WITHIN GROUP (ORDER BY ...)</code> in Postgres, <code>topK(1)(col)[1]</code> in ClickHouse; not supported in Dremio) <code>listagg</code> <code>LISTAGG(expr, sep)</code> Concatenated values (dialect-specific: <code>STRING_AGG</code> in Postgres, <code>ARRAY_JOIN(COLLECT_LIST(...))</code> in Databricks, <code>arrayStringConcat(groupArray(...))</code> in ClickHouse)"},{"location":"guide/model-format/#expression-placeholders","title":"Expression Placeholders","text":"Placeholder Resolves to <code>{[DataObject].[Column]}</code> Column reference by data object and column name"},{"location":"guide/model-format/#measure-filters","title":"Measure Filters","text":"<p>Apply a filter to a measure so it only aggregates matching rows:</p> <pre><code>measures:\n  Sales Profit Ratio:\n    resultType: float\n    aggregation: sum\n    expression: '({[Sales].[Salesamount]} / {[Sales].[Salescosts]}) * 100'\n    filter:\n      column:\n        dataObject: Sales\n        column: Salescosts\n      operator: gt\n      values:\n        - dataType: float\n          valueFloat: 100.00\n</code></pre>"},{"location":"guide/model-format/#listagg-measures","title":"LISTAGG Measures","text":"<p>Use <code>listagg</code> to concatenate column values into a delimited string. OrionBelt renders the correct SQL for each database dialect automatically.</p> <pre><code>measures:\n  Product Names:\n    columns:\n      - dataObject: Products\n        column: Product Name\n    resultType: string\n    aggregation: listagg\n    delimiter: ', '\n    withinGroup:\n      column:\n        dataObject: Products\n        column: Product Name\n      order: ASC\n</code></pre> <p>The <code>delimiter</code> defaults to <code>\",\"</code> if omitted. The <code>withinGroup</code> clause is optional and specifies ordering of the concatenated values.</p>"},{"location":"guide/model-format/#metrics","title":"Metrics","text":"<p>A metric is a composite measure that combines multiple measures into a derived KPI. The expression references measures by name using <code>{[Measure Name]}</code> template syntax.</p> <pre><code>metrics:\n  Revenue per Order:\n    expression: '{[Revenue]} / {[Order Count]}'\n\n  Net Revenue:\n    expression: '{[Sales Amount]} - {[Return Amount]}'\n</code></pre> <p>All artefacts (data objects, dimensions, measures, metrics) have unique names. The <code>{[Name]}</code> placeholders in a metric expression must match existing measure names exactly.</p>"},{"location":"guide/model-format/#metric-properties","title":"Metric Properties","text":"Property Type Required Description <code>expression</code> string Yes Expression with <code>{[Measure Name]}</code> placeholders <code>label</code> string No Display label <code>format</code> string No Display format"},{"location":"guide/model-format/#metric-expression-placeholders","title":"Metric Expression Placeholders","text":"Placeholder Resolves to <code>{[Measure Name]}</code> Named reference to any defined measure"},{"location":"guide/model-format/#custom-extensions","title":"Custom Extensions","text":"<p>All six levels (model, data object, column, dimension, measure, metric) support an optional <code>customExtensions</code> array for vendor-specific metadata. OrionBelt preserves these during parsing and compilation but does not interpret them.</p> <pre><code>customExtensions:\n  - vendor: OSI\n    data: '{\"instructions\": \"Use for retail analytics\", \"synonyms\": [\"sales\"]}'\n  - vendor: GOVERNANCE\n    data: '{\"owner\": \"data-team\", \"classification\": \"internal\"}'\n</code></pre>"},{"location":"guide/model-format/#custom-extension-properties","title":"Custom Extension Properties","text":"Property Type Required Description <code>vendor</code> string Yes Vendor or format identifier (e.g. <code>OSI</code>, <code>GOVERNANCE</code>) <code>data</code> string Yes Opaque data payload (typically a JSON string) <p>Use cases:</p> <ul> <li>OSI interoperability: Preserving <code>ai_context</code> (instructions, synonyms, examples) from OSI models during conversion</li> <li>Governance tags: Owner, classification, cost center, lineage information</li> <li>Vendor-specific metadata: Any key-value data that OrionBelt should pass through without interpretation</li> </ul>"},{"location":"guide/model-format/#validation-rules","title":"Validation Rules","text":"<p>OrionBelt validates models against these rules:</p> <ol> <li>Unique identifiers \u2014 Column names unique within each data object; dimension, measure, and metric names unique across the model</li> <li>No cyclic joins \u2014 Join graph must be acyclic (secondary joins are excluded)</li> <li>No multipath joins \u2014 No ambiguous diamond patterns (secondary joins are excluded). A canonical join exception applies: when a data object has a direct join to a target AND also an indirect path through intermediaries, the direct join is treated as canonical and no error is raised. Only true diamonds (two indirect paths to the same target) are flagged.</li> <li>Secondary join constraints \u2014 Every secondary join must have a <code>pathName</code>; <code>pathName</code> must be unique per <code>(source, target)</code> pair</li> <li>Measures resolve \u2014 All column references in measures must point to existing data object columns</li> <li>Join targets exist \u2014 All <code>joinTo</code> targets must be defined data objects</li> <li>References resolve \u2014 All dimension references (dataObject/column) must resolve</li> </ol> <p>Validation errors include source positions (line/column) when available.</p>"},{"location":"guide/model-format/#full-example","title":"Full Example","text":"<p>See the Sales Model Walkthrough for a complete annotated example.</p>"},{"location":"guide/query-language/","title":"Query Language","text":"<p>OrionBelt uses a structured query object to express analytical queries against a semantic model. The query language selects dimensions and measures, applies filters, sorts results, and limits output \u2014 all using business names rather than raw SQL.</p>"},{"location":"guide/query-language/#query-object-structure","title":"Query Object Structure","text":"<pre><code>select:\n  dimensions:\n    - Customer Country\n    - \"Order Date:month\"      # with time grain\n  measures:\n    - Revenue\n    - Order Count\nwhere:\n  - field: Customer Segment\n    op: in\n    value: [SMB, MidMarket]\nhaving:\n  - field: Revenue\n    op: gt\n    value: 10000\norder_by:\n  - field: Revenue\n    direction: desc\nlimit: 1000\n</code></pre>"},{"location":"guide/query-language/#in-python","title":"In Python","text":"<pre><code>from orionbelt.models.query import (\n    QueryObject,\n    QuerySelect,\n    QueryFilter,\n    QueryOrderBy,\n    FilterOperator,\n    SortDirection,\n)\n\nquery = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Customer Country\", \"Order Date:month\"],\n        measures=[\"Revenue\", \"Order Count\"],\n    ),\n    where=[\n        QueryFilter(field=\"Customer Segment\", op=FilterOperator.IN, value=[\"SMB\", \"MidMarket\"]),\n    ],\n    having=[\n        QueryFilter(field=\"Revenue\", op=FilterOperator.GT, value=10000),\n    ],\n    order_by=[QueryOrderBy(field=\"Revenue\", direction=SortDirection.DESC)],\n    limit=1000,\n)\n</code></pre>"},{"location":"guide/query-language/#select","title":"Select","text":"<p>The <code>select</code> section specifies which dimensions and measures to include.</p>"},{"location":"guide/query-language/#dimensions","title":"Dimensions","text":"<p>Dimensions are referenced by name as defined in the semantic model. They become <code>GROUP BY</code> columns in the generated SQL.</p> <pre><code>select:\n  dimensions:\n    - Customer Country\n    - Product Category\n</code></pre>"},{"location":"guide/query-language/#time-grain-override","title":"Time Grain Override","text":"<p>Apply a time grain at query time using <code>\"dimension:grain\"</code> syntax:</p> <pre><code>select:\n  dimensions:\n    - \"Order Date:month\"     # truncate to month\n    - \"Order Date:quarter\"   # truncate to quarter\n    - \"Order Date:year\"      # truncate to year\n</code></pre> <p>Supported grains: <code>year</code>, <code>quarter</code>, <code>month</code>, <code>week</code>, <code>day</code>, <code>hour</code>, <code>minute</code>, <code>second</code>.</p> <p>This overrides any <code>timeGrain</code> set on the dimension definition.</p>"},{"location":"guide/query-language/#measures","title":"Measures","text":"<p>Measures are referenced by name. They can be simple aggregations, expression-based measures, or metrics.</p> <pre><code>select:\n  measures:\n    - Revenue\n    - Order Count\n    - Revenue per Order    # metric\n</code></pre>"},{"location":"guide/query-language/#secondary-join-paths","title":"Secondary Join Paths","text":"<p>When a model defines secondary joins (e.g., <code>Flights</code> \u2192 <code>Airports</code> via departure and arrival), use <code>usePathNames</code> to select which join path to use:</p> <pre><code>select:\n  dimensions:\n    - Airport Name\n  measures:\n    - Total Ticket Price\nusePathNames:\n  - source: Flights\n    target: Airports\n    pathName: arrival\n</code></pre> <p>Each entry specifies a <code>(source, target, pathName)</code> triple. The <code>pathName</code> must match a secondary join defined in the model. When active, the secondary join replaces the primary join for that pair.</p>"},{"location":"guide/query-language/#in-python_1","title":"In Python","text":"<pre><code>from orionbelt.models.query import QueryObject, QuerySelect, UsePathName\n\nquery = QueryObject(\n    select=QuerySelect(\n        dimensions=[\"Airport Name\"],\n        measures=[\"Total Ticket Price\"],\n    ),\n    use_path_names=[\n        UsePathName(source=\"Flights\", target=\"Airports\", path_name=\"arrival\"),\n    ],\n)\n</code></pre>"},{"location":"guide/query-language/#in-json-full-mode","title":"In JSON (full mode)","text":"<pre><code>{\n  \"select\": {\n    \"dimensions\": [\"Airport Name\"],\n    \"measures\": [\"Total Ticket Price\"]\n  },\n  \"usePathNames\": [\n    {\"source\": \"Flights\", \"target\": \"Airports\", \"pathName\": \"arrival\"}\n  ]\n}\n</code></pre> <p>If a <code>usePathNames</code> entry references a non-existent data object or pathName, the query will return a resolution error.</p>"},{"location":"guide/query-language/#filters","title":"Filters","text":"<p>Filters restrict the result set. Dimension filters go in <code>where</code> (become SQL <code>WHERE</code>), and measure filters go in <code>having</code> (become SQL <code>HAVING</code>).</p> <pre><code>where:\n  - field: Customer Country\n    op: equals\n    value: Germany\n\nhaving:\n  - field: Revenue\n    op: gte\n    value: 5000\n</code></pre>"},{"location":"guide/query-language/#filter-structure","title":"Filter Structure","text":"Property Type Description <code>field</code> string Dimension name (<code>where</code>) or measure name (<code>having</code>) <code>op</code> string Filter operator (see table below) <code>value</code> any Comparison value (string, number, list, etc.)"},{"location":"guide/query-language/#filter-reachability","title":"Filter Reachability","text":"<p>A <code>where</code> filter field must reference a dimension whose data object is reachable from the query's join graph. The data object can be:</p> <ul> <li>Directly joined in the query (base object or any object in the join path)</li> <li>A descendant \u2014 reachable via directed joins from any already-joined object</li> </ul> <p>If the data object is reachable but not yet in the join path, it is auto-joined automatically. If the data object is not reachable at all, the query returns an <code>UNREACHABLE_FILTER_FIELD</code> error.</p> <p>A <code>having</code> filter field must reference a measure name.</p>"},{"location":"guide/query-language/#filter-operators","title":"Filter Operators","text":"<p>OrionBelt supports two operator naming conventions \u2014 OBML style and SQL style. Both are equivalent.</p>"},{"location":"guide/query-language/#comparison-operators","title":"Comparison Operators","text":"OBML SQL Style SQL Output Value Type <code>equals</code> <code>=</code>, <code>eq</code> <code>= value</code> scalar <code>notequals</code> <code>!=</code>, <code>neq</code> <code>&lt;&gt; value</code> scalar <code>gt</code> <code>&gt;</code>, <code>greater</code> <code>&gt; value</code> scalar <code>gte</code> <code>&gt;=</code>, <code>greater_eq</code> <code>&gt;= value</code> scalar <code>lt</code> <code>&lt;</code>, <code>less</code> <code>&lt; value</code> scalar <code>lte</code> <code>&lt;=</code>, <code>less_eq</code> <code>&lt;= value</code> scalar"},{"location":"guide/query-language/#set-operators","title":"Set Operators","text":"OBML SQL Style SQL Output Value Type <code>inlist</code> <code>in</code> <code>IN (v1, v2, ...)</code> list <code>notinlist</code> <code>not_in</code> <code>NOT IN (v1, v2, ...)</code> list"},{"location":"guide/query-language/#null-operators","title":"Null Operators","text":"OBML SQL Style SQL Output Value Type <code>set</code> <code>is_not_null</code> <code>IS NOT NULL</code> none <code>notset</code> <code>is_null</code> <code>IS NULL</code> none"},{"location":"guide/query-language/#string-operators","title":"String Operators","text":"Operator SQL Output Value Type <code>contains</code> <code>LIKE '%value%'</code> (dialect-specific) string <code>notcontains</code> <code>NOT LIKE '%value%'</code> string <code>starts_with</code> <code>LIKE 'value%'</code> string <code>ends_with</code> <code>LIKE '%value'</code> string <code>like</code> <code>LIKE 'pattern'</code> string <code>notlike</code> <code>NOT LIKE 'pattern'</code> string"},{"location":"guide/query-language/#range-operators","title":"Range Operators","text":"Operator SQL Output Value Type <code>between</code> <code>BETWEEN low AND high</code> list of 2 <code>notbetween</code> <code>NOT BETWEEN low AND high</code> list of 2 <code>relative</code> Relative time range object <p>Relative filter object</p> <p>The <code>relative</code> operator expects an object with the following keys:</p> <ul> <li><code>unit</code>: one of <code>day</code>, <code>week</code>, <code>month</code>, <code>year</code></li> <li><code>count</code>: positive integer number of units</li> <li><code>direction</code> (optional): <code>past</code> (default) or <code>future</code></li> <li><code>include_current</code> (optional): boolean, default <code>true</code></li> </ul> <p>Example (last 7 days, inclusive of today):</p> <pre><code>where:\n  - field: Order Date\n    op: relative\n    value:\n      unit: day\n      count: 7\n      direction: past\n      include_current: true\n</code></pre> <p>String contains is dialect-aware</p> <p>The <code>contains</code> operator generates different SQL per dialect:</p> <ul> <li>Postgres/ClickHouse: <code>ILIKE '%' || value || '%'</code></li> <li>Snowflake: <code>CONTAINS(column, value)</code></li> <li>Dremio/Databricks: <code>LOWER(column) LIKE '%' || LOWER(value) || '%'</code></li> </ul>"},{"location":"guide/query-language/#ordering","title":"Ordering","text":"<p>Sort results by dimension or measure names from the query's SELECT, or by numeric position:</p> <pre><code>order_by:\n  - field: Revenue\n    direction: desc\n  - field: Customer Country\n    direction: asc       # default\n  - field: \"1\"           # numeric position (1-based)\n    direction: asc\n</code></pre> Property Type Default Description <code>field</code> string \u2014 Dimension or measure name from the query's SELECT, or a numeric position (e.g. <code>\"1\"</code>) <code>direction</code> enum <code>asc</code> <code>asc</code> or <code>desc</code> <p>The <code>field</code> must reference a dimension or measure that appears in the query's <code>select</code>. It cannot reference fields outside the SELECT. Alternatively, use a numeric string to reference the SELECT column by position (1-based).</p>"},{"location":"guide/query-language/#limit","title":"Limit","text":"<p>Restrict the number of returned rows:</p> <pre><code>limit: 1000\n</code></pre>"},{"location":"guide/query-language/#validation","title":"Validation","text":"<p>Invalid queries return error responses:</p> Error Code Status Cause <code>UNKNOWN_DIMENSION</code> 400 Dimension name not in model <code>UNKNOWN_MEASURE</code> 400 Measure name not in model <code>UNKNOWN_FILTER_FIELD</code> 400 Filter field is not a dimension (WHERE) or measure (HAVING) <code>UNREACHABLE_FILTER_FIELD</code> 400 Filter dimension's data object is not reachable from join graph <code>UNKNOWN_ORDER_BY_FIELD</code> 400 ORDER BY field not in query's SELECT <code>INVALID_ORDER_BY_POSITION</code> 400 Numeric ORDER BY position out of range <code>INVALID_FILTER_OPERATOR</code> 400 Unrecognized filter operator <code>INVALID_RELATIVE_FILTER</code> 400 Malformed relative time filter <code>AMBIGUOUS_JOIN</code> 422 Multiple join paths possible"},{"location":"guide/query-language/#semantics-summary","title":"Semantics Summary","text":"Query Element SQL Equivalent <code>select.dimensions</code> <code>SELECT</code> + <code>GROUP BY</code> columns <code>select.measures</code> <code>SELECT</code> aggregate expressions <code>where</code> <code>WHERE</code> clause <code>having</code> <code>HAVING</code> clause <code>order_by</code> <code>ORDER BY</code> clause <code>limit</code> <code>LIMIT</code> clause"},{"location":"reference/python-api/","title":"Python API Reference","text":"<p>Auto-generated documentation from source code docstrings.</p>"},{"location":"reference/python-api/#service-layer","title":"Service Layer","text":""},{"location":"reference/python-api/#modelstore","title":"ModelStore","text":""},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore","title":"<code>orionbelt.service.model_store.ModelStore</code>","text":"<p>In-memory model registry.  Thread-safe via <code>threading.Lock</code>.</p> <p>Models are keyed by short UUID (8-char hex).  All parsing, validation, and compilation infrastructure is instantiated internally, following the same singleton pattern as <code>api/deps.py</code>.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>class ModelStore:\n    \"\"\"In-memory model registry.  Thread-safe via ``threading.Lock``.\n\n    Models are keyed by short UUID (8-char hex).  All parsing, validation,\n    and compilation infrastructure is instantiated internally, following the\n    same singleton pattern as ``api/deps.py``.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._lock = threading.Lock()\n        self._models: dict[str, SemanticModel] = {}\n\n        # Internal pipeline singletons (stateless, safe to share).\n        self._loader = TrackedLoader()\n        self._resolver = ReferenceResolver()\n        self._validator = SemanticValidator()\n        self._pipeline = CompilationPipeline()\n\n    # -- helpers -------------------------------------------------------------\n\n    @staticmethod\n    def _new_id() -&gt; str:\n        return uuid.uuid4().hex[:8]\n\n    def _parse_and_validate(\n        self, yaml_str: str\n    ) -&gt; tuple[SemanticModel, list[ErrorInfo], list[ErrorInfo]]:\n        \"\"\"Parse YAML, resolve references, run semantic validation.\n\n        Returns ``(model, errors, warnings)``.\n        \"\"\"\n        errors: list[ErrorInfo] = []\n        warnings: list[ErrorInfo] = []\n\n        # 1. Parse YAML\n        try:\n            raw, source_map = self._loader.load_string(yaml_str)\n        except Exception as exc:\n            errors.append(ErrorInfo(code=\"YAML_PARSE_ERROR\", message=str(exc)))\n            return SemanticModel(), errors, warnings\n\n        # 2. Resolve references\n        model, resolution = self._resolver.resolve(raw, source_map)\n        for e in resolution.errors:\n            errors.append(\n                ErrorInfo(\n                    code=e.code,\n                    message=e.message,\n                    path=e.path,\n                    suggestions=list(e.suggestions),\n                )\n            )\n        for w in resolution.warnings:\n            warnings.append(\n                ErrorInfo(\n                    code=w.code,\n                    message=w.message,\n                    path=w.path,\n                    suggestions=list(w.suggestions),\n                )\n            )\n\n        # 3. Semantic validation\n        sem_errors = self._validator.validate(model)\n        for e in sem_errors:\n            errors.append(\n                ErrorInfo(\n                    code=e.code,\n                    message=e.message,\n                    path=e.path,\n                    suggestions=list(e.suggestions),\n                )\n            )\n\n        return model, errors, warnings\n\n    # -- public API ----------------------------------------------------------\n\n    def load_model(self, yaml_str: str) -&gt; LoadResult:\n        \"\"\"Parse, validate, and store a model.  Returns id + summary.\n\n        Raises ``ValueError`` if the model has validation errors.\n        \"\"\"\n        model, errors, warnings = self._parse_and_validate(yaml_str)\n        if errors:\n            msgs = \"; \".join(e.message for e in errors)\n            raise ValueError(f\"Model validation failed: {msgs}\")\n\n        model_id = self._new_id()\n        with self._lock:\n            self._models[model_id] = model\n\n        return LoadResult(\n            model_id=model_id,\n            data_objects=len(model.data_objects),\n            dimensions=len(model.dimensions),\n            measures=len(model.measures),\n            metrics=len(model.metrics),\n            warnings=[w.message for w in warnings],\n        )\n\n    def get_model(self, model_id: str) -&gt; SemanticModel:\n        \"\"\"Look up a loaded model.  Raises ``KeyError`` if not found.\"\"\"\n        with self._lock:\n            try:\n                return self._models[model_id]\n            except KeyError:\n                raise KeyError(f\"No model loaded with id '{model_id}'\") from None\n\n    def describe(self, model_id: str) -&gt; ModelDescription:\n        \"\"\"Return a structured summary suitable for LLM consumption.\"\"\"\n        model = self.get_model(model_id)\n\n        data_objects = [\n            DataObjectInfo(\n                label=obj.label,\n                code=obj.qualified_code,\n                columns=list(obj.columns.keys()),\n                join_targets=[j.join_to for j in obj.joins],\n            )\n            for obj in model.data_objects.values()\n        ]\n\n        dimensions = [\n            DimensionInfo(\n                name=dim.label,\n                result_type=dim.result_type.value,\n                data_object=dim.view,\n                column=dim.column,\n                time_grain=dim.time_grain.value if dim.time_grain else None,\n            )\n            for dim in model.dimensions.values()\n        ]\n\n        measures = [\n            MeasureInfo(\n                name=m.label,\n                result_type=m.result_type.value,\n                aggregation=m.aggregation,\n                expression=m.expression,\n            )\n            for m in model.measures.values()\n        ]\n\n        metrics = [\n            MetricInfo(name=met.label, expression=met.expression) for met in model.metrics.values()\n        ]\n\n        return ModelDescription(\n            model_id=model_id,\n            data_objects=data_objects,\n            dimensions=dimensions,\n            measures=measures,\n            metrics=metrics,\n        )\n\n    def list_models(self) -&gt; list[ModelSummary]:\n        \"\"\"Return a short summary for every loaded model.\"\"\"\n        with self._lock:\n            items = list(self._models.items())\n\n        return [\n            ModelSummary(\n                model_id=mid,\n                data_objects=len(m.data_objects),\n                dimensions=len(m.dimensions),\n                measures=len(m.measures),\n                metrics=len(m.metrics),\n            )\n            for mid, m in items\n        ]\n\n    def remove_model(self, model_id: str) -&gt; None:\n        \"\"\"Unload a model.  Raises ``KeyError`` if not found.\"\"\"\n        with self._lock:\n            try:\n                del self._models[model_id]\n            except KeyError:\n                raise KeyError(f\"No model loaded with id '{model_id}'\") from None\n\n    def compile_query(\n        self,\n        model_id: str,\n        query: QueryObject,\n        dialect: str,\n    ) -&gt; CompilationResult:\n        \"\"\"Compile a query against a loaded model.\"\"\"\n        model = self.get_model(model_id)\n        return self._pipeline.compile(query, model, dialect)\n\n    def validate(self, yaml_str: str) -&gt; ValidationSummary:\n        \"\"\"Validate a YAML model string without storing it.\"\"\"\n        _model, errors, warnings = self._parse_and_validate(yaml_str)\n        return ValidationSummary(\n            valid=len(errors) == 0,\n            errors=errors,\n            warnings=warnings,\n        )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.load_model","title":"<code>load_model(yaml_str)</code>","text":"<p>Parse, validate, and store a model.  Returns id + summary.</p> <p>Raises <code>ValueError</code> if the model has validation errors.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def load_model(self, yaml_str: str) -&gt; LoadResult:\n    \"\"\"Parse, validate, and store a model.  Returns id + summary.\n\n    Raises ``ValueError`` if the model has validation errors.\n    \"\"\"\n    model, errors, warnings = self._parse_and_validate(yaml_str)\n    if errors:\n        msgs = \"; \".join(e.message for e in errors)\n        raise ValueError(f\"Model validation failed: {msgs}\")\n\n    model_id = self._new_id()\n    with self._lock:\n        self._models[model_id] = model\n\n    return LoadResult(\n        model_id=model_id,\n        data_objects=len(model.data_objects),\n        dimensions=len(model.dimensions),\n        measures=len(model.measures),\n        metrics=len(model.metrics),\n        warnings=[w.message for w in warnings],\n    )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.get_model","title":"<code>get_model(model_id)</code>","text":"<p>Look up a loaded model.  Raises <code>KeyError</code> if not found.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def get_model(self, model_id: str) -&gt; SemanticModel:\n    \"\"\"Look up a loaded model.  Raises ``KeyError`` if not found.\"\"\"\n    with self._lock:\n        try:\n            return self._models[model_id]\n        except KeyError:\n            raise KeyError(f\"No model loaded with id '{model_id}'\") from None\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.describe","title":"<code>describe(model_id)</code>","text":"<p>Return a structured summary suitable for LLM consumption.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def describe(self, model_id: str) -&gt; ModelDescription:\n    \"\"\"Return a structured summary suitable for LLM consumption.\"\"\"\n    model = self.get_model(model_id)\n\n    data_objects = [\n        DataObjectInfo(\n            label=obj.label,\n            code=obj.qualified_code,\n            columns=list(obj.columns.keys()),\n            join_targets=[j.join_to for j in obj.joins],\n        )\n        for obj in model.data_objects.values()\n    ]\n\n    dimensions = [\n        DimensionInfo(\n            name=dim.label,\n            result_type=dim.result_type.value,\n            data_object=dim.view,\n            column=dim.column,\n            time_grain=dim.time_grain.value if dim.time_grain else None,\n        )\n        for dim in model.dimensions.values()\n    ]\n\n    measures = [\n        MeasureInfo(\n            name=m.label,\n            result_type=m.result_type.value,\n            aggregation=m.aggregation,\n            expression=m.expression,\n        )\n        for m in model.measures.values()\n    ]\n\n    metrics = [\n        MetricInfo(name=met.label, expression=met.expression) for met in model.metrics.values()\n    ]\n\n    return ModelDescription(\n        model_id=model_id,\n        data_objects=data_objects,\n        dimensions=dimensions,\n        measures=measures,\n        metrics=metrics,\n    )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.list_models","title":"<code>list_models()</code>","text":"<p>Return a short summary for every loaded model.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def list_models(self) -&gt; list[ModelSummary]:\n    \"\"\"Return a short summary for every loaded model.\"\"\"\n    with self._lock:\n        items = list(self._models.items())\n\n    return [\n        ModelSummary(\n            model_id=mid,\n            data_objects=len(m.data_objects),\n            dimensions=len(m.dimensions),\n            measures=len(m.measures),\n            metrics=len(m.metrics),\n        )\n        for mid, m in items\n    ]\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.remove_model","title":"<code>remove_model(model_id)</code>","text":"<p>Unload a model.  Raises <code>KeyError</code> if not found.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def remove_model(self, model_id: str) -&gt; None:\n    \"\"\"Unload a model.  Raises ``KeyError`` if not found.\"\"\"\n    with self._lock:\n        try:\n            del self._models[model_id]\n        except KeyError:\n            raise KeyError(f\"No model loaded with id '{model_id}'\") from None\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.compile_query","title":"<code>compile_query(model_id, query, dialect)</code>","text":"<p>Compile a query against a loaded model.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def compile_query(\n    self,\n    model_id: str,\n    query: QueryObject,\n    dialect: str,\n) -&gt; CompilationResult:\n    \"\"\"Compile a query against a loaded model.\"\"\"\n    model = self.get_model(model_id)\n    return self._pipeline.compile(query, model, dialect)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.model_store.ModelStore.validate","title":"<code>validate(yaml_str)</code>","text":"<p>Validate a YAML model string without storing it.</p> Source code in <code>src/orionbelt/service/model_store.py</code> <pre><code>def validate(self, yaml_str: str) -&gt; ValidationSummary:\n    \"\"\"Validate a YAML model string without storing it.\"\"\"\n    _model, errors, warnings = self._parse_and_validate(yaml_str)\n    return ValidationSummary(\n        valid=len(errors) == 0,\n        errors=errors,\n        warnings=warnings,\n    )\n</code></pre>"},{"location":"reference/python-api/#sessionmanager","title":"SessionManager","text":""},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager","title":"<code>orionbelt.service.session_manager.SessionManager</code>","text":"<p>Manages TTL-scoped sessions, each holding its own <code>ModelStore</code>.</p> <p>Thread-safe.  Call :meth:<code>start</code> to begin the background cleanup thread and :meth:<code>stop</code> to shut it down.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>class SessionManager:\n    \"\"\"Manages TTL-scoped sessions, each holding its own ``ModelStore``.\n\n    Thread-safe.  Call :meth:`start` to begin the background cleanup thread\n    and :meth:`stop` to shut it down.\n    \"\"\"\n\n    def __init__(self, ttl_seconds: int = 1800, cleanup_interval: int = 60) -&gt; None:\n        self._ttl = ttl_seconds\n        self._cleanup_interval = cleanup_interval\n        self._lock = threading.Lock()\n        self._sessions: dict[str, _Session] = {}\n        self._stop_event = threading.Event()\n        self._cleanup_thread: threading.Thread | None = None\n\n    # -- lifecycle -----------------------------------------------------------\n\n    def start(self) -&gt; None:\n        \"\"\"Start the background cleanup daemon thread.\"\"\"\n        if self._cleanup_thread is not None:\n            return\n        self._stop_event.clear()\n        self._cleanup_thread = threading.Thread(\n            target=self._cleanup_loop, daemon=True, name=\"session-cleanup\"\n        )\n        self._cleanup_thread.start()\n\n    def stop(self) -&gt; None:\n        \"\"\"Signal the cleanup thread to stop and wait for it.\"\"\"\n        self._stop_event.set()\n        if self._cleanup_thread is not None:\n            self._cleanup_thread.join(timeout=5)\n            self._cleanup_thread = None\n\n    # -- public API ----------------------------------------------------------\n\n    def create_session(self, metadata: dict[str, str] | None = None) -&gt; SessionInfo:\n        \"\"\"Create a new session and return its info.\"\"\"\n        session_id = secrets.token_hex(16)  # 32-char hex (128-bit)\n        now_mono = time.monotonic()\n        now_wall = datetime.now(UTC)\n        session = _Session(\n            session_id=session_id,\n            store=ModelStore(),\n            created_at=now_wall,\n            last_accessed=now_mono,\n            metadata=metadata or {},\n            created_at_wall=now_wall,\n            last_accessed_wall=now_wall,\n        )\n        with self._lock:\n            self._sessions[session_id] = session\n        return self._session_info(session)\n\n    def get_store(self, session_id: str) -&gt; ModelStore:\n        \"\"\"Get the ModelStore for a session, updating its last-accessed time.\n\n        Raises :class:`SessionNotFoundError` if the session is missing or expired.\n        \"\"\"\n        now_mono = time.monotonic()\n        with self._lock:\n            session = self._sessions.get(session_id)\n            if session is None:\n                raise SessionNotFoundError(f\"Session '{session_id}' not found\")\n            # Lazy expiration check\n            if now_mono - session.last_accessed &gt; self._ttl:\n                del self._sessions[session_id]\n                raise SessionNotFoundError(f\"Session '{session_id}' has expired\")\n            session.last_accessed = now_mono\n            session.last_accessed_wall = datetime.now(UTC)\n            return session.store\n\n    def get_session(self, session_id: str) -&gt; SessionInfo:\n        \"\"\"Get session info (also refreshes last-accessed).\"\"\"\n        now_mono = time.monotonic()\n        with self._lock:\n            session = self._sessions.get(session_id)\n            if session is None:\n                raise SessionNotFoundError(f\"Session '{session_id}' not found\")\n            if now_mono - session.last_accessed &gt; self._ttl:\n                del self._sessions[session_id]\n                raise SessionNotFoundError(f\"Session '{session_id}' has expired\")\n            session.last_accessed = now_mono\n            session.last_accessed_wall = datetime.now(UTC)\n            return self._session_info(session)\n\n    def close_session(self, session_id: str) -&gt; None:\n        \"\"\"Explicitly close a session.\"\"\"\n        with self._lock:\n            if session_id not in self._sessions:\n                raise SessionNotFoundError(f\"Session '{session_id}' not found\")\n            del self._sessions[session_id]\n\n    def list_sessions(self) -&gt; list[SessionInfo]:\n        \"\"\"Return info for all non-expired sessions (excluding default).\"\"\"\n        now_mono = time.monotonic()\n        result: list[SessionInfo] = []\n        with self._lock:\n            for session in self._sessions.values():\n                if session.session_id == _DEFAULT_SESSION_ID:\n                    continue\n                if now_mono - session.last_accessed &lt;= self._ttl:\n                    result.append(self._session_info(session))\n        return result\n\n    @property\n    def active_count(self) -&gt; int:\n        \"\"\"Number of active (non-expired) sessions.\"\"\"\n        now_mono = time.monotonic()\n        with self._lock:\n            return sum(\n                1 for s in self._sessions.values() if now_mono - s.last_accessed &lt;= self._ttl\n            )\n\n    def get_or_create_default(self) -&gt; ModelStore:\n        \"\"\"Get (or lazily create) the default session for stdio MCP.\"\"\"\n        with self._lock:\n            session = self._sessions.get(_DEFAULT_SESSION_ID)\n            if session is not None:\n                session.last_accessed = time.monotonic()\n                session.last_accessed_wall = datetime.now(UTC)\n                return session.store\n            now_wall = datetime.now(UTC)\n            session = _Session(\n                session_id=_DEFAULT_SESSION_ID,\n                store=ModelStore(),\n                created_at=now_wall,\n                last_accessed=time.monotonic(),\n                created_at_wall=now_wall,\n                last_accessed_wall=now_wall,\n            )\n            self._sessions[_DEFAULT_SESSION_ID] = session\n            return session.store\n\n    # -- internal ------------------------------------------------------------\n\n    @staticmethod\n    def _session_info(session: _Session) -&gt; SessionInfo:\n        return SessionInfo(\n            session_id=session.session_id,\n            created_at=session.created_at_wall,\n            last_accessed_at=session.last_accessed_wall,\n            model_count=len(session.store.list_models()),\n            metadata=session.metadata,\n        )\n\n    def _purge_expired(self) -&gt; None:\n        \"\"\"Remove all expired sessions (called by cleanup thread).\"\"\"\n        now_mono = time.monotonic()\n        with self._lock:\n            expired = [\n                sid for sid, s in self._sessions.items() if now_mono - s.last_accessed &gt; self._ttl\n            ]\n            for sid in expired:\n                del self._sessions[sid]\n\n    def _cleanup_loop(self) -&gt; None:\n        \"\"\"Background loop that periodically purges expired sessions.\"\"\"\n        while not self._stop_event.wait(timeout=self._cleanup_interval):\n            self._purge_expired()\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.active_count","title":"<code>active_count</code>  <code>property</code>","text":"<p>Number of active (non-expired) sessions.</p>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.start","title":"<code>start()</code>","text":"<p>Start the background cleanup daemon thread.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the background cleanup daemon thread.\"\"\"\n    if self._cleanup_thread is not None:\n        return\n    self._stop_event.clear()\n    self._cleanup_thread = threading.Thread(\n        target=self._cleanup_loop, daemon=True, name=\"session-cleanup\"\n    )\n    self._cleanup_thread.start()\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.stop","title":"<code>stop()</code>","text":"<p>Signal the cleanup thread to stop and wait for it.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Signal the cleanup thread to stop and wait for it.\"\"\"\n    self._stop_event.set()\n    if self._cleanup_thread is not None:\n        self._cleanup_thread.join(timeout=5)\n        self._cleanup_thread = None\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.create_session","title":"<code>create_session(metadata=None)</code>","text":"<p>Create a new session and return its info.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def create_session(self, metadata: dict[str, str] | None = None) -&gt; SessionInfo:\n    \"\"\"Create a new session and return its info.\"\"\"\n    session_id = secrets.token_hex(16)  # 32-char hex (128-bit)\n    now_mono = time.monotonic()\n    now_wall = datetime.now(UTC)\n    session = _Session(\n        session_id=session_id,\n        store=ModelStore(),\n        created_at=now_wall,\n        last_accessed=now_mono,\n        metadata=metadata or {},\n        created_at_wall=now_wall,\n        last_accessed_wall=now_wall,\n    )\n    with self._lock:\n        self._sessions[session_id] = session\n    return self._session_info(session)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.get_store","title":"<code>get_store(session_id)</code>","text":"<p>Get the ModelStore for a session, updating its last-accessed time.</p> <p>Raises :class:<code>SessionNotFoundError</code> if the session is missing or expired.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def get_store(self, session_id: str) -&gt; ModelStore:\n    \"\"\"Get the ModelStore for a session, updating its last-accessed time.\n\n    Raises :class:`SessionNotFoundError` if the session is missing or expired.\n    \"\"\"\n    now_mono = time.monotonic()\n    with self._lock:\n        session = self._sessions.get(session_id)\n        if session is None:\n            raise SessionNotFoundError(f\"Session '{session_id}' not found\")\n        # Lazy expiration check\n        if now_mono - session.last_accessed &gt; self._ttl:\n            del self._sessions[session_id]\n            raise SessionNotFoundError(f\"Session '{session_id}' has expired\")\n        session.last_accessed = now_mono\n        session.last_accessed_wall = datetime.now(UTC)\n        return session.store\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.get_session","title":"<code>get_session(session_id)</code>","text":"<p>Get session info (also refreshes last-accessed).</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def get_session(self, session_id: str) -&gt; SessionInfo:\n    \"\"\"Get session info (also refreshes last-accessed).\"\"\"\n    now_mono = time.monotonic()\n    with self._lock:\n        session = self._sessions.get(session_id)\n        if session is None:\n            raise SessionNotFoundError(f\"Session '{session_id}' not found\")\n        if now_mono - session.last_accessed &gt; self._ttl:\n            del self._sessions[session_id]\n            raise SessionNotFoundError(f\"Session '{session_id}' has expired\")\n        session.last_accessed = now_mono\n        session.last_accessed_wall = datetime.now(UTC)\n        return self._session_info(session)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.close_session","title":"<code>close_session(session_id)</code>","text":"<p>Explicitly close a session.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def close_session(self, session_id: str) -&gt; None:\n    \"\"\"Explicitly close a session.\"\"\"\n    with self._lock:\n        if session_id not in self._sessions:\n            raise SessionNotFoundError(f\"Session '{session_id}' not found\")\n        del self._sessions[session_id]\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.list_sessions","title":"<code>list_sessions()</code>","text":"<p>Return info for all non-expired sessions (excluding default).</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def list_sessions(self) -&gt; list[SessionInfo]:\n    \"\"\"Return info for all non-expired sessions (excluding default).\"\"\"\n    now_mono = time.monotonic()\n    result: list[SessionInfo] = []\n    with self._lock:\n        for session in self._sessions.values():\n            if session.session_id == _DEFAULT_SESSION_ID:\n                continue\n            if now_mono - session.last_accessed &lt;= self._ttl:\n                result.append(self._session_info(session))\n    return result\n</code></pre>"},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionManager.get_or_create_default","title":"<code>get_or_create_default()</code>","text":"<p>Get (or lazily create) the default session for stdio MCP.</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>def get_or_create_default(self) -&gt; ModelStore:\n    \"\"\"Get (or lazily create) the default session for stdio MCP.\"\"\"\n    with self._lock:\n        session = self._sessions.get(_DEFAULT_SESSION_ID)\n        if session is not None:\n            session.last_accessed = time.monotonic()\n            session.last_accessed_wall = datetime.now(UTC)\n            return session.store\n        now_wall = datetime.now(UTC)\n        session = _Session(\n            session_id=_DEFAULT_SESSION_ID,\n            store=ModelStore(),\n            created_at=now_wall,\n            last_accessed=time.monotonic(),\n            created_at_wall=now_wall,\n            last_accessed_wall=now_wall,\n        )\n        self._sessions[_DEFAULT_SESSION_ID] = session\n        return session.store\n</code></pre>"},{"location":"reference/python-api/#sessioninfo","title":"SessionInfo","text":""},{"location":"reference/python-api/#orionbelt.service.session_manager.SessionInfo","title":"<code>orionbelt.service.session_manager.SessionInfo</code>  <code>dataclass</code>","text":"<p>Public session metadata (returned by list/get).</p> Source code in <code>src/orionbelt/service/session_manager.py</code> <pre><code>@dataclass\nclass SessionInfo:\n    \"\"\"Public session metadata (returned by list/get).\"\"\"\n\n    session_id: str\n    created_at: datetime\n    last_accessed_at: datetime\n    model_count: int\n    metadata: dict[str, str]\n</code></pre>"},{"location":"reference/python-api/#compiler-pipeline","title":"Compiler Pipeline","text":""},{"location":"reference/python-api/#orionbelt.compiler.pipeline.CompilationPipeline","title":"<code>orionbelt.compiler.pipeline.CompilationPipeline</code>","text":"<p>Orchestrates: Query \u2192 Resolution \u2192 Planning \u2192 AST \u2192 SQL.</p> Source code in <code>src/orionbelt/compiler/pipeline.py</code> <pre><code>class CompilationPipeline:\n    \"\"\"Orchestrates: Query \u2192 Resolution \u2192 Planning \u2192 AST \u2192 SQL.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._resolver = QueryResolver()\n        self._star_planner = StarSchemaPlanner()\n        self._cfl_planner = CFLPlanner()\n\n    def compile(\n        self,\n        query: QueryObject,\n        model: SemanticModel,\n        dialect_name: str,\n    ) -&gt; CompilationResult:\n        \"\"\"Compile a query to SQL for the specified dialect.\"\"\"\n        # Phase 1: Resolution\n        resolved = self._resolver.resolve(query, model)\n\n        # Phase 1.5: Fanout detection (skip for CFL \u2014 each fact queried independently)\n        if not resolved.requires_cfl:\n            detect_fanout(resolved, model)\n\n        # Phase 2: Planning (star schema or CFL)\n        if resolved.requires_cfl:\n            plan = self._cfl_planner.plan(resolved, model)\n        else:\n            plan = self._star_planner.plan(resolved, model)\n\n        # Phase 2.5: Wrap with totals CTE if needed\n        wrapped_ast = wrap_with_totals(plan.ast, resolved)\n\n        # Phase 3: Dialect-specific SQL rendering\n        dialect = DialectRegistry.get(dialect_name)\n        codegen = CodeGenerator(dialect)\n        sql = codegen.generate(wrapped_ast)\n\n        # Phase 4: SQL validation (non-blocking)\n        validation_errors = validate_sql(sql, dialect_name)\n        sql_valid = len(validation_errors) == 0\n        warnings = resolved.warnings\n        if not sql_valid:\n            warnings = warnings + [f\"SQL validation: {e}\" for e in validation_errors]\n\n        return CompilationResult(\n            sql=sql,\n            dialect=dialect_name,\n            resolved=ResolvedInfo(\n                fact_tables=resolved.fact_tables,\n                dimensions=[d.name for d in resolved.dimensions],\n                measures=[m.name for m in resolved.measures],\n            ),\n            warnings=warnings,\n            sql_valid=sql_valid,\n        )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.pipeline.CompilationPipeline.compile","title":"<code>compile(query, model, dialect_name)</code>","text":"<p>Compile a query to SQL for the specified dialect.</p> Source code in <code>src/orionbelt/compiler/pipeline.py</code> <pre><code>def compile(\n    self,\n    query: QueryObject,\n    model: SemanticModel,\n    dialect_name: str,\n) -&gt; CompilationResult:\n    \"\"\"Compile a query to SQL for the specified dialect.\"\"\"\n    # Phase 1: Resolution\n    resolved = self._resolver.resolve(query, model)\n\n    # Phase 1.5: Fanout detection (skip for CFL \u2014 each fact queried independently)\n    if not resolved.requires_cfl:\n        detect_fanout(resolved, model)\n\n    # Phase 2: Planning (star schema or CFL)\n    if resolved.requires_cfl:\n        plan = self._cfl_planner.plan(resolved, model)\n    else:\n        plan = self._star_planner.plan(resolved, model)\n\n    # Phase 2.5: Wrap with totals CTE if needed\n    wrapped_ast = wrap_with_totals(plan.ast, resolved)\n\n    # Phase 3: Dialect-specific SQL rendering\n    dialect = DialectRegistry.get(dialect_name)\n    codegen = CodeGenerator(dialect)\n    sql = codegen.generate(wrapped_ast)\n\n    # Phase 4: SQL validation (non-blocking)\n    validation_errors = validate_sql(sql, dialect_name)\n    sql_valid = len(validation_errors) == 0\n    warnings = resolved.warnings\n    if not sql_valid:\n        warnings = warnings + [f\"SQL validation: {e}\" for e in validation_errors]\n\n    return CompilationResult(\n        sql=sql,\n        dialect=dialect_name,\n        resolved=ResolvedInfo(\n            fact_tables=resolved.fact_tables,\n            dimensions=[d.name for d in resolved.dimensions],\n            measures=[m.name for m in resolved.measures],\n        ),\n        warnings=warnings,\n        sql_valid=sql_valid,\n    )\n</code></pre>"},{"location":"reference/python-api/#query-resolution","title":"Query Resolution","text":""},{"location":"reference/python-api/#orionbelt.compiler.resolution.QueryResolver","title":"<code>orionbelt.compiler.resolution.QueryResolver</code>","text":"<p>Resolves a QueryObject + SemanticModel into a ResolvedQuery.</p> Source code in <code>src/orionbelt/compiler/resolution.py</code> <pre><code>class QueryResolver:\n    \"\"\"Resolves a QueryObject + SemanticModel into a ResolvedQuery.\"\"\"\n\n    def resolve(self, query: QueryObject, model: SemanticModel) -&gt; ResolvedQuery:\n        errors: list[SemanticError] = []\n        result = ResolvedQuery(\n            limit=query.limit,\n            use_path_names=list(query.use_path_names),\n        )\n\n        # Build global column lookup: col_name \u2192 (object_name, source_column)\n        global_columns: dict[str, tuple[str, str]] = {}\n        for obj_name, obj in model.data_objects.items():\n            for col_name, col_obj in obj.columns.items():\n                global_columns[col_name] = (obj_name, col_obj.code)\n\n        # 1. Resolve dimensions\n        for dim_str in query.select.dimensions:\n            dim_ref = DimensionRef.parse(dim_str)\n            resolved_dim = self._resolve_dimension(dim_ref, model, errors)\n            if resolved_dim:\n                result.dimensions.append(resolved_dim)\n                result.required_objects.add(resolved_dim.object_name)\n\n        # 2. Resolve measures and track their source objects\n        for measure_name in query.select.measures:\n            resolved_meas = self._resolve_measure(\n                measure_name, model, global_columns, errors, result\n            )\n            if resolved_meas:\n                result.measures.append(resolved_meas)\n                # Collect all source objects for this measure/metric\n                source_objs = self._get_measure_source_objects(measure_name, model, global_columns)\n                result.measure_source_objects.update(source_objs)\n                result.required_objects.update(source_objs)\n\n        # 3. Determine base object (the one with most joins / most measures)\n        result.base_object = self._select_base_object(result, model)\n        if result.base_object:\n            result.required_objects.add(result.base_object)\n\n        # Detect multi-fact: CFL is needed only when measure source objects\n        # span multiple independent fact tables.  If all measure sources are\n        # reachable from the base object via directed join paths (i.e. they\n        # are dimension tables joined from the same fact), a single star\n        # schema query suffices.\n        if len(result.measure_source_objects) &gt; 1:\n            graph = JoinGraph(model, use_path_names=query.use_path_names or None)\n            reachable = graph.descendants(result.base_object)\n            unreachable = result.measure_source_objects - reachable - {result.base_object}\n            if unreachable:\n                result.requires_cfl = True\n\n        # 4. Validate usePathNames before building join graph\n        for upn in query.use_path_names:\n            if upn.source not in model.data_objects:\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_DATA_OBJECT\",\n                        message=(f\"usePathNames references unknown data object '{upn.source}'\"),\n                        path=\"usePathNames\",\n                    )\n                )\n                continue\n            if upn.target not in model.data_objects:\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_DATA_OBJECT\",\n                        message=(f\"usePathNames references unknown data object '{upn.target}'\"),\n                        path=\"usePathNames\",\n                    )\n                )\n                continue\n            # Check that a secondary join with this pathName exists for the pair\n            source_obj = model.data_objects[upn.source]\n            found = False\n            for join in source_obj.joins:\n                if (\n                    join.join_to == upn.target\n                    and join.secondary\n                    and join.path_name == upn.path_name\n                ):\n                    found = True\n                    break\n            if not found:\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_PATH_NAME\",\n                        message=(\n                            f\"No secondary join with pathName '{upn.path_name}' \"\n                            f\"from '{upn.source}' to '{upn.target}'\"\n                        ),\n                        path=\"usePathNames\",\n                    )\n                )\n\n        # 5. Resolve join paths\n        graph = JoinGraph(model, use_path_names=query.use_path_names or None)\n        if result.base_object and len(result.required_objects) &gt; 1:\n            result.join_steps = graph.find_join_path({result.base_object}, result.required_objects)\n\n        # Build set of all objects present in the query's join graph\n        joined_objects: set[str] = set()\n        if result.base_object:\n            joined_objects.add(result.base_object)\n        for step in result.join_steps:\n            joined_objects.add(step.to_object)\n\n        # 6. Classify filters \u2014 filters may auto-extend the join path\n        for qf in query.where:\n            resolved_filter = self._resolve_filter(\n                qf,\n                model,\n                is_having=False,\n                errors=errors,\n                joined_objects=joined_objects,\n                graph=graph,\n                result=result,\n            )\n            if resolved_filter:\n                result.where_filters.append(resolved_filter)\n\n        for qf in query.having:\n            resolved_filter = self._resolve_filter(\n                qf,\n                model,\n                is_having=True,\n                errors=errors,\n                joined_objects=joined_objects,\n                graph=graph,\n                result=result,\n            )\n            if resolved_filter:\n                result.having_filters.append(resolved_filter)\n\n        # 7. Resolve order by \u2014 must reference a dimension or measure in SELECT\n        select_count = len(result.dimensions) + len(result.measures)\n        for ob in query.order_by:\n            expr = self._resolve_order_by_field(\n                ob.field,\n                result,\n                errors=errors,\n                select_count=select_count,\n            )\n            if expr:\n                result.order_by_exprs.append((expr, ob.direction == \"desc\"))\n\n        if errors:\n            raise ResolutionError(errors)\n\n        return result\n\n    def _resolve_dimension(\n        self,\n        ref: DimensionRef,\n        model: SemanticModel,\n        errors: list[SemanticError],\n    ) -&gt; ResolvedDimension | None:\n        \"\"\"Resolve a dimension reference to its physical column.\"\"\"\n        dim = model.dimensions.get(ref.name)\n        if dim is None:\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_DIMENSION\",\n                    message=f\"Unknown dimension '{ref.name}'\",\n                    path=\"select.dimensions\",\n                )\n            )\n            return None\n\n        obj_name = dim.view\n        col_name = dim.column\n        obj = model.data_objects.get(obj_name)\n        if obj is None:\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_DATA_OBJECT\",\n                    message=f\"Dimension '{ref.name}' references unknown data object '{obj_name}'\",\n                )\n            )\n            return None\n\n        vf = obj.columns.get(col_name)\n        source_col = vf.code if vf else col_name\n\n        return ResolvedDimension(\n            name=ref.name,\n            object_name=obj_name,\n            column_name=col_name,\n            source_column=source_col,\n            grain=ref.grain or dim.time_grain,\n        )\n\n    def _resolve_measure(\n        self,\n        name: str,\n        model: SemanticModel,\n        global_columns: dict[str, tuple[str, str]],\n        errors: list[SemanticError],\n        result: ResolvedQuery | None = None,\n    ) -&gt; ResolvedMeasure | None:\n        \"\"\"Resolve a measure name to its aggregate expression.\"\"\"\n        measure = model.measures.get(name)\n        if measure is None:\n            # Check metrics\n            metric = model.metrics.get(name)\n            if metric:\n                return self._resolve_metric(name, metric, model, global_columns, errors, result)\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_MEASURE\",\n                    message=f\"Unknown measure '{name}'\",\n                    path=\"select.measures\",\n                )\n            )\n            return None\n\n        expr = self._build_measure_expr(measure.label, measure, model, global_columns)\n        return ResolvedMeasure(\n            name=name,\n            aggregation=measure.aggregation,\n            expression=expr,\n            is_expression=measure.expression is not None,\n            total=measure.total,\n        )\n\n    def _build_measure_expr(\n        self,\n        name: str,\n        measure: Measure,\n        model: SemanticModel,\n        global_columns: dict[str, tuple[str, str]],\n    ) -&gt; Expr:\n        \"\"\"Build the aggregate expression for a measure.\"\"\"\n        if measure.expression:\n            return self._expand_expression(measure, model, global_columns)\n\n        # Build column references for all columns\n        args: list[Expr] = []\n        if measure.columns:\n            for ref in measure.columns:\n                obj_name = ref.view or \"\"\n                col_name = ref.column or \"\"\n                obj = model.data_objects.get(obj_name)\n                source = obj.columns[col_name].code if obj and col_name in obj.columns else col_name\n                args.append(ColumnRef(name=source, table=obj_name))\n        if not args:\n            args = [Literal.number(1)]\n\n        agg = measure.aggregation.upper()\n        distinct = measure.distinct\n        if agg == \"COUNT_DISTINCT\":\n            agg = \"COUNT\"\n            distinct = True\n\n        # LISTAGG: attach separator and optional ordering\n        separator: str | None = None\n        order_by: list[OrderByItem] = []\n        if agg == \"LISTAGG\":\n            separator = measure.delimiter if measure.delimiter is not None else \",\"\n            if measure.within_group:\n                wg = measure.within_group\n                wg_obj_name = wg.column.view or \"\"\n                wg_col_name = wg.column.column or \"\"\n                wg_obj = model.data_objects.get(wg_obj_name)\n                wg_source = (\n                    wg_obj.columns[wg_col_name].code\n                    if wg_obj and wg_col_name in wg_obj.columns\n                    else wg_col_name\n                )\n                order_by = [\n                    OrderByItem(\n                        expr=ColumnRef(name=wg_source, table=wg_obj_name),\n                        desc=wg.order.upper() == \"DESC\",\n                    )\n                ]\n\n        return FunctionCall(\n            name=agg,\n            args=args,\n            distinct=distinct,\n            order_by=order_by,\n            separator=separator,\n        )\n\n    def _expand_expression(\n        self,\n        measure: Measure,\n        model: SemanticModel,\n        global_columns: dict[str, tuple[str, str]],\n    ) -&gt; Expr:\n        \"\"\"Expand a measure expression into AST nodes.\n\n        Handles {[DataObject].[Column]} placeholders.\n        \"\"\"\n\n        formula = measure.expression or \"\"\n        agg = measure.aggregation.upper()\n\n        # Replace {[DataObject].[Column]} with column references\n        named_refs = re.findall(r\"\\{\\[([^\\]]+)\\]\\.\\[([^\\]]+)\\]\\}\", formula)\n        for obj_name, col_name in named_refs:\n            obj = model.data_objects.get(obj_name)\n            if obj and col_name in obj.columns:\n                source = obj.columns[col_name].code\n                formula = formula.replace(f\"{{[{obj_name}].[{col_name}]}}\", f\"{obj_name}.{source}\")\n\n        # Wrap the whole formula in the aggregation function as raw SQL\n        from orionbelt.ast.nodes import RawSQL\n\n        distinct = measure.distinct\n        if agg == \"COUNT_DISTINCT\":\n            agg = \"COUNT\"\n            distinct = True\n\n        return FunctionCall(\n            name=agg,\n            args=[RawSQL(sql=formula)],\n            distinct=distinct,\n        )\n\n    def _resolve_metric(\n        self,\n        name: str,\n        metric: Metric,\n        model: SemanticModel,\n        global_columns: dict[str, tuple[str, str]],\n        errors: list[SemanticError],\n        result: ResolvedQuery | None = None,\n    ) -&gt; ResolvedMeasure | None:\n        \"\"\"Resolve a metric to its combined expression.\n\n        Parses the formula into a proper AST tree and resolves each\n        component measure so that planners can substitute them later.\n        \"\"\"\n        formula = metric.expression\n\n        # Extract and resolve each component measure\n        component_names = re.findall(r\"\\{\\[([^\\]]+)\\]\\}\", formula)\n        for comp_name in component_names:\n            if result is not None and comp_name not in result.metric_components:\n                comp = self._resolve_measure(comp_name, model, global_columns, errors, result)\n                if comp:\n                    result.metric_components[comp_name] = comp\n\n        # Parse the formula into an AST tree\n        try:\n            tokens = _tokenize_formula(formula)\n            parsed_expr = _parse_metric_formula(tokens)\n        except Exception as exc:\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_METRIC_EXPRESSION\",\n                    message=f\"Metric '{name}' has invalid expression: {exc}\",\n                    path=f\"metrics.{name}.expression\",\n                )\n            )\n            return None\n\n        return ResolvedMeasure(\n            name=name,\n            aggregation=\"\",\n            expression=parsed_expr,\n            component_measures=component_names,\n            is_expression=True,\n        )\n\n    def _get_measure_source_objects(\n        self,\n        name: str,\n        model: SemanticModel,\n        global_columns: dict[str, tuple[str, str]],\n    ) -&gt; set[str]:\n        \"\"\"Extract all source data objects for a measure or metric.\"\"\"\n        result: set[str] = set()\n\n        # Check simple measures first\n        measure = model.measures.get(name)\n        if measure:\n            # Columns-based measure\n            for cref in measure.columns:\n                if cref.view:\n                    result.add(cref.view)\n            # Expression-based measure: extract {[DataObject].[Column]} references\n            if measure.expression:\n                col_refs = re.findall(r\"\\{\\[([^\\]]+)\\]\\.\\[([^\\]]+)\\]\\}\", measure.expression)\n                for obj_name, _col_name in col_refs:\n                    result.add(obj_name)\n            return result\n\n        # Check metrics: {[Measure Name]} references \u2192 recurse\n        metric = model.metrics.get(name)\n        if metric:\n            measure_refs = re.findall(r\"\\{\\[([^\\]]+)\\]\\}\", metric.expression)\n            for ref_name in measure_refs:\n                result.update(self._get_measure_source_objects(ref_name, model, global_columns))\n\n        return result\n\n    def _select_base_object(self, result: ResolvedQuery, model: SemanticModel) -&gt; str:\n        \"\"\"Select the base (fact) object \u2014 prefer measure source objects with most joins.\"\"\"\n        # Priority 1: measure source objects (true fact tables) \u2014 pick the one with most joins\n        if result.measure_source_objects:\n            best = \"\"\n            best_joins = -1\n            for obj_name in sorted(result.measure_source_objects):\n                obj = model.data_objects.get(obj_name)\n                n = len(obj.joins) if obj else 0\n                if n &gt; best_joins:\n                    best = obj_name\n                    best_joins = n\n            if best:\n                return best\n\n        # Priority 2: any required object with joins defined\n        for obj_name in sorted(result.required_objects):\n            obj = model.data_objects.get(obj_name)\n            if obj and obj.joins:\n                return obj_name\n\n        # Fallback: first required object\n        if result.required_objects:\n            return next(iter(sorted(result.required_objects)))\n        if model.data_objects:\n            return next(iter(model.data_objects))\n        return \"\"\n\n    def _resolve_filter(\n        self,\n        qf: QueryFilter,\n        model: SemanticModel,\n        is_having: bool,\n        errors: list[SemanticError],\n        joined_objects: set[str] | None = None,\n        graph: JoinGraph | None = None,\n        result: ResolvedQuery | None = None,\n    ) -&gt; ResolvedFilter | None:\n        \"\"\"Resolve a query filter to a physical expression.\n\n        Filter fields can reference any dimension whose data object is\n        reachable from the query's join graph (including descendants).\n        If the object is reachable but not yet joined, the join path is\n        auto-extended.\n        \"\"\"\n        filter_path = \"having\" if is_having else \"where\"\n\n        # Try to find the field in dimensions first\n        dim = model.dimensions.get(qf.field)\n        if dim:\n            obj_name = dim.view\n            # Check reachability: in join graph or reachable via descendants\n            if joined_objects is not None and obj_name not in joined_objects:\n                # Check if reachable via directed joins from any joined object\n                reachable = False\n                if graph is not None:\n                    for joined_obj in list(joined_objects):\n                        if obj_name in graph.descendants(joined_obj):\n                            reachable = True\n                            break\n                if not reachable:\n                    errors.append(\n                        SemanticError(\n                            code=\"UNREACHABLE_FILTER_FIELD\",\n                            message=(\n                                f\"Filter field '{qf.field}' references data object \"\n                                f\"'{obj_name}' which is not reachable from \"\n                                f\"the query's join graph\"\n                            ),\n                            path=filter_path,\n                        )\n                    )\n                    return None\n                # Auto-extend the join path to include this object\n                if graph is not None and result is not None:\n                    new_steps = graph.find_join_path(joined_objects, {obj_name})\n                    for step in new_steps:\n                        if step.to_object not in joined_objects:\n                            result.join_steps.append(step)\n                            joined_objects.add(step.to_object)\n                            result.required_objects.add(step.to_object)\n\n            col_name = dim.column\n            obj = model.data_objects.get(obj_name)\n            source = obj.columns[col_name].code if obj and col_name in obj.columns else col_name\n            col_expr: Expr = ColumnRef(name=source, table=obj_name)\n        elif is_having and qf.field in model.measures:\n            # HAVING filter on a measure \u2014 valid\n            col_expr = ColumnRef(name=qf.field)\n        else:\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_FILTER_FIELD\",\n                    message=f\"Unknown filter field '{qf.field}'\",\n                    path=filter_path,\n                )\n            )\n            return None\n\n        filter_expr = self._build_filter_expr(col_expr, qf, errors)\n        if filter_expr is None:\n            return None\n        return ResolvedFilter(expression=filter_expr, is_aggregate=is_having)\n\n    def _build_filter_expr(\n        self, col: Expr, qf: QueryFilter, errors: list[SemanticError]\n    ) -&gt; Expr | None:\n        \"\"\"Build a filter expression from operator and value.\"\"\"\n        from orionbelt.ast.nodes import InList, IsNull\n\n        op = qf.op\n        val = qf.value\n\n        match op:\n            case FilterOperator.EQUALS | FilterOperator.EQ:\n                return BinaryOp(left=col, op=\"=\", right=Literal(value=val))\n            case FilterOperator.NOT_EQUALS | FilterOperator.NEQ:\n                return BinaryOp(left=col, op=\"&lt;&gt;\", right=Literal(value=val))\n            case FilterOperator.GT | FilterOperator.GREATER:\n                return BinaryOp(left=col, op=\"&gt;\", right=Literal(value=val))\n            case FilterOperator.GTE | FilterOperator.GREATER_EQ:\n                return BinaryOp(left=col, op=\"&gt;=\", right=Literal(value=val))\n            case FilterOperator.LT | FilterOperator.LESS:\n                return BinaryOp(left=col, op=\"&lt;\", right=Literal(value=val))\n            case FilterOperator.LTE | FilterOperator.LESS_EQ:\n                return BinaryOp(left=col, op=\"&lt;=\", right=Literal(value=val))\n            case FilterOperator.IN_LIST | FilterOperator.IN:\n                vals: list[Expr] = (\n                    [Literal(value=v) for v in val]\n                    if isinstance(val, list)\n                    else [Literal(value=val)]\n                )\n                return InList(expr=col, values=vals)\n            case FilterOperator.NOT_IN_LIST | FilterOperator.NOT_IN:\n                not_vals: list[Expr] = (\n                    [Literal(value=v) for v in val]\n                    if isinstance(val, list)\n                    else [Literal(value=val)]\n                )\n                return InList(expr=col, values=not_vals, negated=True)\n            case FilterOperator.SET | FilterOperator.IS_NOT_NULL:\n                return IsNull(expr=col, negated=True)\n            case FilterOperator.NOT_SET | FilterOperator.IS_NULL:\n                return IsNull(expr=col, negated=False)\n            case FilterOperator.CONTAINS:\n                return BinaryOp(\n                    left=col,\n                    op=\"LIKE\",\n                    right=Literal.string(f\"%{val}%\"),\n                )\n            case FilterOperator.NOT_CONTAINS:\n                return BinaryOp(\n                    left=col,\n                    op=\"NOT LIKE\",\n                    right=Literal.string(f\"%{val}%\"),\n                )\n            case FilterOperator.STARTS_WITH:\n                return BinaryOp(\n                    left=col,\n                    op=\"LIKE\",\n                    right=Literal.string(f\"{val}%\"),\n                )\n            case FilterOperator.ENDS_WITH:\n                return BinaryOp(\n                    left=col,\n                    op=\"LIKE\",\n                    right=Literal.string(f\"%{val}\"),\n                )\n            case FilterOperator.LIKE:\n                return BinaryOp(left=col, op=\"LIKE\", right=Literal.string(str(val)))\n            case FilterOperator.NOT_LIKE:\n                return BinaryOp(left=col, op=\"NOT LIKE\", right=Literal.string(str(val)))\n            case FilterOperator.BETWEEN:\n                from orionbelt.ast.nodes import Between\n\n                if isinstance(val, list) and len(val) &gt;= 2:\n                    return Between(\n                        expr=col,\n                        low=Literal(value=val[0]),\n                        high=Literal(value=val[1]),\n                    )\n                return BinaryOp(left=col, op=\"=\", right=Literal(value=val))\n            case FilterOperator.NOT_BETWEEN:\n                from orionbelt.ast.nodes import Between\n\n                if isinstance(val, list) and len(val) &gt;= 2:\n                    return Between(\n                        expr=col,\n                        low=Literal(value=val[0]),\n                        high=Literal(value=val[1]),\n                        negated=True,\n                    )\n                return BinaryOp(left=col, op=\"&lt;&gt;\", right=Literal(value=val))\n            case FilterOperator.RELATIVE:\n                relative = self._parse_relative_filter(val, errors, field=qf.field)\n                if relative is None:\n                    return None\n                return RelativeDateRange(\n                    column=col,\n                    unit=relative[\"unit\"],\n                    count=relative[\"count\"],\n                    direction=relative[\"direction\"],\n                    include_current=relative[\"include_current\"],\n                )\n            case _:\n                errors.append(\n                    SemanticError(\n                        code=\"INVALID_FILTER_OPERATOR\",\n                        message=f\"Unsupported filter operator '{op}'\",\n                        path=\"filters\",\n                    )\n                )\n                return None\n\n    def _parse_relative_filter(\n        self, value: object, errors: list[SemanticError], field: str\n    ) -&gt; _RelativeFilterParsed | None:\n        if not isinstance(value, dict):\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_RELATIVE_FILTER\",\n                    message=(\n                        f\"Relative filter for '{field}' must be an object \"\n                        \"with keys {unit, count, direction?, include_current?}\"\n                    ),\n                    path=\"filters\",\n                )\n            )\n            return None\n\n        unit = value.get(\"unit\")\n        count = value.get(\"count\")\n        direction = value.get(\"direction\", \"past\")\n        include_current = value.get(\"include_current\", value.get(\"includeCurrent\", True))\n\n        if not isinstance(unit, str):\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_RELATIVE_FILTER\",\n                    message=f\"Relative filter for '{field}' requires string 'unit'\",\n                    path=\"filters\",\n                )\n            )\n            return None\n        unit = unit.lower()\n        if unit not in {\"day\", \"week\", \"month\", \"year\"}:\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_RELATIVE_FILTER\",\n                    message=f\"Relative filter for '{field}' has unsupported unit '{unit}'\",\n                    path=\"filters\",\n                )\n            )\n            return None\n        if not isinstance(count, int) or count &lt;= 0:\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_RELATIVE_FILTER\",\n                    message=f\"Relative filter for '{field}' requires positive integer 'count'\",\n                    path=\"filters\",\n                )\n            )\n            return None\n        if direction not in {\"past\", \"future\"}:\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_RELATIVE_FILTER\",\n                    message=(f\"Relative filter for '{field}' has invalid direction '{direction}'\"),\n                    path=\"filters\",\n                )\n            )\n            return None\n        if not isinstance(include_current, bool):\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_RELATIVE_FILTER\",\n                    message=(f\"Relative filter for '{field}' has non-boolean include_current\"),\n                    path=\"filters\",\n                )\n            )\n            return None\n\n        return {\n            \"unit\": unit,\n            \"count\": count,\n            \"direction\": direction,\n            \"include_current\": include_current,\n        }\n\n    def _resolve_order_by_field(\n        self,\n        field_name: str,\n        result: ResolvedQuery,\n        errors: list[SemanticError],\n        select_count: int,\n    ) -&gt; Expr | None:\n        \"\"\"Resolve an order-by field to its expression.\n\n        Must reference a dimension or measure already in the query's SELECT,\n        or be a numeric positional reference (1-based).\n        \"\"\"\n        # Check if it's a resolved dimension in SELECT\n        for dim in result.dimensions:\n            if dim.name == field_name:\n                return ColumnRef(name=dim.source_column, table=dim.object_name)\n\n        # Check if it's a resolved measure in SELECT\n        for meas in result.measures:\n            if meas.name == field_name:\n                return meas.expression\n\n        # Check if it's a numeric positional reference (e.g. \"1\", \"2\")\n        if field_name.isdigit():\n            pos = int(field_name)\n            if 1 &lt;= pos &lt;= select_count:\n                return Literal.number(pos)\n            errors.append(\n                SemanticError(\n                    code=\"INVALID_ORDER_BY_POSITION\",\n                    message=(\n                        f\"ORDER BY position {pos} is out of range \"\n                        f\"(SELECT has {select_count} columns)\"\n                    ),\n                    path=\"order_by\",\n                )\n            )\n            return None\n\n        errors.append(\n            SemanticError(\n                code=\"UNKNOWN_ORDER_BY_FIELD\",\n                message=(\n                    f\"ORDER BY field '{field_name}' is not a dimension \"\n                    f\"or measure in the query's SELECT\"\n                ),\n                path=\"order_by\",\n            )\n        )\n        return None\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.resolution.QueryResolver.resolve","title":"<code>resolve(query, model)</code>","text":"Source code in <code>src/orionbelt/compiler/resolution.py</code> <pre><code>def resolve(self, query: QueryObject, model: SemanticModel) -&gt; ResolvedQuery:\n    errors: list[SemanticError] = []\n    result = ResolvedQuery(\n        limit=query.limit,\n        use_path_names=list(query.use_path_names),\n    )\n\n    # Build global column lookup: col_name \u2192 (object_name, source_column)\n    global_columns: dict[str, tuple[str, str]] = {}\n    for obj_name, obj in model.data_objects.items():\n        for col_name, col_obj in obj.columns.items():\n            global_columns[col_name] = (obj_name, col_obj.code)\n\n    # 1. Resolve dimensions\n    for dim_str in query.select.dimensions:\n        dim_ref = DimensionRef.parse(dim_str)\n        resolved_dim = self._resolve_dimension(dim_ref, model, errors)\n        if resolved_dim:\n            result.dimensions.append(resolved_dim)\n            result.required_objects.add(resolved_dim.object_name)\n\n    # 2. Resolve measures and track their source objects\n    for measure_name in query.select.measures:\n        resolved_meas = self._resolve_measure(\n            measure_name, model, global_columns, errors, result\n        )\n        if resolved_meas:\n            result.measures.append(resolved_meas)\n            # Collect all source objects for this measure/metric\n            source_objs = self._get_measure_source_objects(measure_name, model, global_columns)\n            result.measure_source_objects.update(source_objs)\n            result.required_objects.update(source_objs)\n\n    # 3. Determine base object (the one with most joins / most measures)\n    result.base_object = self._select_base_object(result, model)\n    if result.base_object:\n        result.required_objects.add(result.base_object)\n\n    # Detect multi-fact: CFL is needed only when measure source objects\n    # span multiple independent fact tables.  If all measure sources are\n    # reachable from the base object via directed join paths (i.e. they\n    # are dimension tables joined from the same fact), a single star\n    # schema query suffices.\n    if len(result.measure_source_objects) &gt; 1:\n        graph = JoinGraph(model, use_path_names=query.use_path_names or None)\n        reachable = graph.descendants(result.base_object)\n        unreachable = result.measure_source_objects - reachable - {result.base_object}\n        if unreachable:\n            result.requires_cfl = True\n\n    # 4. Validate usePathNames before building join graph\n    for upn in query.use_path_names:\n        if upn.source not in model.data_objects:\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_DATA_OBJECT\",\n                    message=(f\"usePathNames references unknown data object '{upn.source}'\"),\n                    path=\"usePathNames\",\n                )\n            )\n            continue\n        if upn.target not in model.data_objects:\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_DATA_OBJECT\",\n                    message=(f\"usePathNames references unknown data object '{upn.target}'\"),\n                    path=\"usePathNames\",\n                )\n            )\n            continue\n        # Check that a secondary join with this pathName exists for the pair\n        source_obj = model.data_objects[upn.source]\n        found = False\n        for join in source_obj.joins:\n            if (\n                join.join_to == upn.target\n                and join.secondary\n                and join.path_name == upn.path_name\n            ):\n                found = True\n                break\n        if not found:\n            errors.append(\n                SemanticError(\n                    code=\"UNKNOWN_PATH_NAME\",\n                    message=(\n                        f\"No secondary join with pathName '{upn.path_name}' \"\n                        f\"from '{upn.source}' to '{upn.target}'\"\n                    ),\n                    path=\"usePathNames\",\n                )\n            )\n\n    # 5. Resolve join paths\n    graph = JoinGraph(model, use_path_names=query.use_path_names or None)\n    if result.base_object and len(result.required_objects) &gt; 1:\n        result.join_steps = graph.find_join_path({result.base_object}, result.required_objects)\n\n    # Build set of all objects present in the query's join graph\n    joined_objects: set[str] = set()\n    if result.base_object:\n        joined_objects.add(result.base_object)\n    for step in result.join_steps:\n        joined_objects.add(step.to_object)\n\n    # 6. Classify filters \u2014 filters may auto-extend the join path\n    for qf in query.where:\n        resolved_filter = self._resolve_filter(\n            qf,\n            model,\n            is_having=False,\n            errors=errors,\n            joined_objects=joined_objects,\n            graph=graph,\n            result=result,\n        )\n        if resolved_filter:\n            result.where_filters.append(resolved_filter)\n\n    for qf in query.having:\n        resolved_filter = self._resolve_filter(\n            qf,\n            model,\n            is_having=True,\n            errors=errors,\n            joined_objects=joined_objects,\n            graph=graph,\n            result=result,\n        )\n        if resolved_filter:\n            result.having_filters.append(resolved_filter)\n\n    # 7. Resolve order by \u2014 must reference a dimension or measure in SELECT\n    select_count = len(result.dimensions) + len(result.measures)\n    for ob in query.order_by:\n        expr = self._resolve_order_by_field(\n            ob.field,\n            result,\n            errors=errors,\n            select_count=select_count,\n        )\n        if expr:\n            result.order_by_exprs.append((expr, ob.direction == \"desc\"))\n\n    if errors:\n        raise ResolutionError(errors)\n\n    return result\n</code></pre>"},{"location":"reference/python-api/#star-schema-planner","title":"Star Schema Planner","text":""},{"location":"reference/python-api/#orionbelt.compiler.star.StarSchemaPlanner","title":"<code>orionbelt.compiler.star.StarSchemaPlanner</code>","text":"<p>Plans star-schema queries: single fact base with dimension joins.</p> Source code in <code>src/orionbelt/compiler/star.py</code> <pre><code>class StarSchemaPlanner:\n    \"\"\"Plans star-schema queries: single fact base with dimension joins.\"\"\"\n\n    def plan(self, resolved: ResolvedQuery, model: SemanticModel) -&gt; QueryPlan:\n        builder = QueryBuilder()\n        graph = JoinGraph(model, use_path_names=resolved.use_path_names or None)\n\n        base_object = model.data_objects.get(resolved.base_object)\n        if not base_object:\n            return QueryPlan(ast=builder.build())\n\n        base_alias = resolved.base_object\n\n        # SELECT: dimensions\n        for dim in resolved.dimensions:\n            col = ColumnRef(name=dim.source_column, table=dim.object_name)\n            if dim.grain:\n                # Time grain will be applied by dialect, for now use column directly\n                builder.select(AliasedExpr(expr=col, alias=dim.name))\n            else:\n                builder.select(AliasedExpr(expr=col, alias=dim.name))\n\n        # SELECT: measures (aggregated) \u2014 for metrics, substitute component refs\n        for measure in resolved.measures:\n            if measure.component_measures:\n                substituted = _substitute_measure_refs(\n                    measure.expression, resolved.metric_components\n                )\n                builder.select(AliasedExpr(expr=substituted, alias=measure.name))\n            else:\n                builder.select(AliasedExpr(expr=measure.expression, alias=measure.name))\n\n        # FROM: base fact table\n        builder.from_(base_object.qualified_code, alias=base_alias)\n\n        # JOINs: dimension tables\n        for step in resolved.join_steps:\n            target_object = model.data_objects.get(step.to_object)\n            if not target_object:\n                continue\n            on_expr = graph.build_join_condition(step)\n            builder.join(\n                table=target_object.qualified_code,\n                on=on_expr,\n                join_type=step.join_type,\n                alias=step.to_object,\n            )\n\n        # WHERE\n        for wf in resolved.where_filters:\n            builder.where(wf.expression)\n\n        # GROUP BY (all dimension columns)\n        for dim in resolved.dimensions:\n            col = ColumnRef(name=dim.source_column, table=dim.object_name)\n            builder.group_by(col)\n\n        # HAVING\n        for hf in resolved.having_filters:\n            builder.having(hf.expression)\n\n        # ORDER BY\n        for expr, desc in resolved.order_by_exprs:\n            builder.order_by(expr, desc=desc)\n\n        # LIMIT\n        if resolved.limit is not None:\n            builder.limit(resolved.limit)\n\n        return QueryPlan(ast=builder.build())\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.star.StarSchemaPlanner.plan","title":"<code>plan(resolved, model)</code>","text":"Source code in <code>src/orionbelt/compiler/star.py</code> <pre><code>def plan(self, resolved: ResolvedQuery, model: SemanticModel) -&gt; QueryPlan:\n    builder = QueryBuilder()\n    graph = JoinGraph(model, use_path_names=resolved.use_path_names or None)\n\n    base_object = model.data_objects.get(resolved.base_object)\n    if not base_object:\n        return QueryPlan(ast=builder.build())\n\n    base_alias = resolved.base_object\n\n    # SELECT: dimensions\n    for dim in resolved.dimensions:\n        col = ColumnRef(name=dim.source_column, table=dim.object_name)\n        if dim.grain:\n            # Time grain will be applied by dialect, for now use column directly\n            builder.select(AliasedExpr(expr=col, alias=dim.name))\n        else:\n            builder.select(AliasedExpr(expr=col, alias=dim.name))\n\n    # SELECT: measures (aggregated) \u2014 for metrics, substitute component refs\n    for measure in resolved.measures:\n        if measure.component_measures:\n            substituted = _substitute_measure_refs(\n                measure.expression, resolved.metric_components\n            )\n            builder.select(AliasedExpr(expr=substituted, alias=measure.name))\n        else:\n            builder.select(AliasedExpr(expr=measure.expression, alias=measure.name))\n\n    # FROM: base fact table\n    builder.from_(base_object.qualified_code, alias=base_alias)\n\n    # JOINs: dimension tables\n    for step in resolved.join_steps:\n        target_object = model.data_objects.get(step.to_object)\n        if not target_object:\n            continue\n        on_expr = graph.build_join_condition(step)\n        builder.join(\n            table=target_object.qualified_code,\n            on=on_expr,\n            join_type=step.join_type,\n            alias=step.to_object,\n        )\n\n    # WHERE\n    for wf in resolved.where_filters:\n        builder.where(wf.expression)\n\n    # GROUP BY (all dimension columns)\n    for dim in resolved.dimensions:\n        col = ColumnRef(name=dim.source_column, table=dim.object_name)\n        builder.group_by(col)\n\n    # HAVING\n    for hf in resolved.having_filters:\n        builder.having(hf.expression)\n\n    # ORDER BY\n    for expr, desc in resolved.order_by_exprs:\n        builder.order_by(expr, desc=desc)\n\n    # LIMIT\n    if resolved.limit is not None:\n        builder.limit(resolved.limit)\n\n    return QueryPlan(ast=builder.build())\n</code></pre>"},{"location":"reference/python-api/#cfl-planner","title":"CFL Planner","text":""},{"location":"reference/python-api/#orionbelt.compiler.cfl.CFLPlanner","title":"<code>orionbelt.compiler.cfl.CFLPlanner</code>","text":"<p>Plans Composite Fact Layer queries: conformed dimensions + fact stitching.</p> <p>Uses a UNION ALL strategy: 1. Each fact leg SELECTs conformed dimensions + its own measures (NULL for others) 2. UNION ALL combines the legs into a single CTE 3. Outer query aggregates over the union, grouping by conformed dimensions</p> Source code in <code>src/orionbelt/compiler/cfl.py</code> <pre><code>class CFLPlanner:\n    \"\"\"Plans Composite Fact Layer queries: conformed dimensions + fact stitching.\n\n    Uses a UNION ALL strategy:\n    1. Each fact leg SELECTs conformed dimensions + its own measures (NULL for others)\n    2. UNION ALL combines the legs into a single CTE\n    3. Outer query aggregates over the union, grouping by conformed dimensions\n    \"\"\"\n\n    def plan(self, resolved: ResolvedQuery, model: SemanticModel) -&gt; QueryPlan:\n        \"\"\"Plan a CFL query.\"\"\"\n        self._validate_fanout(resolved, model)\n\n        # Group measures by their source object\n        measures_by_object, cross_fact = self._group_measures_by_object(resolved, model)\n\n        if len(measures_by_object) &lt;= 1 and not cross_fact:\n            # Single fact \u2014 delegate to star schema\n            from orionbelt.compiler.star import StarSchemaPlanner\n\n            return StarSchemaPlanner().plan(resolved, model)\n\n        # Multi-fact: UNION ALL strategy\n        return self._plan_union_all(resolved, model, measures_by_object, cross_fact)\n\n    def _validate_fanout(self, resolved: ResolvedQuery, model: SemanticModel) -&gt; None:\n        \"\"\"Validate that grain is compatible and no fanout will occur.\"\"\"\n        errors: list[str] = []\n\n        for dim in resolved.dimensions:\n            if dim.object_name not in model.data_objects:\n                errors.append(\n                    f\"Dimension '{dim.name}' references unknown data object '{dim.object_name}'\"\n                )\n\n        if errors:\n            raise FanoutError(\"; \".join(errors))\n\n    def _group_measures_by_object(\n        self,\n        resolved: ResolvedQuery,\n        model: SemanticModel,\n    ) -&gt; tuple[dict[str, list[ResolvedMeasure]], list[ResolvedMeasure]]:\n        \"\"\"Group measures by their primary source object.\n\n        Returns ``(groups, cross_fact)`` where *cross_fact* contains\n        multi-field measures whose fields span multiple objects.\n        For metrics, expand their component measures into the grouping\n        instead of the metric itself.  Cross-fact measures ensure every\n        involved object has a leg, but are not assigned to any single\n        group \u2014 their individual fields are distributed per-leg by\n        ``_plan_union_all``.\n        \"\"\"\n        groups: dict[str, list[ResolvedMeasure]] = {}\n        cross_fact: list[ResolvedMeasure] = []\n        seen: set[str] = set()\n\n        for measure in resolved.measures:\n            if measure.component_measures:\n                # Metric: add each component measure to its source object\n                for comp_name in measure.component_measures:\n                    if comp_name in seen:\n                        continue\n                    seen.add(comp_name)\n                    comp = resolved.metric_components.get(comp_name)\n                    if comp is None:\n                        continue\n                    model_measure = model.measures.get(comp_name)\n                    if model_measure and model_measure.columns:\n                        obj_name = model_measure.columns[0].view or resolved.base_object\n                    else:\n                        obj_name = resolved.base_object\n                    groups.setdefault(obj_name, []).append(comp)\n            else:\n                if measure.name in seen:\n                    continue\n                seen.add(measure.name)\n                model_measure = model.measures.get(measure.name)\n                if not model_measure or not model_measure.columns:\n                    groups.setdefault(resolved.base_object, []).append(measure)\n                    continue\n\n                # Collect all distinct source objects for this measure\n                field_objects = {f.view for f in model_measure.columns if f.view}\n                if len(field_objects) &gt; 1:\n                    # Cross-fact multi-field measure: ensure each\n                    # involved object has a leg, but don't assign\n                    # the measure to any single group.\n                    cross_fact.append(measure)\n                    for obj in field_objects:\n                        groups.setdefault(obj, [])\n                else:\n                    obj_name = model_measure.columns[0].view or resolved.base_object\n                    groups.setdefault(obj_name, []).append(measure)\n\n        return groups, cross_fact\n\n    @staticmethod\n    def _is_multi_field(measure: ResolvedMeasure) -&gt; bool:\n        \"\"\"Check if a measure has multiple field args (e.g. COUNT(a, b)).\"\"\"\n        return isinstance(measure.expression, FunctionCall) and len(measure.expression.args) &gt; 1\n\n    @staticmethod\n    def _multi_field_cte_alias(measure_name: str, idx: int) -&gt; str:\n        \"\"\"CTE column name for the *idx*-th field of a multi-field measure.\"\"\"\n        return f\"{measure_name}__f{idx}\"\n\n    @staticmethod\n    def _unwrap_aggregation(measure: ResolvedMeasure) -&gt; Expr:\n        \"\"\"Extract the inner expression from an aggregated measure.\n\n        For FunctionCall(SUM, [inner]) \u2192 returns inner.\n        Falls back to the full expression if not a FunctionCall.\n        \"\"\"\n        if isinstance(measure.expression, FunctionCall) and measure.expression.args:\n            return measure.expression.args[0]\n        return measure.expression\n\n    def _build_outer_metric_expr(\n        self,\n        metric: ResolvedMeasure,\n        resolved: ResolvedQuery,\n    ) -&gt; Expr:\n        \"\"\"Build the outer query expression for a metric.\n\n        Walks the metric's AST tree and replaces each ColumnRef(measure_name)\n        with ``AGG(\"measure_name\")`` using the component measure's aggregation.\n        \"\"\"\n        return self._substitute_outer_refs(metric.expression, resolved)\n\n    def _substitute_outer_refs(self, expr: Expr, resolved: ResolvedQuery) -&gt; Expr:\n        \"\"\"Recursively substitute measure refs with outer aggregations.\"\"\"\n        if isinstance(expr, ColumnRef) and expr.table is None:\n            comp = resolved.metric_components.get(expr.name)\n            if comp:\n                agg = comp.aggregation.upper()\n                distinct = False\n                if agg == \"COUNT_DISTINCT\":\n                    agg = \"COUNT\"\n                    distinct = True\n                if isinstance(comp.expression, FunctionCall) and comp.expression.distinct:\n                    distinct = True\n                return FunctionCall(\n                    name=agg,\n                    args=[ColumnRef(name=comp.name)],\n                    distinct=distinct,\n                )\n        if isinstance(expr, BinaryOp):\n            new_left = self._substitute_outer_refs(expr.left, resolved)\n            new_right = self._substitute_outer_refs(expr.right, resolved)\n            if new_left is not expr.left or new_right is not expr.right:\n                return BinaryOp(left=new_left, op=expr.op, right=new_right)\n        return expr\n\n    @staticmethod\n    def _collect_table_refs(expr: Expr, tables: set[str]) -&gt; None:\n        \"\"\"Recursively collect table names from ColumnRef nodes.\"\"\"\n        if isinstance(expr, ColumnRef) and expr.table:\n            tables.add(expr.table)\n        elif isinstance(expr, BinaryOp):\n            CFLPlanner._collect_table_refs(expr.left, tables)\n            CFLPlanner._collect_table_refs(expr.right, tables)\n        elif isinstance(expr, (InList, IsNull, Between)):\n            CFLPlanner._collect_table_refs(expr.expr, tables)\n        elif isinstance(expr, RelativeDateRange):\n            CFLPlanner._collect_table_refs(expr.column, tables)\n        elif isinstance(expr, FunctionCall):\n            for arg in expr.args:\n                CFLPlanner._collect_table_refs(arg, tables)\n\n    @staticmethod\n    def _remap_cfl_order_by(expr: Expr, resolved: ResolvedQuery) -&gt; Expr:\n        \"\"\"Remap ORDER BY expressions to use CTE aliases for the outer query.\n\n        In CFL, the outer query selects from the composite CTE \u2014 original\n        table-qualified refs are out of scope.  Remap dimension and measure\n        expressions to their CTE alias names.\n        \"\"\"\n        # Dimension: ColumnRef(name=source_col, table=obj) \u2192 ColumnRef(name=dim.name)\n        if isinstance(expr, ColumnRef) and expr.table is not None:\n            for dim in resolved.dimensions:\n                if expr.name == dim.source_column and expr.table == dim.object_name:\n                    return ColumnRef(name=dim.name)\n        # Measure: match by identity (same expression object)\n        for meas in resolved.measures:\n            if expr is meas.expression:\n                return ColumnRef(name=meas.name)\n        # Numeric position \u2014 pass through\n        return expr\n\n    def _build_outer_concat_count(\n        self,\n        measure_name: str,\n        n_fields: int,\n        agg: str,\n        distinct: bool,\n    ) -&gt; Expr:\n        \"\"\"Build ``COUNT(DISTINCT CAST(f0 AS VARCHAR) || '|' || ...)`` for the outer query.\"\"\"\n        parts: list[Expr] = [\n            Cast(\n                expr=ColumnRef(name=self._multi_field_cte_alias(measure_name, i)),\n                type_name=\"VARCHAR\",\n            )\n            for i in range(n_fields)\n        ]\n        concat: Expr = parts[0]\n        for part in parts[1:]:\n            concat = BinaryOp(\n                left=concat,\n                op=\"||\",\n                right=BinaryOp(\n                    left=Literal.string(\"|\"),\n                    op=\"||\",\n                    right=part,\n                ),\n            )\n        return FunctionCall(name=agg, args=[concat], distinct=distinct)\n\n    def _plan_union_all(\n        self,\n        resolved: ResolvedQuery,\n        model: SemanticModel,\n        measures_by_object: dict[str, list[ResolvedMeasure]],\n        cross_fact: list[ResolvedMeasure] | None = None,\n    ) -&gt; QueryPlan:\n        \"\"\"UNION ALL strategy: stack fact legs with NULL padding, aggregate outside.\"\"\"\n        graph = JoinGraph(model, use_path_names=resolved.use_path_names or None)\n\n        # Collect all measures across all objects + cross-fact measures\n        all_measures: list[ResolvedMeasure] = []\n        for measures in measures_by_object.values():\n            all_measures.extend(measures)\n        if cross_fact:\n            all_measures.extend(cross_fact)\n\n        # Collect data objects referenced by WHERE filters \u2014 each leg\n        # must join these tables so the filter predicates are valid.\n        filter_objects: set[str] = set()\n        for wf in resolved.where_filters:\n            self._collect_table_refs(wf.expression, filter_objects)\n\n        # Build one SELECT per fact object group.\n        # Each leg computes its own LCA (least common ancestor) as the lead\n        # table \u2014 the graph-central node that can reach all dimension objects\n        # and the measure's source object with minimal hops.\n        union_legs: list[Select] = []\n        for obj_name, measures in measures_by_object.items():\n            leg_builder = QueryBuilder()\n            this_measure_names = {m.name for m in measures}\n\n            # SELECT conformed dimensions\n            for dim in resolved.dimensions:\n                col = ColumnRef(name=dim.source_column, table=dim.object_name)\n                leg_builder.select(AliasedExpr(expr=col, alias=dim.name))\n\n            # SELECT this fact's measures (raw expressions, no aggregation)\n            # and NULL for the other facts' measures.\n            # Multi-field measures expand into one CTE column per field.\n            for m in all_measures:\n                if self._is_multi_field(m):\n                    assert isinstance(m.expression, FunctionCall)\n                    for i, arg in enumerate(m.expression.args):\n                        alias = self._multi_field_cte_alias(m.name, i)\n                        # Each field goes into the leg that owns its table\n                        arg_table = arg.table if isinstance(arg, ColumnRef) else None\n                        if arg_table == obj_name:\n                            leg_builder.select(AliasedExpr(expr=arg, alias=alias))\n                        else:\n                            leg_builder.select(AliasedExpr(expr=Literal.null(), alias=alias))\n                elif m.name in this_measure_names:\n                    leg_builder.select(AliasedExpr(expr=self._unwrap_aggregation(m), alias=m.name))\n                else:\n                    leg_builder.select(AliasedExpr(expr=Literal.null(), alias=m.name))\n\n            # Determine the common root for this leg:\n            # the deepest directed ancestor that can reach all dimension\n            # objects, measure's source object, and filter-referenced objects.\n            leg_required = {dim.object_name for dim in resolved.dimensions}\n            leg_required.add(obj_name)\n            leg_required.update(filter_objects)\n            lead = graph.find_common_root(leg_required)\n            lead_obj = model.data_objects.get(lead)\n\n            # FROM: the lead (LCA) table\n            if lead_obj:\n                leg_builder.from_(lead_obj.qualified_code, alias=lead)\n\n            # JOINs: all required objects reachable from the lead\n            join_targets = leg_required - {lead}\n            if join_targets:\n                steps = graph.find_join_path({lead}, leg_required)\n                for step in steps:\n                    target_object = model.data_objects.get(step.to_object)\n                    if target_object:\n                        on_expr = graph.build_join_condition(step)\n                        leg_builder.join(\n                            table=target_object.qualified_code,\n                            on=on_expr,\n                            join_type=step.join_type,\n                            alias=step.to_object,\n                        )\n\n            # Apply WHERE filters to each leg\n            for wf in resolved.where_filters:\n                leg_builder.where(wf.expression)\n\n            union_legs.append(leg_builder.build())\n\n        # Create the UNION ALL CTE\n        cte_name = \"composite_01\"\n        union_cte = CTE(name=cte_name, query=UnionAll(queries=union_legs))\n\n        # Build outer query: aggregate over the composite CTE\n        outer_builder = QueryBuilder()\n\n        # SELECT dimensions\n        for dim in resolved.dimensions:\n            outer_builder.select(\n                AliasedExpr(\n                    expr=ColumnRef(name=dim.name),\n                    alias=dim.name,\n                )\n            )\n\n        # SELECT aggregated measures and metrics\n        # First, add all component measures (from UNION ALL legs)\n        seen_measure_names: set[str] = set()\n        for m in all_measures:\n            seen_measure_names.add(m.name)\n            agg = m.aggregation.upper()\n            distinct = False\n            if agg == \"COUNT_DISTINCT\":\n                agg = \"COUNT\"\n                distinct = True\n            if isinstance(m.expression, FunctionCall) and m.expression.distinct:\n                distinct = True\n\n            if self._is_multi_field(m):\n                # Multi-field: concat CTE columns in outer query\n                assert isinstance(m.expression, FunctionCall)\n                n_fields = len(m.expression.args)\n                agg_expr: Expr = self._build_outer_concat_count(m.name, n_fields, agg, distinct)\n            else:\n                agg_expr = FunctionCall(\n                    name=agg,\n                    args=[ColumnRef(name=m.name)],\n                    distinct=distinct,\n                )\n            outer_builder.select(AliasedExpr(expr=agg_expr, alias=m.name))\n\n        # Then, add metric expressions that combine component measures\n        for m in resolved.measures:\n            if m.component_measures and m.name not in seen_measure_names:\n                metric_expr = self._build_outer_metric_expr(m, resolved)\n                outer_builder.select(AliasedExpr(expr=metric_expr, alias=m.name))\n\n        outer_builder.from_(cte_name, alias=cte_name)\n\n        # GROUP BY dimensions\n        for dim in resolved.dimensions:\n            outer_builder.group_by(ColumnRef(name=dim.name))\n\n        # HAVING filters on the outer query\n        for hf in resolved.having_filters:\n            outer_builder.having(hf.expression)\n\n        # ORDER BY and LIMIT \u2014 remap to CTE aliases\n        for expr, desc in resolved.order_by_exprs:\n            outer_builder.order_by(self._remap_cfl_order_by(expr, resolved), desc=desc)\n        if resolved.limit is not None:\n            outer_builder.limit(resolved.limit)\n\n        outer_select = outer_builder.build()\n\n        # Attach CTE\n        final = Select(\n            columns=outer_select.columns,\n            from_=outer_select.from_,\n            joins=outer_select.joins,\n            where=outer_select.where,\n            group_by=outer_select.group_by,\n            having=outer_select.having,\n            order_by=outer_select.order_by,\n            limit=outer_select.limit,\n            ctes=[union_cte],\n        )\n\n        return QueryPlan(ast=final)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.cfl.CFLPlanner.plan","title":"<code>plan(resolved, model)</code>","text":"<p>Plan a CFL query.</p> Source code in <code>src/orionbelt/compiler/cfl.py</code> <pre><code>def plan(self, resolved: ResolvedQuery, model: SemanticModel) -&gt; QueryPlan:\n    \"\"\"Plan a CFL query.\"\"\"\n    self._validate_fanout(resolved, model)\n\n    # Group measures by their source object\n    measures_by_object, cross_fact = self._group_measures_by_object(resolved, model)\n\n    if len(measures_by_object) &lt;= 1 and not cross_fact:\n        # Single fact \u2014 delegate to star schema\n        from orionbelt.compiler.star import StarSchemaPlanner\n\n        return StarSchemaPlanner().plan(resolved, model)\n\n    # Multi-fact: UNION ALL strategy\n    return self._plan_union_all(resolved, model, measures_by_object, cross_fact)\n</code></pre>"},{"location":"reference/python-api/#join-graph","title":"Join Graph","text":""},{"location":"reference/python-api/#orionbelt.compiler.graph.JoinGraph","title":"<code>orionbelt.compiler.graph.JoinGraph</code>","text":"<p>Graph of data objects (nodes) and relationships (edges) for join path resolution.</p> Source code in <code>src/orionbelt/compiler/graph.py</code> <pre><code>class JoinGraph:\n    \"\"\"Graph of data objects (nodes) and relationships (edges) for join path resolution.\"\"\"\n\n    def __init__(\n        self,\n        model: SemanticModel,\n        use_path_names: list[UsePathName] | None = None,\n    ) -&gt; None:\n        self._graph: nx.Graph[str] = nx.Graph()\n        self._directed: nx.DiGraph[str] = nx.DiGraph()\n        self._model = model\n        self._build(model, use_path_names)\n\n    def _build(\n        self,\n        model: SemanticModel,\n        use_path_names: list[UsePathName] | None = None,\n    ) -&gt; None:\n        \"\"\"Build the graph from the semantic model.\n\n        Secondary joins are only included when their pathName is requested\n        via *use_path_names*.  When a secondary override is active for a\n        ``(source, target)`` pair, the primary join for that pair is excluded.\n        \"\"\"\n        for name in model.data_objects:\n            self._graph.add_node(name)\n            self._directed.add_node(name)\n\n        # Build a lookup: (source, target) \u2192 pathName for active overrides\n        active_overrides: dict[tuple[str, str], str] = {}\n        if use_path_names:\n            for upn in use_path_names:\n                active_overrides[(upn.source, upn.target)] = upn.path_name\n\n        for obj_name, obj in model.data_objects.items():\n            for join in obj.joins:\n                if join.join_to not in model.data_objects:\n                    continue\n                pair = (obj_name, join.join_to)\n\n                if join.secondary:\n                    # Only include if this secondary join's pathName is active\n                    if pair in active_overrides and active_overrides[pair] == join.path_name:\n                        self._add_edge(obj_name, join)\n                else:\n                    # Primary join: skip if an active override exists for this pair\n                    if pair not in active_overrides:\n                        self._add_edge(obj_name, join)\n\n    def _add_edge(self, obj_name: str, join: object) -&gt; None:\n        \"\"\"Add an edge to both the undirected and directed graphs.\"\"\"\n        from orionbelt.models.semantic import DataObjectJoin\n\n        assert isinstance(join, DataObjectJoin)\n        self._graph.add_edge(\n            obj_name,\n            join.join_to,\n            columns_from=join.columns_from,\n            columns_to=join.columns_to,\n            cardinality=join.join_type,\n            source_object=obj_name,\n        )\n        self._directed.add_edge(\n            obj_name,\n            join.join_to,\n            columns_from=join.columns_from,\n            columns_to=join.columns_to,\n            cardinality=join.join_type,\n        )\n\n    def descendants(self, node: str) -&gt; set[str]:\n        \"\"\"Return all nodes reachable from *node* via directed join paths.\"\"\"\n        if node not in self._directed:\n            return set()\n        return nx.descendants(self._directed, node)\n\n    def find_common_root(self, required_objects: set[str]) -&gt; str:\n        \"\"\"Find the common root for a set of required objects.\n\n        The join graph is a DAG (joins define direction: source \u2192 joinTo).\n        The common root is the **deepest** node that can reach ALL\n        *required_objects* via directed join paths.  \"Deepest\" = smallest\n        descendant set (most specific ancestor, closest to the required nodes).\n\n        In ``returns \u2192 sales \u2192 customer``, with required ``{customer, item}``,\n        the common root is ``sales`` (it can reach both).  With required\n        ``{customer, item, returns}``, the common root is ``returns`` (the\n        only node that can reach all three).\n        \"\"\"\n        required = required_objects &amp; set(self._directed.nodes)\n        if len(required) &lt;= 1:\n            return next(iter(sorted(required))) if required else \"\"\n\n        # Find all nodes that can reach ALL required nodes via directed paths\n        candidates: list[tuple[str, int]] = []\n        for node in self._directed.nodes:\n            reachable = nx.descendants(self._directed, node) | {node}\n            if required &lt;= reachable:\n                candidates.append((node, len(reachable)))\n\n        if not candidates:\n            # Fallback: no single directed ancestor covers all \u2014\n            # use undirected shortest-path center\n            return self._find_center_undirected(required)\n\n        # Pick the deepest ancestor: smallest reachable set that still covers all\n        candidates.sort(key=lambda x: (x[1], x[0]))\n        return candidates[0][0]\n\n    def _find_center_undirected(self, required: set[str]) -&gt; str:\n        \"\"\"Fallback: center of the Steiner tree in the undirected graph.\"\"\"\n        nodes = sorted(required)\n        if len(nodes) &lt;= 1:\n            return nodes[0] if nodes else \"\"\n\n        steiner: set[str] = set()\n        for i in range(len(nodes)):\n            for j in range(i + 1, len(nodes)):\n                try:\n                    path: list[str] = nx.shortest_path(self._graph, nodes[i], nodes[j])\n                    steiner.update(path)\n                except nx.NetworkXNoPath:\n                    pass\n\n        if not steiner:\n            return nodes[0]\n\n        best: str = nodes[0]\n        best_max = len(self._graph.nodes) + 1\n        for node in sorted(steiner):\n            max_dist = max(\n                nx.shortest_path_length(self._graph, node, r) for r in nodes\n            )\n            if max_dist &lt; best_max:\n                best_max = max_dist\n                best = node\n        return best\n\n    def find_join_path(self, from_objects: set[str], to_objects: set[str]) -&gt; list[JoinStep]:\n        \"\"\"Find a minimal join path connecting all required data objects.\n\n        Uses shortest path for each target object from the set of source objects.\n        \"\"\"\n        steps: list[JoinStep] = []\n        visited_edges: set[tuple[str, str]] = set()\n\n        all_targets = to_objects - from_objects\n        source_list = list(from_objects)\n\n        for target in sorted(all_targets):\n            best_path: list[str] | None = None\n            for source in source_list:\n                try:\n                    path = nx.shortest_path(self._graph, source, target)\n                    if best_path is None or len(path) &lt; len(best_path):\n                        best_path = path\n                except nx.NetworkXNoPath:\n                    continue\n\n            if best_path is None:\n                continue\n\n            for i in range(len(best_path) - 1):\n                edge = (best_path[i], best_path[i + 1])\n                rev_edge = (best_path[i + 1], best_path[i])\n                if edge in visited_edges or rev_edge in visited_edges:\n                    continue\n                visited_edges.add(edge)\n\n                edge_data = self._graph.edges[edge]\n                source_object = edge_data.get(\"source_object\", edge[0])\n\n                if source_object == edge[0]:\n                    step = JoinStep(\n                        from_object=edge[0],\n                        to_object=edge[1],\n                        from_columns=edge_data[\"columns_from\"],\n                        to_columns=edge_data[\"columns_to\"],\n                        join_type=ASTJoinType.LEFT,\n                        cardinality=edge_data[\"cardinality\"],\n                    )\n                else:\n                    step = JoinStep(\n                        from_object=edge[1],\n                        to_object=edge[0],\n                        from_columns=edge_data[\"columns_to\"],\n                        to_columns=edge_data[\"columns_from\"],\n                        join_type=ASTJoinType.LEFT,\n                        cardinality=edge_data[\"cardinality\"],\n                        reversed=True,\n                    )\n                steps.append(step)\n\n            # Add target to sources for subsequent lookups\n            source_list.append(target)\n\n        return steps\n\n    def build_join_condition(self, step: JoinStep) -&gt; Expr:\n        \"\"\"Build the ON clause expression for a join step.\"\"\"\n        conditions: list[Expr] = []\n        for from_c, to_c in zip(step.from_columns, step.to_columns, strict=True):\n            # Resolve to physical column names\n            from_obj = self._model.data_objects.get(step.from_object)\n            to_obj = self._model.data_objects.get(step.to_object)\n            if from_obj and from_c in from_obj.columns:\n                from_col = from_obj.columns[from_c].code\n            else:\n                from_col = from_c\n            to_col = to_obj.columns[to_c].code if to_obj and to_c in to_obj.columns else to_c\n            conditions.append(\n                BinaryOp(\n                    left=ColumnRef(name=from_col, table=step.from_object),\n                    op=\"=\",\n                    right=ColumnRef(name=to_col, table=step.to_object),\n                )\n            )\n\n        result: Expr = conditions[0]\n        for cond in conditions[1:]:\n            result = BinaryOp(left=result, op=\"AND\", right=cond)\n        return result\n\n    def detect_cycles(self) -&gt; list[list[str]]:\n        \"\"\"Detect cyclic join paths.\"\"\"\n        try:\n            cycles = list(nx.simple_cycles(self._directed))\n            return cycles\n        except nx.NetworkXError:\n            return []\n\n    def validate_deterministic(self) -&gt; list[SemanticError]:\n        \"\"\"Ensure join paths are deterministic (no ambiguity).\"\"\"\n        errors: list[SemanticError] = []\n        # Check for multiple edges between the same pair of nodes\n        for u, v in self._graph.edges():\n            if self._graph.number_of_edges(u, v) &gt; 1:\n                errors.append(\n                    SemanticError(\n                        code=\"AMBIGUOUS_JOIN\",\n                        message=f\"Multiple join paths between '{u}' and '{v}'\",\n                        path=f\"dataObjects.{u}.joins\",\n                    )\n                )\n        return errors\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.graph.JoinGraph.find_join_path","title":"<code>find_join_path(from_objects, to_objects)</code>","text":"<p>Find a minimal join path connecting all required data objects.</p> <p>Uses shortest path for each target object from the set of source objects.</p> Source code in <code>src/orionbelt/compiler/graph.py</code> <pre><code>def find_join_path(self, from_objects: set[str], to_objects: set[str]) -&gt; list[JoinStep]:\n    \"\"\"Find a minimal join path connecting all required data objects.\n\n    Uses shortest path for each target object from the set of source objects.\n    \"\"\"\n    steps: list[JoinStep] = []\n    visited_edges: set[tuple[str, str]] = set()\n\n    all_targets = to_objects - from_objects\n    source_list = list(from_objects)\n\n    for target in sorted(all_targets):\n        best_path: list[str] | None = None\n        for source in source_list:\n            try:\n                path = nx.shortest_path(self._graph, source, target)\n                if best_path is None or len(path) &lt; len(best_path):\n                    best_path = path\n            except nx.NetworkXNoPath:\n                continue\n\n        if best_path is None:\n            continue\n\n        for i in range(len(best_path) - 1):\n            edge = (best_path[i], best_path[i + 1])\n            rev_edge = (best_path[i + 1], best_path[i])\n            if edge in visited_edges or rev_edge in visited_edges:\n                continue\n            visited_edges.add(edge)\n\n            edge_data = self._graph.edges[edge]\n            source_object = edge_data.get(\"source_object\", edge[0])\n\n            if source_object == edge[0]:\n                step = JoinStep(\n                    from_object=edge[0],\n                    to_object=edge[1],\n                    from_columns=edge_data[\"columns_from\"],\n                    to_columns=edge_data[\"columns_to\"],\n                    join_type=ASTJoinType.LEFT,\n                    cardinality=edge_data[\"cardinality\"],\n                )\n            else:\n                step = JoinStep(\n                    from_object=edge[1],\n                    to_object=edge[0],\n                    from_columns=edge_data[\"columns_to\"],\n                    to_columns=edge_data[\"columns_from\"],\n                    join_type=ASTJoinType.LEFT,\n                    cardinality=edge_data[\"cardinality\"],\n                    reversed=True,\n                )\n            steps.append(step)\n\n        # Add target to sources for subsequent lookups\n        source_list.append(target)\n\n    return steps\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.graph.JoinGraph.build_join_condition","title":"<code>build_join_condition(step)</code>","text":"<p>Build the ON clause expression for a join step.</p> Source code in <code>src/orionbelt/compiler/graph.py</code> <pre><code>def build_join_condition(self, step: JoinStep) -&gt; Expr:\n    \"\"\"Build the ON clause expression for a join step.\"\"\"\n    conditions: list[Expr] = []\n    for from_c, to_c in zip(step.from_columns, step.to_columns, strict=True):\n        # Resolve to physical column names\n        from_obj = self._model.data_objects.get(step.from_object)\n        to_obj = self._model.data_objects.get(step.to_object)\n        if from_obj and from_c in from_obj.columns:\n            from_col = from_obj.columns[from_c].code\n        else:\n            from_col = from_c\n        to_col = to_obj.columns[to_c].code if to_obj and to_c in to_obj.columns else to_c\n        conditions.append(\n            BinaryOp(\n                left=ColumnRef(name=from_col, table=step.from_object),\n                op=\"=\",\n                right=ColumnRef(name=to_col, table=step.to_object),\n            )\n        )\n\n    result: Expr = conditions[0]\n    for cond in conditions[1:]:\n        result = BinaryOp(left=result, op=\"AND\", right=cond)\n    return result\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.graph.JoinGraph.detect_cycles","title":"<code>detect_cycles()</code>","text":"<p>Detect cyclic join paths.</p> Source code in <code>src/orionbelt/compiler/graph.py</code> <pre><code>def detect_cycles(self) -&gt; list[list[str]]:\n    \"\"\"Detect cyclic join paths.\"\"\"\n    try:\n        cycles = list(nx.simple_cycles(self._directed))\n        return cycles\n    except nx.NetworkXError:\n        return []\n</code></pre>"},{"location":"reference/python-api/#code-generator","title":"Code Generator","text":""},{"location":"reference/python-api/#orionbelt.compiler.codegen.CodeGenerator","title":"<code>orionbelt.compiler.codegen.CodeGenerator</code>","text":"<p>Generates SQL from AST using a dialect.</p> Source code in <code>src/orionbelt/compiler/codegen.py</code> <pre><code>class CodeGenerator:\n    \"\"\"Generates SQL from AST using a dialect.\"\"\"\n\n    def __init__(self, dialect: Dialect) -&gt; None:\n        self._dialect = dialect\n\n    @property\n    def dialect(self) -&gt; Dialect:\n        return self._dialect\n\n    def generate(self, ast: Select) -&gt; str:\n        \"\"\"Generate SQL string from AST using the configured dialect.\"\"\"\n        return self._dialect.compile(ast)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.compiler.codegen.CodeGenerator.generate","title":"<code>generate(ast)</code>","text":"<p>Generate SQL string from AST using the configured dialect.</p> Source code in <code>src/orionbelt/compiler/codegen.py</code> <pre><code>def generate(self, ast: Select) -&gt; str:\n    \"\"\"Generate SQL string from AST using the configured dialect.\"\"\"\n    return self._dialect.compile(ast)\n</code></pre>"},{"location":"reference/python-api/#dialect-base","title":"Dialect Base","text":""},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect","title":"<code>orionbelt.dialect.base.Dialect</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for all SQL dialects.</p> <p>Provides default SQL compilation; dialects override specific methods.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>class Dialect(ABC):\n    \"\"\"Abstract base for all SQL dialects.\n\n    Provides default SQL compilation; dialects override specific methods.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str: ...\n\n    @property\n    @abstractmethod\n    def capabilities(self) -&gt; DialectCapabilities: ...\n\n    @abstractmethod\n    def quote_identifier(self, name: str) -&gt; str:\n        \"\"\"Quote an identifier per dialect rules.\"\"\"\n\n    @abstractmethod\n    def render_time_grain(self, column: Expr, grain: TimeGrain) -&gt; Expr:\n        \"\"\"Wrap a column expression for the given time grain.\"\"\"\n\n    @abstractmethod\n    def render_cast(self, expr: Expr, target_type: str) -&gt; Expr:\n        \"\"\"Render a CAST expression.\"\"\"\n\n    @abstractmethod\n    def current_date_sql(self) -&gt; str:\n        \"\"\"Return SQL for the current date.\"\"\"\n\n    @abstractmethod\n    def date_add_sql(self, date_sql: str, unit: str, count: int) -&gt; str:\n        \"\"\"Return SQL that adds count units to date_sql.\"\"\"\n\n    def render_string_contains(self, column: Expr, pattern: Expr) -&gt; Expr:\n        \"\"\"Default: column LIKE '%' || pattern || '%'.\"\"\"\n        return BinaryOp(\n            left=column,\n            op=\"LIKE\",\n            right=BinaryOp(\n                left=BinaryOp(left=Literal.string(\"%\"), op=\"||\", right=pattern),\n                op=\"||\",\n                right=Literal.string(\"%\"),\n            ),\n        )\n\n    def _map_function_name(self, name: str) -&gt; str:\n        \"\"\"Map a function name to the dialect-specific equivalent.\n\n        Override in subclasses to remap names (e.g. ANY_VALUE \u2192 any in ClickHouse).\n        \"\"\"\n        return name\n\n    def _compile_median(self, args: list[Expr]) -&gt; str:\n        \"\"\"Compile MEDIAN \u2014 default uses MEDIAN(col).\n\n        Works for Snowflake, ClickHouse, Databricks, and Dremio. Postgres overrides.\n        \"\"\"\n        col_sql = self.compile_expr(args[0]) if args else \"NULL\"\n        return f\"MEDIAN({col_sql})\"\n\n    def _compile_mode(self, args: list[Expr]) -&gt; str:\n        \"\"\"Compile MODE \u2014 default uses MODE(col).\n\n        Works for Snowflake and Databricks. Postgres, ClickHouse, and Dremio override.\n        \"\"\"\n        col_sql = self.compile_expr(args[0]) if args else \"NULL\"\n        return f\"MODE({col_sql})\"\n\n    def _compile_listagg(\n        self,\n        args: list[Expr],\n        distinct: bool,\n        order_by: list[OrderByItem],\n        separator: str | None,\n    ) -&gt; str:\n        \"\"\"Compile LISTAGG \u2014 default uses LISTAGG(col, sep) WITHIN GROUP (ORDER BY ...).\n\n        Works for Snowflake and Dremio. Postgres, ClickHouse, and Databricks override.\n        \"\"\"\n        sep = separator if separator is not None else \",\"\n        col_sql = self.compile_expr(args[0]) if args else \"''\"\n        distinct_sql = \"DISTINCT \" if distinct else \"\"\n        escaped_sep = sep.replace(\"'\", \"''\")\n        result = f\"LISTAGG({distinct_sql}{col_sql}, '{escaped_sep}')\"\n        if order_by:\n            ob = \", \".join(self.compile_order_by(o) for o in order_by)\n            result += f\" WITHIN GROUP (ORDER BY {ob})\"\n        return result\n\n    def _compile_multi_field_count(self, args: list[Expr], distinct: bool) -&gt; str:\n        \"\"\"Compile COUNT with multiple fields by concatenating with ``||``.\n\n        Default (non-Snowflake) strategy: cast each field to VARCHAR and\n        join with ``'|'`` separator so the database sees a single expression.\n        Snowflake overrides this to emit native ``COUNT(col1, col2)``.\n        \"\"\"\n        parts = [f\"CAST({self.compile_expr(a)} AS VARCHAR)\" for a in args]\n        concat = \" || '|' || \".join(parts)\n        if distinct:\n            return f\"COUNT(DISTINCT {concat})\"\n        return f\"COUNT({concat})\"\n\n    def compile(self, ast: Select) -&gt; str:\n        \"\"\"Render a complete SQL AST to a dialect-specific string.\"\"\"\n        return self.compile_select(ast)\n\n    def compile_select(self, node: Select) -&gt; str:\n        \"\"\"Compile a SELECT statement.\"\"\"\n        parts: list[str] = []\n\n        # CTEs\n        if node.ctes:\n            cte_parts = []\n            for cte in node.ctes:\n                if isinstance(cte.query, UnionAll):\n                    cte_sql = self.compile_union_all(cte.query)\n                else:\n                    cte_sql = self.compile_select(cte.query)\n                cte_parts.append(f\"{self.quote_identifier(cte.name)} AS (\\n{cte_sql}\\n)\")\n            parts.append(\"WITH \" + \",\\n\".join(cte_parts))\n\n        # SELECT\n        if node.columns:\n            cols = \", \".join(self.compile_expr(c) for c in node.columns)\n            parts.append(f\"SELECT {cols}\")\n        else:\n            parts.append(\"SELECT *\")\n\n        # FROM\n        if node.from_:\n            parts.append(f\"FROM {self.compile_from(node.from_)}\")\n\n        # JOINs\n        for join in node.joins:\n            parts.append(self.compile_join(join))\n\n        # WHERE\n        if node.where:\n            parts.append(f\"WHERE {self.compile_expr(node.where)}\")\n\n        # GROUP BY\n        if node.group_by:\n            groups = \", \".join(self.compile_expr(g) for g in node.group_by)\n            parts.append(f\"GROUP BY {groups}\")\n\n        # HAVING\n        if node.having:\n            parts.append(f\"HAVING {self.compile_expr(node.having)}\")\n\n        # ORDER BY\n        if node.order_by:\n            orders = \", \".join(self.compile_order_by(o) for o in node.order_by)\n            parts.append(f\"ORDER BY {orders}\")\n\n        # LIMIT\n        if node.limit is not None:\n            parts.append(f\"LIMIT {node.limit}\")\n\n        # OFFSET\n        if node.offset is not None:\n            parts.append(f\"OFFSET {node.offset}\")\n\n        return \"\\n\".join(parts)\n\n    def compile_from(self, node: From) -&gt; str:\n        if isinstance(node.source, Select):\n            sub = self.compile_select(node.source)\n            result = f\"(\\n{sub}\\n)\"\n        else:\n            result = str(node.source)\n        if node.alias:\n            result += f\" AS {self.quote_identifier(node.alias)}\"\n        return result\n\n    def compile_join(self, node: Join) -&gt; str:\n        if isinstance(node.source, Select):\n            source = f\"(\\n{self.compile_select(node.source)}\\n)\"\n        else:\n            source = str(node.source)\n        if node.alias:\n            source += f\" AS {self.quote_identifier(node.alias)}\"\n\n        parts = [f\"{node.join_type.value} JOIN {source}\"]\n        if node.on:\n            parts.append(f\"ON {self.compile_expr(node.on)}\")\n        return \" \".join(parts)\n\n    def compile_order_by(self, node: OrderByItem) -&gt; str:\n        result = self.compile_expr(node.expr)\n        if node.desc:\n            result += \" DESC\"\n        else:\n            result += \" ASC\"\n        if node.nulls_last is True:\n            result += \" NULLS LAST\"\n        elif node.nulls_last is False:\n            result += \" NULLS FIRST\"\n        return result\n\n    def compile_union_all(self, node: UnionAll) -&gt; str:\n        \"\"\"Compile a UNION ALL of multiple SELECT statements.\"\"\"\n        return \"\\nUNION ALL\\n\".join(self.compile_select(q) for q in node.queries)\n\n    def compile_expr(self, expr: Expr) -&gt; str:\n        \"\"\"Compile an expression node to SQL string.\"\"\"\n        match expr:\n            case Literal(value=None):\n                return \"NULL\"\n            case Literal(value=True):\n                return \"TRUE\"\n            case Literal(value=False):\n                return \"FALSE\"\n            case Literal(value=v) if isinstance(v, str):\n                escaped = v.replace(\"'\", \"''\")\n                return f\"'{escaped}'\"\n            case Literal(value=v):\n                return str(v)\n            case Star(table=None):\n                return \"*\"\n            case Star(table=t) if t is not None:\n                return f\"{self.quote_identifier(t)}.*\"\n            case ColumnRef(name=name, table=None):\n                return self.quote_identifier(name)\n            case ColumnRef(name=name, table=table) if table is not None:\n                return f\"{self.quote_identifier(table)}.{self.quote_identifier(name)}\"\n            case AliasedExpr(expr=inner, alias=alias):\n                return f\"{self.compile_expr(inner)} AS {self.quote_identifier(alias)}\"\n            case FunctionCall(\n                name=fname,\n                args=args,\n                distinct=distinct,\n                order_by=order_by,\n                separator=separator,\n            ):\n                # LISTAGG: dialect-specific rendering\n                if fname.upper() == \"LISTAGG\":\n                    return self._compile_listagg(args, distinct, order_by, separator)\n                # MODE: dialect-specific rendering\n                if fname.upper() == \"MODE\":\n                    return self._compile_mode(args)\n                # MEDIAN: dialect-specific rendering\n                if fname.upper() == \"MEDIAN\":\n                    return self._compile_median(args)\n                # Multi-field COUNT: concatenate fields for portability\n                # (Snowflake overrides to use native multi-arg syntax)\n                if fname.upper() == \"COUNT\" and len(args) &gt; 1:\n                    return self._compile_multi_field_count(args, distinct)\n                fname = self._map_function_name(fname)\n                args_sql = \", \".join(self.compile_expr(a) for a in args)\n                if distinct:\n                    return f\"{fname}(DISTINCT {args_sql})\"\n                return f\"{fname}({args_sql})\"\n            case BinaryOp(left=left, op=op, right=right):\n                return f\"({self.compile_expr(left)} {op} {self.compile_expr(right)})\"\n            case UnaryOp(op=op, operand=operand):\n                return f\"({op} {self.compile_expr(operand)})\"\n            case IsNull(expr=inner, negated=False):\n                return f\"({self.compile_expr(inner)} IS NULL)\"\n            case IsNull(expr=inner, negated=True):\n                return f\"({self.compile_expr(inner)} IS NOT NULL)\"\n            case InList(expr=inner, values=values, negated=negated):\n                vals = \", \".join(self.compile_expr(v) for v in values)\n                op = \"NOT IN\" if negated else \"IN\"\n                return f\"({self.compile_expr(inner)} {op} ({vals}))\"\n            case CaseExpr(when_clauses=whens, else_clause=else_):\n                parts = [\"CASE\"]\n                for when_cond, then_val in whens:\n                    parts.append(\n                        f\"WHEN {self.compile_expr(when_cond)} THEN {self.compile_expr(then_val)}\"\n                    )\n                if else_ is not None:\n                    parts.append(f\"ELSE {self.compile_expr(else_)}\")\n                parts.append(\"END\")\n                return \" \".join(parts)\n            case Cast(expr=inner, type_name=type_name):\n                return f\"CAST({self.compile_expr(inner)} AS {type_name})\"\n            case SubqueryExpr(query=query):\n                return f\"(\\n{self.compile_select(query)}\\n)\"\n            case RawSQL(sql=sql):\n                return sql\n            case Between(expr=inner, low=low, high=high, negated=negated):\n                op = \"NOT BETWEEN\" if negated else \"BETWEEN\"\n                return (\n                    f\"({self.compile_expr(inner)} {op} \"\n                    f\"{self.compile_expr(low)} AND {self.compile_expr(high)})\"\n                )\n            case RelativeDateRange(\n                column=column,\n                unit=unit,\n                count=count,\n                direction=direction,\n                include_current=include_current,\n            ):\n                return self.compile_relative_date_range(\n                    column=column,\n                    unit=unit,\n                    count=count,\n                    direction=direction,\n                    include_current=include_current,\n                )\n            case WindowFunction(\n                func_name=fname,\n                args=args,\n                partition_by=partition_by,\n                order_by=order_by,\n                distinct=distinct,\n            ):\n                args_sql = \", \".join(self.compile_expr(a) for a in args)\n                func_sql = f\"{fname}(DISTINCT {args_sql})\" if distinct else f\"{fname}({args_sql})\"\n                over_parts: list[str] = []\n                if partition_by:\n                    pb = \", \".join(self.compile_expr(p) for p in partition_by)\n                    over_parts.append(f\"PARTITION BY {pb}\")\n                if order_by:\n                    ob = \", \".join(self.compile_order_by(o) for o in order_by)\n                    over_parts.append(f\"ORDER BY {ob}\")\n                over_clause = \" \".join(over_parts)\n                return f\"{func_sql} OVER ({over_clause})\"\n            case _:\n                raise ValueError(f\"Unknown AST node type: {type(expr).__name__}\")\n\n    def compile_relative_date_range(\n        self,\n        column: Expr,\n        unit: str,\n        count: int,\n        direction: str,\n        include_current: bool,\n    ) -&gt; str:\n        \"\"\"Compile a relative date range predicate to SQL.\"\"\"\n        col_sql = self.compile_expr(column)\n        base = self.current_date_sql()\n\n        if direction == \"future\":\n            start = base if include_current else self.date_add_sql(base, \"day\", 1)\n            end = self.date_add_sql(start, unit, count)\n        else:\n            end = self.date_add_sql(base, \"day\", 1) if include_current else base\n            start = self.date_add_sql(end, unit, -count)\n\n        return f\"({col_sql} &gt;= {start} AND {col_sql} &lt; {end})\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.quote_identifier","title":"<code>quote_identifier(name)</code>  <code>abstractmethod</code>","text":"<p>Quote an identifier per dialect rules.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>@abstractmethod\ndef quote_identifier(self, name: str) -&gt; str:\n    \"\"\"Quote an identifier per dialect rules.\"\"\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.render_time_grain","title":"<code>render_time_grain(column, grain)</code>  <code>abstractmethod</code>","text":"<p>Wrap a column expression for the given time grain.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>@abstractmethod\ndef render_time_grain(self, column: Expr, grain: TimeGrain) -&gt; Expr:\n    \"\"\"Wrap a column expression for the given time grain.\"\"\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.render_cast","title":"<code>render_cast(expr, target_type)</code>  <code>abstractmethod</code>","text":"<p>Render a CAST expression.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>@abstractmethod\ndef render_cast(self, expr: Expr, target_type: str) -&gt; Expr:\n    \"\"\"Render a CAST expression.\"\"\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.current_date_sql","title":"<code>current_date_sql()</code>  <code>abstractmethod</code>","text":"<p>Return SQL for the current date.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>@abstractmethod\ndef current_date_sql(self) -&gt; str:\n    \"\"\"Return SQL for the current date.\"\"\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.date_add_sql","title":"<code>date_add_sql(date_sql, unit, count)</code>  <code>abstractmethod</code>","text":"<p>Return SQL that adds count units to date_sql.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>@abstractmethod\ndef date_add_sql(self, date_sql: str, unit: str, count: int) -&gt; str:\n    \"\"\"Return SQL that adds count units to date_sql.\"\"\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.render_string_contains","title":"<code>render_string_contains(column, pattern)</code>","text":"<p>Default: column LIKE '%' || pattern || '%'.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>def render_string_contains(self, column: Expr, pattern: Expr) -&gt; Expr:\n    \"\"\"Default: column LIKE '%' || pattern || '%'.\"\"\"\n    return BinaryOp(\n        left=column,\n        op=\"LIKE\",\n        right=BinaryOp(\n            left=BinaryOp(left=Literal.string(\"%\"), op=\"||\", right=pattern),\n            op=\"||\",\n            right=Literal.string(\"%\"),\n        ),\n    )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.compile","title":"<code>compile(ast)</code>","text":"<p>Render a complete SQL AST to a dialect-specific string.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>def compile(self, ast: Select) -&gt; str:\n    \"\"\"Render a complete SQL AST to a dialect-specific string.\"\"\"\n    return self.compile_select(ast)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.compile_select","title":"<code>compile_select(node)</code>","text":"<p>Compile a SELECT statement.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>def compile_select(self, node: Select) -&gt; str:\n    \"\"\"Compile a SELECT statement.\"\"\"\n    parts: list[str] = []\n\n    # CTEs\n    if node.ctes:\n        cte_parts = []\n        for cte in node.ctes:\n            if isinstance(cte.query, UnionAll):\n                cte_sql = self.compile_union_all(cte.query)\n            else:\n                cte_sql = self.compile_select(cte.query)\n            cte_parts.append(f\"{self.quote_identifier(cte.name)} AS (\\n{cte_sql}\\n)\")\n        parts.append(\"WITH \" + \",\\n\".join(cte_parts))\n\n    # SELECT\n    if node.columns:\n        cols = \", \".join(self.compile_expr(c) for c in node.columns)\n        parts.append(f\"SELECT {cols}\")\n    else:\n        parts.append(\"SELECT *\")\n\n    # FROM\n    if node.from_:\n        parts.append(f\"FROM {self.compile_from(node.from_)}\")\n\n    # JOINs\n    for join in node.joins:\n        parts.append(self.compile_join(join))\n\n    # WHERE\n    if node.where:\n        parts.append(f\"WHERE {self.compile_expr(node.where)}\")\n\n    # GROUP BY\n    if node.group_by:\n        groups = \", \".join(self.compile_expr(g) for g in node.group_by)\n        parts.append(f\"GROUP BY {groups}\")\n\n    # HAVING\n    if node.having:\n        parts.append(f\"HAVING {self.compile_expr(node.having)}\")\n\n    # ORDER BY\n    if node.order_by:\n        orders = \", \".join(self.compile_order_by(o) for o in node.order_by)\n        parts.append(f\"ORDER BY {orders}\")\n\n    # LIMIT\n    if node.limit is not None:\n        parts.append(f\"LIMIT {node.limit}\")\n\n    # OFFSET\n    if node.offset is not None:\n        parts.append(f\"OFFSET {node.offset}\")\n\n    return \"\\n\".join(parts)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.compile_union_all","title":"<code>compile_union_all(node)</code>","text":"<p>Compile a UNION ALL of multiple SELECT statements.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>def compile_union_all(self, node: UnionAll) -&gt; str:\n    \"\"\"Compile a UNION ALL of multiple SELECT statements.\"\"\"\n    return \"\\nUNION ALL\\n\".join(self.compile_select(q) for q in node.queries)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.compile_expr","title":"<code>compile_expr(expr)</code>","text":"<p>Compile an expression node to SQL string.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>def compile_expr(self, expr: Expr) -&gt; str:\n    \"\"\"Compile an expression node to SQL string.\"\"\"\n    match expr:\n        case Literal(value=None):\n            return \"NULL\"\n        case Literal(value=True):\n            return \"TRUE\"\n        case Literal(value=False):\n            return \"FALSE\"\n        case Literal(value=v) if isinstance(v, str):\n            escaped = v.replace(\"'\", \"''\")\n            return f\"'{escaped}'\"\n        case Literal(value=v):\n            return str(v)\n        case Star(table=None):\n            return \"*\"\n        case Star(table=t) if t is not None:\n            return f\"{self.quote_identifier(t)}.*\"\n        case ColumnRef(name=name, table=None):\n            return self.quote_identifier(name)\n        case ColumnRef(name=name, table=table) if table is not None:\n            return f\"{self.quote_identifier(table)}.{self.quote_identifier(name)}\"\n        case AliasedExpr(expr=inner, alias=alias):\n            return f\"{self.compile_expr(inner)} AS {self.quote_identifier(alias)}\"\n        case FunctionCall(\n            name=fname,\n            args=args,\n            distinct=distinct,\n            order_by=order_by,\n            separator=separator,\n        ):\n            # LISTAGG: dialect-specific rendering\n            if fname.upper() == \"LISTAGG\":\n                return self._compile_listagg(args, distinct, order_by, separator)\n            # MODE: dialect-specific rendering\n            if fname.upper() == \"MODE\":\n                return self._compile_mode(args)\n            # MEDIAN: dialect-specific rendering\n            if fname.upper() == \"MEDIAN\":\n                return self._compile_median(args)\n            # Multi-field COUNT: concatenate fields for portability\n            # (Snowflake overrides to use native multi-arg syntax)\n            if fname.upper() == \"COUNT\" and len(args) &gt; 1:\n                return self._compile_multi_field_count(args, distinct)\n            fname = self._map_function_name(fname)\n            args_sql = \", \".join(self.compile_expr(a) for a in args)\n            if distinct:\n                return f\"{fname}(DISTINCT {args_sql})\"\n            return f\"{fname}({args_sql})\"\n        case BinaryOp(left=left, op=op, right=right):\n            return f\"({self.compile_expr(left)} {op} {self.compile_expr(right)})\"\n        case UnaryOp(op=op, operand=operand):\n            return f\"({op} {self.compile_expr(operand)})\"\n        case IsNull(expr=inner, negated=False):\n            return f\"({self.compile_expr(inner)} IS NULL)\"\n        case IsNull(expr=inner, negated=True):\n            return f\"({self.compile_expr(inner)} IS NOT NULL)\"\n        case InList(expr=inner, values=values, negated=negated):\n            vals = \", \".join(self.compile_expr(v) for v in values)\n            op = \"NOT IN\" if negated else \"IN\"\n            return f\"({self.compile_expr(inner)} {op} ({vals}))\"\n        case CaseExpr(when_clauses=whens, else_clause=else_):\n            parts = [\"CASE\"]\n            for when_cond, then_val in whens:\n                parts.append(\n                    f\"WHEN {self.compile_expr(when_cond)} THEN {self.compile_expr(then_val)}\"\n                )\n            if else_ is not None:\n                parts.append(f\"ELSE {self.compile_expr(else_)}\")\n            parts.append(\"END\")\n            return \" \".join(parts)\n        case Cast(expr=inner, type_name=type_name):\n            return f\"CAST({self.compile_expr(inner)} AS {type_name})\"\n        case SubqueryExpr(query=query):\n            return f\"(\\n{self.compile_select(query)}\\n)\"\n        case RawSQL(sql=sql):\n            return sql\n        case Between(expr=inner, low=low, high=high, negated=negated):\n            op = \"NOT BETWEEN\" if negated else \"BETWEEN\"\n            return (\n                f\"({self.compile_expr(inner)} {op} \"\n                f\"{self.compile_expr(low)} AND {self.compile_expr(high)})\"\n            )\n        case RelativeDateRange(\n            column=column,\n            unit=unit,\n            count=count,\n            direction=direction,\n            include_current=include_current,\n        ):\n            return self.compile_relative_date_range(\n                column=column,\n                unit=unit,\n                count=count,\n                direction=direction,\n                include_current=include_current,\n            )\n        case WindowFunction(\n            func_name=fname,\n            args=args,\n            partition_by=partition_by,\n            order_by=order_by,\n            distinct=distinct,\n        ):\n            args_sql = \", \".join(self.compile_expr(a) for a in args)\n            func_sql = f\"{fname}(DISTINCT {args_sql})\" if distinct else f\"{fname}({args_sql})\"\n            over_parts: list[str] = []\n            if partition_by:\n                pb = \", \".join(self.compile_expr(p) for p in partition_by)\n                over_parts.append(f\"PARTITION BY {pb}\")\n            if order_by:\n                ob = \", \".join(self.compile_order_by(o) for o in order_by)\n                over_parts.append(f\"ORDER BY {ob}\")\n            over_clause = \" \".join(over_parts)\n            return f\"{func_sql} OVER ({over_clause})\"\n        case _:\n            raise ValueError(f\"Unknown AST node type: {type(expr).__name__}\")\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.Dialect.compile_relative_date_range","title":"<code>compile_relative_date_range(column, unit, count, direction, include_current)</code>","text":"<p>Compile a relative date range predicate to SQL.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>def compile_relative_date_range(\n    self,\n    column: Expr,\n    unit: str,\n    count: int,\n    direction: str,\n    include_current: bool,\n) -&gt; str:\n    \"\"\"Compile a relative date range predicate to SQL.\"\"\"\n    col_sql = self.compile_expr(column)\n    base = self.current_date_sql()\n\n    if direction == \"future\":\n        start = base if include_current else self.date_add_sql(base, \"day\", 1)\n        end = self.date_add_sql(start, unit, count)\n    else:\n        end = self.date_add_sql(base, \"day\", 1) if include_current else base\n        start = self.date_add_sql(end, unit, -count)\n\n    return f\"({col_sql} &gt;= {start} AND {col_sql} &lt; {end})\"\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.base.DialectCapabilities","title":"<code>orionbelt.dialect.base.DialectCapabilities</code>  <code>dataclass</code>","text":"<p>Flags indicating what SQL features a dialect supports.</p> Source code in <code>src/orionbelt/dialect/base.py</code> <pre><code>@dataclass\nclass DialectCapabilities:\n    \"\"\"Flags indicating what SQL features a dialect supports.\"\"\"\n\n    supports_cte: bool = True\n    supports_qualify: bool = False\n    supports_arrays: bool = False\n    supports_window_filters: bool = False\n    supports_ilike: bool = False\n    supports_time_travel: bool = False\n    supports_semi_structured: bool = False\n</code></pre>"},{"location":"reference/python-api/#dialect-registry","title":"Dialect Registry","text":""},{"location":"reference/python-api/#orionbelt.dialect.registry.DialectRegistry","title":"<code>orionbelt.dialect.registry.DialectRegistry</code>","text":"<p>Registry for SQL dialect plugins.</p> Source code in <code>src/orionbelt/dialect/registry.py</code> <pre><code>class DialectRegistry:\n    \"\"\"Registry for SQL dialect plugins.\"\"\"\n\n    _dialects: dict[str, type[Dialect]] = {}\n\n    @classmethod\n    def register(cls, dialect_class: type[Dialect]) -&gt; type[Dialect]:\n        \"\"\"Register a dialect class. Can be used as a decorator.\"\"\"\n        # Instantiate to read the name property\n        instance = dialect_class()\n        cls._dialects[instance.name] = dialect_class\n        return dialect_class\n\n    @classmethod\n    def get(cls, name: str) -&gt; Dialect:\n        \"\"\"Get an instance of the named dialect.\"\"\"\n        if name not in cls._dialects:\n            raise UnsupportedDialectError(name, available=cls.available())\n        return cls._dialects[name]()\n\n    @classmethod\n    def available(cls) -&gt; list[str]:\n        \"\"\"List registered dialect names.\"\"\"\n        return sorted(cls._dialects.keys())\n\n    @classmethod\n    def reset(cls) -&gt; None:\n        \"\"\"Clear all registered dialects (for testing).\"\"\"\n        cls._dialects.clear()\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.registry.DialectRegistry.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Get an instance of the named dialect.</p> Source code in <code>src/orionbelt/dialect/registry.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; Dialect:\n    \"\"\"Get an instance of the named dialect.\"\"\"\n    if name not in cls._dialects:\n        raise UnsupportedDialectError(name, available=cls.available())\n    return cls._dialects[name]()\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.registry.DialectRegistry.available","title":"<code>available()</code>  <code>classmethod</code>","text":"<p>List registered dialect names.</p> Source code in <code>src/orionbelt/dialect/registry.py</code> <pre><code>@classmethod\ndef available(cls) -&gt; list[str]:\n    \"\"\"List registered dialect names.\"\"\"\n    return sorted(cls._dialects.keys())\n</code></pre>"},{"location":"reference/python-api/#orionbelt.dialect.registry.DialectRegistry.register","title":"<code>register(dialect_class)</code>  <code>classmethod</code>","text":"<p>Register a dialect class. Can be used as a decorator.</p> Source code in <code>src/orionbelt/dialect/registry.py</code> <pre><code>@classmethod\ndef register(cls, dialect_class: type[Dialect]) -&gt; type[Dialect]:\n    \"\"\"Register a dialect class. Can be used as a decorator.\"\"\"\n    # Instantiate to read the name property\n    instance = dialect_class()\n    cls._dialects[instance.name] = dialect_class\n    return dialect_class\n</code></pre>"},{"location":"reference/python-api/#yaml-parser","title":"YAML Parser","text":""},{"location":"reference/python-api/#orionbelt.parser.loader.TrackedLoader","title":"<code>orionbelt.parser.loader.TrackedLoader</code>","text":"<p>YAML loader that tracks source positions for error reporting.</p> <p>Uses ruamel.yaml which preserves line/column info on every parsed node.</p> Source code in <code>src/orionbelt/parser/loader.py</code> <pre><code>class TrackedLoader:\n    \"\"\"YAML loader that tracks source positions for error reporting.\n\n    Uses ruamel.yaml which preserves line/column info on every parsed node.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._yaml = YAML()\n        self._yaml.preserve_quotes = True\n\n    def load(self, path: Path) -&gt; tuple[dict[str, Any], SourceMap]:\n        \"\"\"Load a YAML file and return parsed dict + source position map.\"\"\"\n        with path.open(\"r\", encoding=\"utf-8\") as handle:\n            data = self._yaml.load(handle)\n        if data is None:\n            return {}, SourceMap()\n        source_map = SourceMap()\n        self._extract_positions(data, str(path), \"\", source_map)\n        return self._to_plain_dict(data), source_map\n\n    def load_string(\n        self, content: str, filename: str = \"&lt;string&gt;\"\n    ) -&gt; tuple[dict[str, Any], SourceMap]:\n        \"\"\"Load YAML from a string.\"\"\"\n        data = self._yaml.load(content)\n        if data is None:\n            return {}, SourceMap()\n        source_map = SourceMap()\n        self._extract_positions(data, filename, \"\", source_map)\n        return self._to_plain_dict(data), source_map\n\n    def load_model_directory(self, root: Path) -&gt; tuple[dict[str, Any], SourceMap]:\n        \"\"\"Load a model directory: model.yaml + facts/*.yaml + dimensions/*.yaml + measures/*.yaml.\n\n        Returns a merged dict with all artifacts and a combined source map.\n        \"\"\"\n        merged: dict[str, Any] = {}\n        combined_map = SourceMap()\n\n        # Load model.yaml (root file)\n        model_file = root / \"model.yaml\"\n        if model_file.exists():\n            data, smap = self.load(model_file)\n            merged.update(data)\n            combined_map.merge(smap)\n\n        # Load subdirectory YAML files\n        for subdir in (\"facts\", \"dimensions\", \"measures\", \"macros\", \"policies\"):\n            subdir_path = root / subdir\n            if subdir_path.is_dir():\n                section: dict[str, Any] = merged.get(subdir, {})\n                for yaml_file in sorted(subdir_path.glob(\"*.yaml\")):\n                    data, smap = self.load(yaml_file)\n                    if isinstance(data, dict):\n                        # Use the filename stem as key if the file is a single artifact\n                        if \"name\" in data:\n                            section[data[\"name\"]] = data\n                        else:\n                            section.update(data)\n                    combined_map.merge(smap)\n                if section:\n                    merged[subdir] = section\n\n        return merged, combined_map\n\n    def _extract_positions(\n        self,\n        data: Any,\n        filename: str,\n        prefix: str,\n        source_map: SourceMap,\n    ) -&gt; None:\n        \"\"\"Recursively extract source positions from ruamel.yaml nodes.\"\"\"\n        if isinstance(data, CommentedMap):\n            for key in data:\n                key_path = f\"{prefix}.{key}\" if prefix else str(key)\n                # Try to get position for this key from ruamel.yaml's lc object\n                try:\n                    lc = data.lc\n                    # lc.key() returns a callable in newer ruamel.yaml\n                    key_positions = lc.key(key)\n                    if key_positions:\n                        line, col = key_positions\n                        source_map.add(\n                            key_path,\n                            SourceSpan(file=filename, line=line + 1, column=col + 1),\n                        )\n                except (AttributeError, KeyError, TypeError):\n                    # Fallback: use the map's own position\n                    try:\n                        lc = data.lc\n                        source_map.add(\n                            key_path,\n                            SourceSpan(file=filename, line=lc.line + 1, column=lc.col + 1),\n                        )\n                    except (AttributeError, TypeError):\n                        pass\n                self._extract_positions(data[key], filename, key_path, source_map)\n        elif isinstance(data, CommentedSeq):\n            for i, item in enumerate(data):\n                item_path = f\"{prefix}[{i}]\"\n                try:\n                    lc = data.lc\n                    item_pos = lc.item(i)\n                    if item_pos:\n                        line, col = item_pos\n                        source_map.add(\n                            item_path,\n                            SourceSpan(file=filename, line=line + 1, column=col + 1),\n                        )\n                except (AttributeError, KeyError, TypeError):\n                    pass\n                self._extract_positions(item, filename, item_path, source_map)\n\n    def _to_plain_dict(self, data: Any) -&gt; dict[str, Any]:\n        \"\"\"Convert ruamel.yaml CommentedMap/Seq to plain Python dict/list.\"\"\"\n        if isinstance(data, CommentedMap):\n            return {str(k): self._to_plain_value(v) for k, v in data.items()}\n        if isinstance(data, dict):\n            return {str(k): self._to_plain_value(v) for k, v in data.items()}\n        return {}\n\n    def _to_plain_value(self, data: Any) -&gt; Any:\n        if isinstance(data, CommentedMap):\n            return {str(k): self._to_plain_value(v) for k, v in data.items()}\n        if isinstance(data, CommentedSeq):\n            return [self._to_plain_value(item) for item in data]\n        if isinstance(data, dict):\n            return {str(k): self._to_plain_value(v) for k, v in data.items()}\n        if isinstance(data, list):\n            return [self._to_plain_value(item) for item in data]\n        return data\n</code></pre>"},{"location":"reference/python-api/#orionbelt.parser.loader.TrackedLoader.load","title":"<code>load(path)</code>","text":"<p>Load a YAML file and return parsed dict + source position map.</p> Source code in <code>src/orionbelt/parser/loader.py</code> <pre><code>def load(self, path: Path) -&gt; tuple[dict[str, Any], SourceMap]:\n    \"\"\"Load a YAML file and return parsed dict + source position map.\"\"\"\n    with path.open(\"r\", encoding=\"utf-8\") as handle:\n        data = self._yaml.load(handle)\n    if data is None:\n        return {}, SourceMap()\n    source_map = SourceMap()\n    self._extract_positions(data, str(path), \"\", source_map)\n    return self._to_plain_dict(data), source_map\n</code></pre>"},{"location":"reference/python-api/#orionbelt.parser.loader.TrackedLoader.load_string","title":"<code>load_string(content, filename='&lt;string&gt;')</code>","text":"<p>Load YAML from a string.</p> Source code in <code>src/orionbelt/parser/loader.py</code> <pre><code>def load_string(\n    self, content: str, filename: str = \"&lt;string&gt;\"\n) -&gt; tuple[dict[str, Any], SourceMap]:\n    \"\"\"Load YAML from a string.\"\"\"\n    data = self._yaml.load(content)\n    if data is None:\n        return {}, SourceMap()\n    source_map = SourceMap()\n    self._extract_positions(data, filename, \"\", source_map)\n    return self._to_plain_dict(data), source_map\n</code></pre>"},{"location":"reference/python-api/#orionbelt.parser.loader.TrackedLoader.load_model_directory","title":"<code>load_model_directory(root)</code>","text":"<p>Load a model directory: model.yaml + facts/.yaml + dimensions/.yaml + measures/*.yaml.</p> <p>Returns a merged dict with all artifacts and a combined source map.</p> Source code in <code>src/orionbelt/parser/loader.py</code> <pre><code>def load_model_directory(self, root: Path) -&gt; tuple[dict[str, Any], SourceMap]:\n    \"\"\"Load a model directory: model.yaml + facts/*.yaml + dimensions/*.yaml + measures/*.yaml.\n\n    Returns a merged dict with all artifacts and a combined source map.\n    \"\"\"\n    merged: dict[str, Any] = {}\n    combined_map = SourceMap()\n\n    # Load model.yaml (root file)\n    model_file = root / \"model.yaml\"\n    if model_file.exists():\n        data, smap = self.load(model_file)\n        merged.update(data)\n        combined_map.merge(smap)\n\n    # Load subdirectory YAML files\n    for subdir in (\"facts\", \"dimensions\", \"measures\", \"macros\", \"policies\"):\n        subdir_path = root / subdir\n        if subdir_path.is_dir():\n            section: dict[str, Any] = merged.get(subdir, {})\n            for yaml_file in sorted(subdir_path.glob(\"*.yaml\")):\n                data, smap = self.load(yaml_file)\n                if isinstance(data, dict):\n                    # Use the filename stem as key if the file is a single artifact\n                    if \"name\" in data:\n                        section[data[\"name\"]] = data\n                    else:\n                        section.update(data)\n                combined_map.merge(smap)\n            if section:\n                merged[subdir] = section\n\n    return merged, combined_map\n</code></pre>"},{"location":"reference/python-api/#reference-resolver","title":"Reference Resolver","text":""},{"location":"reference/python-api/#orionbelt.parser.resolver.ReferenceResolver","title":"<code>orionbelt.parser.resolver.ReferenceResolver</code>","text":"<p>Resolves all references in a raw YAML model to a fully-typed SemanticModel.</p> Source code in <code>src/orionbelt/parser/resolver.py</code> <pre><code>class ReferenceResolver:\n    \"\"\"Resolves all references in a raw YAML model to a fully-typed SemanticModel.\"\"\"\n\n    def resolve(\n        self,\n        raw: dict[str, Any],\n        source_map: SourceMap | None = None,\n    ) -&gt; tuple[SemanticModel, ValidationResult]:\n        \"\"\"Resolve raw YAML dict into a validated SemanticModel.\n\n        Returns (model, validation_result). If there are errors,\n        the model may be partially populated.\n        \"\"\"\n        errors: list[SemanticError] = []\n        warnings: list[SemanticError] = []\n\n        # Parse data objects\n        data_objects: dict[str, DataObject] = {}\n        raw_objects = raw.get(\"dataObjects\", {})\n        if not isinstance(raw_objects, dict):\n            errors.append(\n                SemanticError(\n                    code=\"DATA_OBJECT_PARSE_ERROR\",\n                    message=\"'dataObjects' must be a YAML mapping, not a list or scalar\",\n                    path=\"dataObjects\",\n                )\n            )\n            raw_objects = {}\n        for name, raw_obj in raw_objects.items():\n            try:\n                obj_columns: dict[str, DataObjectColumn] = {}\n                for fname, fdata in raw_obj.get(\"columns\", {}).items():\n                    obj_columns[fname] = DataObjectColumn(\n                        label=fname,\n                        code=fdata.get(\"code\", fname),\n                        abstract_type=fdata.get(\"abstractType\", \"string\"),\n                        sql_type=fdata.get(\"sqlType\"),\n                        sql_precision=fdata.get(\"sqlPrecision\"),\n                        sql_scale=fdata.get(\"sqlScale\"),\n                        comment=fdata.get(\"comment\"),\n                        custom_extensions=_parse_extensions(fdata),\n                    )\n\n                obj_joins: list[DataObjectJoin] = []\n                for jdata in raw_obj.get(\"joins\", []):\n                    obj_joins.append(\n                        DataObjectJoin(\n                            join_type=jdata[\"joinType\"],\n                            join_to=jdata[\"joinTo\"],\n                            columns_from=jdata[\"columnsFrom\"],\n                            columns_to=jdata[\"columnsTo\"],\n                            secondary=jdata.get(\"secondary\", False),\n                            path_name=jdata.get(\"pathName\"),\n                        )\n                    )\n\n                data_objects[name] = DataObject(\n                    label=name,\n                    code=raw_obj.get(\"code\", \"\"),\n                    database=raw_obj.get(\"database\", \"\"),\n                    schema_name=raw_obj.get(\"schema\", \"\"),\n                    columns=obj_columns,\n                    joins=obj_joins,\n                    comment=raw_obj.get(\"comment\"),\n                    custom_extensions=_parse_extensions(raw_obj),\n                )\n            except Exception as e:\n                span = source_map.get(f\"dataObjects.{name}\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"DATA_OBJECT_PARSE_ERROR\",\n                        message=f\"Failed to parse data object '{name}': {e}\",\n                        path=f\"dataObjects.{name}\",\n                        span=span,\n                    )\n                )\n\n        # Parse dimensions\n        dimensions: dict[str, Dimension] = {}\n        raw_dims = raw.get(\"dimensions\", {})\n        if not isinstance(raw_dims, dict):\n            errors.append(\n                SemanticError(\n                    code=\"DIMENSION_PARSE_ERROR\",\n                    message=\"'dimensions' must be a YAML mapping, not a list or scalar\",\n                    path=\"dimensions\",\n                )\n            )\n            raw_dims = {}\n        for name, raw_dim in raw_dims.items():\n            try:\n                data_object = raw_dim.get(\"dataObject\")\n                column = raw_dim.get(\"column\")\n\n                # Validate the data object exists\n                if data_object and data_object not in data_objects:\n                    span = source_map.get(f\"dimensions.{name}\") if source_map else None\n                    errors.append(\n                        SemanticError(\n                            code=\"UNKNOWN_DATA_OBJECT\",\n                            message=(\n                                f\"Dimension '{name}' references unknown data object '{data_object}'\"\n                            ),\n                            path=f\"dimensions.{name}\",\n                            span=span,\n                            suggestions=_suggest_similar(data_object, list(data_objects.keys())),\n                        )\n                    )\n\n                # Validate the column exists in the data object\n                if (\n                    data_object\n                    and column\n                    and data_object in data_objects\n                    and column not in data_objects[data_object].columns\n                ):\n                    span = source_map.get(f\"dimensions.{name}\") if source_map else None\n                    errors.append(\n                        SemanticError(\n                            code=\"UNKNOWN_COLUMN\",\n                            message=(\n                                f\"Dimension '{name}' references unknown column \"\n                                f\"'{column}' in data object '{data_object}'\"\n                            ),\n                            path=f\"dimensions.{name}\",\n                            span=span,\n                            suggestions=_suggest_similar(\n                                column, list(data_objects[data_object].columns.keys())\n                            ),\n                        )\n                    )\n\n                dimensions[name] = Dimension(\n                    label=name,\n                    view=data_object or \"\",\n                    column=column or \"\",\n                    result_type=raw_dim.get(\"resultType\", \"string\"),\n                    time_grain=raw_dim.get(\"timeGrain\"),\n                    format=raw_dim.get(\"format\"),\n                    custom_extensions=_parse_extensions(raw_dim),\n                )\n            except Exception as e:\n                span = source_map.get(f\"dimensions.{name}\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"DIMENSION_PARSE_ERROR\",\n                        message=f\"Failed to parse dimension '{name}': {e}\",\n                        path=f\"dimensions.{name}\",\n                        span=span,\n                    )\n                )\n\n        # Parse measures\n        measures: dict[str, Measure] = {}\n        raw_measures = raw.get(\"measures\", {})\n        if not isinstance(raw_measures, dict):\n            errors.append(\n                SemanticError(\n                    code=\"MEASURE_PARSE_ERROR\",\n                    message=\"'measures' must be a YAML mapping, not a list or scalar\",\n                    path=\"measures\",\n                )\n            )\n            raw_measures = {}\n        for name, raw_meas in raw_measures.items():\n            try:\n                measure_columns: list[DataColumnRef] = []\n                for fdata in raw_meas.get(\"columns\", []):\n                    measure_columns.append(\n                        DataColumnRef(\n                            view=fdata.get(\"dataObject\"),\n                            column=fdata.get(\"column\"),\n                        )\n                    )\n\n                # Resolve expression field references\n                expression = raw_meas.get(\"expression\")\n                if expression:\n                    self._validate_expression_refs(\n                        name, expression, data_objects, errors, source_map\n                    )\n\n                mfilter = None\n                raw_filter = raw_meas.get(\"filter\")\n                if raw_filter:\n                    filter_values = []\n                    for vdata in raw_filter.get(\"values\", []):\n                        filter_values.append(\n                            FilterValue(\n                                data_type=vdata.get(\"dataType\", \"string\"),\n                                is_null=vdata.get(\"isNull\"),\n                                value_string=vdata.get(\"valueString\"),\n                                value_int=vdata.get(\"valueInt\"),\n                                value_float=vdata.get(\"valueFloat\"),\n                                value_date=vdata.get(\"valueDate\"),\n                                value_boolean=vdata.get(\"valueBoolean\"),\n                            )\n                        )\n                    filter_column = None\n                    if \"column\" in raw_filter:\n                        filter_column = DataColumnRef(\n                            view=raw_filter[\"column\"].get(\"dataObject\"),\n                            column=raw_filter[\"column\"].get(\"column\"),\n                        )\n                    mfilter = MeasureFilter(\n                        column=filter_column,\n                        operator=raw_filter.get(\"operator\", \"equals\"),\n                        values=filter_values,\n                    )\n\n                measures[name] = Measure(\n                    label=name,\n                    columns=measure_columns,\n                    result_type=raw_meas.get(\"resultType\", \"float\"),\n                    aggregation=raw_meas.get(\"aggregation\", \"sum\"),\n                    expression=expression,\n                    distinct=raw_meas.get(\"distinct\", False),\n                    total=raw_meas.get(\"total\", False),\n                    filter=mfilter,\n                    format=raw_meas.get(\"format\"),\n                    allow_fan_out=raw_meas.get(\"allowFanOut\", False),\n                    custom_extensions=_parse_extensions(raw_meas),\n                )\n            except Exception as e:\n                span = source_map.get(f\"measures.{name}\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"MEASURE_PARSE_ERROR\",\n                        message=f\"Failed to parse measure '{name}': {e}\",\n                        path=f\"measures.{name}\",\n                        span=span,\n                    )\n                )\n\n        # Parse metrics\n        metrics: dict[str, Metric] = {}\n        raw_metrics = raw.get(\"metrics\", {})\n        if not isinstance(raw_metrics, dict):\n            errors.append(\n                SemanticError(\n                    code=\"METRIC_PARSE_ERROR\",\n                    message=\"'metrics' must be a YAML mapping, not a list or scalar\",\n                    path=\"metrics\",\n                )\n            )\n            raw_metrics = {}\n        for name, raw_metric in raw_metrics.items():\n            try:\n                # Validate measure references in expression\n                expression = raw_metric.get(\"expression\", \"\")\n                self._validate_metric_expression_refs(\n                    name, expression, measures, errors, source_map\n                )\n\n                metrics[name] = Metric(\n                    label=name,\n                    expression=expression,\n                    format=raw_metric.get(\"format\"),\n                    custom_extensions=_parse_extensions(raw_metric),\n                )\n            except Exception as e:\n                span = source_map.get(f\"metrics.{name}\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"METRIC_PARSE_ERROR\",\n                        message=f\"Failed to parse metric '{name}': {e}\",\n                        path=f\"metrics.{name}\",\n                        span=span,\n                    )\n                )\n\n        model = SemanticModel(\n            version=raw.get(\"version\", 1.0),\n            data_objects=data_objects,\n            dimensions=dimensions,\n            measures=measures,\n            metrics=metrics,\n            custom_extensions=_parse_extensions(raw),\n        )\n\n        result = ValidationResult(\n            valid=len(errors) == 0,\n            errors=errors,\n            warnings=warnings,\n        )\n\n        return model, result\n\n    def _validate_expression_refs(\n        self,\n        measure_name: str,\n        expression: str,\n        data_objects: dict[str, DataObject],\n        errors: list[SemanticError],\n        source_map: SourceMap | None,\n    ) -&gt; None:\n        \"\"\"Validate {[DataObject].[Column]} references in a measure expression.\"\"\"\n        named_refs = re.findall(r\"\\{\\[([^\\]]+)\\]\\.\\[([^\\]]+)\\]\\}\", expression)\n        for obj_name, col_name in named_refs:\n            span = source_map.get(f\"measures.{measure_name}.expression\") if source_map else None\n            if obj_name not in data_objects:\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_DATA_OBJECT_IN_EXPRESSION\",\n                        message=(\n                            f\"Measure '{measure_name}' expression references unknown \"\n                            f\"data object '{obj_name}'\"\n                        ),\n                        path=f\"measures.{measure_name}.expression\",\n                        span=span,\n                    )\n                )\n            elif col_name not in data_objects[obj_name].columns:\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_COLUMN_IN_EXPRESSION\",\n                        message=(\n                            f\"Measure '{measure_name}' expression references unknown column \"\n                            f\"'{col_name}' in data object '{obj_name}'\"\n                        ),\n                        path=f\"measures.{measure_name}.expression\",\n                        span=span,\n                    )\n                )\n\n    def _validate_metric_expression_refs(\n        self,\n        metric_name: str,\n        expression: str,\n        measures: dict[str, Measure],\n        errors: list[SemanticError],\n        source_map: SourceMap | None,\n    ) -&gt; None:\n        \"\"\"Validate {[Measure Name]} references in a metric expression.\"\"\"\n        # Extract all {[Name]} references from the expression\n        named_refs = re.findall(r\"\\{\\[([^\\]]+)\\]\\}\", expression)\n        for ref_name in named_refs:\n            if ref_name not in measures:\n                span = source_map.get(f\"metrics.{metric_name}.expression\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_MEASURE_REF\",\n                        message=(f\"Metric '{metric_name}' references unknown measure '{ref_name}'\"),\n                        path=f\"metrics.{metric_name}.expression\",\n                        span=span,\n                        suggestions=_suggest_similar(ref_name, list(measures.keys())),\n                    )\n                )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.parser.resolver.ReferenceResolver.resolve","title":"<code>resolve(raw, source_map=None)</code>","text":"<p>Resolve raw YAML dict into a validated SemanticModel.</p> <p>Returns (model, validation_result). If there are errors, the model may be partially populated.</p> Source code in <code>src/orionbelt/parser/resolver.py</code> <pre><code>def resolve(\n    self,\n    raw: dict[str, Any],\n    source_map: SourceMap | None = None,\n) -&gt; tuple[SemanticModel, ValidationResult]:\n    \"\"\"Resolve raw YAML dict into a validated SemanticModel.\n\n    Returns (model, validation_result). If there are errors,\n    the model may be partially populated.\n    \"\"\"\n    errors: list[SemanticError] = []\n    warnings: list[SemanticError] = []\n\n    # Parse data objects\n    data_objects: dict[str, DataObject] = {}\n    raw_objects = raw.get(\"dataObjects\", {})\n    if not isinstance(raw_objects, dict):\n        errors.append(\n            SemanticError(\n                code=\"DATA_OBJECT_PARSE_ERROR\",\n                message=\"'dataObjects' must be a YAML mapping, not a list or scalar\",\n                path=\"dataObjects\",\n            )\n        )\n        raw_objects = {}\n    for name, raw_obj in raw_objects.items():\n        try:\n            obj_columns: dict[str, DataObjectColumn] = {}\n            for fname, fdata in raw_obj.get(\"columns\", {}).items():\n                obj_columns[fname] = DataObjectColumn(\n                    label=fname,\n                    code=fdata.get(\"code\", fname),\n                    abstract_type=fdata.get(\"abstractType\", \"string\"),\n                    sql_type=fdata.get(\"sqlType\"),\n                    sql_precision=fdata.get(\"sqlPrecision\"),\n                    sql_scale=fdata.get(\"sqlScale\"),\n                    comment=fdata.get(\"comment\"),\n                    custom_extensions=_parse_extensions(fdata),\n                )\n\n            obj_joins: list[DataObjectJoin] = []\n            for jdata in raw_obj.get(\"joins\", []):\n                obj_joins.append(\n                    DataObjectJoin(\n                        join_type=jdata[\"joinType\"],\n                        join_to=jdata[\"joinTo\"],\n                        columns_from=jdata[\"columnsFrom\"],\n                        columns_to=jdata[\"columnsTo\"],\n                        secondary=jdata.get(\"secondary\", False),\n                        path_name=jdata.get(\"pathName\"),\n                    )\n                )\n\n            data_objects[name] = DataObject(\n                label=name,\n                code=raw_obj.get(\"code\", \"\"),\n                database=raw_obj.get(\"database\", \"\"),\n                schema_name=raw_obj.get(\"schema\", \"\"),\n                columns=obj_columns,\n                joins=obj_joins,\n                comment=raw_obj.get(\"comment\"),\n                custom_extensions=_parse_extensions(raw_obj),\n            )\n        except Exception as e:\n            span = source_map.get(f\"dataObjects.{name}\") if source_map else None\n            errors.append(\n                SemanticError(\n                    code=\"DATA_OBJECT_PARSE_ERROR\",\n                    message=f\"Failed to parse data object '{name}': {e}\",\n                    path=f\"dataObjects.{name}\",\n                    span=span,\n                )\n            )\n\n    # Parse dimensions\n    dimensions: dict[str, Dimension] = {}\n    raw_dims = raw.get(\"dimensions\", {})\n    if not isinstance(raw_dims, dict):\n        errors.append(\n            SemanticError(\n                code=\"DIMENSION_PARSE_ERROR\",\n                message=\"'dimensions' must be a YAML mapping, not a list or scalar\",\n                path=\"dimensions\",\n            )\n        )\n        raw_dims = {}\n    for name, raw_dim in raw_dims.items():\n        try:\n            data_object = raw_dim.get(\"dataObject\")\n            column = raw_dim.get(\"column\")\n\n            # Validate the data object exists\n            if data_object and data_object not in data_objects:\n                span = source_map.get(f\"dimensions.{name}\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_DATA_OBJECT\",\n                        message=(\n                            f\"Dimension '{name}' references unknown data object '{data_object}'\"\n                        ),\n                        path=f\"dimensions.{name}\",\n                        span=span,\n                        suggestions=_suggest_similar(data_object, list(data_objects.keys())),\n                    )\n                )\n\n            # Validate the column exists in the data object\n            if (\n                data_object\n                and column\n                and data_object in data_objects\n                and column not in data_objects[data_object].columns\n            ):\n                span = source_map.get(f\"dimensions.{name}\") if source_map else None\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_COLUMN\",\n                        message=(\n                            f\"Dimension '{name}' references unknown column \"\n                            f\"'{column}' in data object '{data_object}'\"\n                        ),\n                        path=f\"dimensions.{name}\",\n                        span=span,\n                        suggestions=_suggest_similar(\n                            column, list(data_objects[data_object].columns.keys())\n                        ),\n                    )\n                )\n\n            dimensions[name] = Dimension(\n                label=name,\n                view=data_object or \"\",\n                column=column or \"\",\n                result_type=raw_dim.get(\"resultType\", \"string\"),\n                time_grain=raw_dim.get(\"timeGrain\"),\n                format=raw_dim.get(\"format\"),\n                custom_extensions=_parse_extensions(raw_dim),\n            )\n        except Exception as e:\n            span = source_map.get(f\"dimensions.{name}\") if source_map else None\n            errors.append(\n                SemanticError(\n                    code=\"DIMENSION_PARSE_ERROR\",\n                    message=f\"Failed to parse dimension '{name}': {e}\",\n                    path=f\"dimensions.{name}\",\n                    span=span,\n                )\n            )\n\n    # Parse measures\n    measures: dict[str, Measure] = {}\n    raw_measures = raw.get(\"measures\", {})\n    if not isinstance(raw_measures, dict):\n        errors.append(\n            SemanticError(\n                code=\"MEASURE_PARSE_ERROR\",\n                message=\"'measures' must be a YAML mapping, not a list or scalar\",\n                path=\"measures\",\n            )\n        )\n        raw_measures = {}\n    for name, raw_meas in raw_measures.items():\n        try:\n            measure_columns: list[DataColumnRef] = []\n            for fdata in raw_meas.get(\"columns\", []):\n                measure_columns.append(\n                    DataColumnRef(\n                        view=fdata.get(\"dataObject\"),\n                        column=fdata.get(\"column\"),\n                    )\n                )\n\n            # Resolve expression field references\n            expression = raw_meas.get(\"expression\")\n            if expression:\n                self._validate_expression_refs(\n                    name, expression, data_objects, errors, source_map\n                )\n\n            mfilter = None\n            raw_filter = raw_meas.get(\"filter\")\n            if raw_filter:\n                filter_values = []\n                for vdata in raw_filter.get(\"values\", []):\n                    filter_values.append(\n                        FilterValue(\n                            data_type=vdata.get(\"dataType\", \"string\"),\n                            is_null=vdata.get(\"isNull\"),\n                            value_string=vdata.get(\"valueString\"),\n                            value_int=vdata.get(\"valueInt\"),\n                            value_float=vdata.get(\"valueFloat\"),\n                            value_date=vdata.get(\"valueDate\"),\n                            value_boolean=vdata.get(\"valueBoolean\"),\n                        )\n                    )\n                filter_column = None\n                if \"column\" in raw_filter:\n                    filter_column = DataColumnRef(\n                        view=raw_filter[\"column\"].get(\"dataObject\"),\n                        column=raw_filter[\"column\"].get(\"column\"),\n                    )\n                mfilter = MeasureFilter(\n                    column=filter_column,\n                    operator=raw_filter.get(\"operator\", \"equals\"),\n                    values=filter_values,\n                )\n\n            measures[name] = Measure(\n                label=name,\n                columns=measure_columns,\n                result_type=raw_meas.get(\"resultType\", \"float\"),\n                aggregation=raw_meas.get(\"aggregation\", \"sum\"),\n                expression=expression,\n                distinct=raw_meas.get(\"distinct\", False),\n                total=raw_meas.get(\"total\", False),\n                filter=mfilter,\n                format=raw_meas.get(\"format\"),\n                allow_fan_out=raw_meas.get(\"allowFanOut\", False),\n                custom_extensions=_parse_extensions(raw_meas),\n            )\n        except Exception as e:\n            span = source_map.get(f\"measures.{name}\") if source_map else None\n            errors.append(\n                SemanticError(\n                    code=\"MEASURE_PARSE_ERROR\",\n                    message=f\"Failed to parse measure '{name}': {e}\",\n                    path=f\"measures.{name}\",\n                    span=span,\n                )\n            )\n\n    # Parse metrics\n    metrics: dict[str, Metric] = {}\n    raw_metrics = raw.get(\"metrics\", {})\n    if not isinstance(raw_metrics, dict):\n        errors.append(\n            SemanticError(\n                code=\"METRIC_PARSE_ERROR\",\n                message=\"'metrics' must be a YAML mapping, not a list or scalar\",\n                path=\"metrics\",\n            )\n        )\n        raw_metrics = {}\n    for name, raw_metric in raw_metrics.items():\n        try:\n            # Validate measure references in expression\n            expression = raw_metric.get(\"expression\", \"\")\n            self._validate_metric_expression_refs(\n                name, expression, measures, errors, source_map\n            )\n\n            metrics[name] = Metric(\n                label=name,\n                expression=expression,\n                format=raw_metric.get(\"format\"),\n                custom_extensions=_parse_extensions(raw_metric),\n            )\n        except Exception as e:\n            span = source_map.get(f\"metrics.{name}\") if source_map else None\n            errors.append(\n                SemanticError(\n                    code=\"METRIC_PARSE_ERROR\",\n                    message=f\"Failed to parse metric '{name}': {e}\",\n                    path=f\"metrics.{name}\",\n                    span=span,\n                )\n            )\n\n    model = SemanticModel(\n        version=raw.get(\"version\", 1.0),\n        data_objects=data_objects,\n        dimensions=dimensions,\n        measures=measures,\n        metrics=metrics,\n        custom_extensions=_parse_extensions(raw),\n    )\n\n    result = ValidationResult(\n        valid=len(errors) == 0,\n        errors=errors,\n        warnings=warnings,\n    )\n\n    return model, result\n</code></pre>"},{"location":"reference/python-api/#semantic-validator","title":"Semantic Validator","text":""},{"location":"reference/python-api/#orionbelt.parser.validator.SemanticValidator","title":"<code>orionbelt.parser.validator.SemanticValidator</code>","text":"<p>Validates semantic rules from spec \u00a73.8.</p> Source code in <code>src/orionbelt/parser/validator.py</code> <pre><code>class SemanticValidator:\n    \"\"\"Validates semantic rules from spec \u00a73.8.\"\"\"\n\n    def validate(self, model: SemanticModel) -&gt; list[SemanticError]:\n        errors: list[SemanticError] = []\n        errors.extend(self._check_unique_identifiers(model))\n        errors.extend(self._check_unique_column_names(model))\n        errors.extend(self._check_secondary_joins(model))\n        errors.extend(self._check_no_cyclic_joins(model))\n        errors.extend(self._check_no_multipath_joins(model))\n        errors.extend(self._check_measures_resolve(model))\n        errors.extend(self._check_join_targets_exist(model))\n        errors.extend(self._check_references_resolve(model))\n        return errors\n\n    def _check_unique_identifiers(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Ensure no duplicate names across data objects, dimensions, measures, metrics.\"\"\"\n        errors: list[SemanticError] = []\n        all_names: dict[str, str] = {}  # name -&gt; type\n\n        def _register(name: str, kind: str, path: str) -&gt; None:\n            existing = all_names.get(name)\n            if existing is not None:\n                errors.append(\n                    SemanticError(\n                        code=\"DUPLICATE_IDENTIFIER\",\n                        message=(\n                            f\"{kind.title()} '{name}' conflicts with existing {existing} '{name}'\"\n                        ),\n                        path=path,\n                    )\n                )\n            all_names[name] = kind\n\n        for name in model.data_objects:\n            _register(name, \"dataObject\", f\"dataObjects.{name}\")\n\n        for name in model.dimensions:\n            _register(name, \"dimension\", f\"dimensions.{name}\")\n\n        for name in model.measures:\n            _register(name, \"measure\", f\"measures.{name}\")\n\n        for name in model.metrics:\n            _register(name, \"metric\", f\"metrics.{name}\")\n\n        return errors\n\n    def _check_unique_column_names(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Column names must be unique within each data object.\n\n        Since columns are stored as dict keys, YAML parsers silently drop\n        duplicates. This validator therefore returns no errors \u2014 uniqueness\n        is structurally enforced by the dict representation. The method is\n        retained as a hook for future stricter duplicate-key detection at\n        the YAML parse level.\n        \"\"\"\n        return []\n\n    def _check_secondary_joins(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Validate secondary join constraints.\n\n        - Every secondary join MUST have a pathName.\n        - pathName must be unique per (source, target) pair.\n        \"\"\"\n        errors: list[SemanticError] = []\n        # Track pathName per (source, target) pair\n        path_names: dict[tuple[str, str], set[str]] = {}\n\n        for obj_name, obj in model.data_objects.items():\n            for i, join in enumerate(obj.joins):\n                if join.secondary and not join.path_name:\n                    errors.append(\n                        SemanticError(\n                            code=\"SECONDARY_JOIN_MISSING_PATH_NAME\",\n                            message=(\n                                f\"Data object '{obj_name}' join[{i}] is secondary \"\n                                f\"but has no pathName\"\n                            ),\n                            path=f\"dataObjects.{obj_name}.joins[{i}]\",\n                        )\n                    )\n                if join.path_name:\n                    pair = (obj_name, join.join_to)\n                    if pair not in path_names:\n                        path_names[pair] = set()\n                    if join.path_name in path_names[pair]:\n                        errors.append(\n                            SemanticError(\n                                code=\"DUPLICATE_JOIN_PATH_NAME\",\n                                message=(\n                                    f\"Data object '{obj_name}' join[{i}] has duplicate \"\n                                    f\"pathName '{join.path_name}' for target '{join.join_to}'\"\n                                ),\n                                path=f\"dataObjects.{obj_name}.joins[{i}]\",\n                            )\n                        )\n                    else:\n                        path_names[pair].add(join.path_name)\n\n        return errors\n\n    def _check_no_cyclic_joins(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Detect cyclic join paths.\"\"\"\n        errors: list[SemanticError] = []\n\n        # Build adjacency list from joins (skip secondary joins)\n        adj: dict[str, set[str]] = {}\n        for obj_name, obj in model.data_objects.items():\n            if obj_name not in adj:\n                adj[obj_name] = set()\n            for join in obj.joins:\n                if not join.secondary:\n                    adj[obj_name].add(join.join_to)\n\n        # DFS cycle detection\n        visited: set[str] = set()\n        rec_stack: set[str] = set()\n\n        def _dfs(node: str, path: list[str]) -&gt; None:\n            visited.add(node)\n            rec_stack.add(node)\n            for neighbor in adj.get(node, set()):\n                if neighbor not in visited:\n                    _dfs(neighbor, path + [neighbor])\n                elif neighbor in rec_stack:\n                    if neighbor in path:\n                        cycle = path[path.index(neighbor) :] + [neighbor]\n                    else:\n                        cycle = [node, neighbor]\n                    errors.append(\n                        SemanticError(\n                            code=\"CYCLIC_JOIN\",\n                            message=f\"Cyclic join detected: {' -&gt; '.join(cycle)}\",\n                            path=f\"dataObjects.{node}.joins\",\n                        )\n                    )\n            rec_stack.discard(node)\n\n        for node in adj:\n            if node not in visited:\n                _dfs(node, [node])\n\n        return errors\n\n    def _check_no_multipath_joins(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Detect multiple distinct paths between any pair of nodes in the join DAG.\n\n        Only flags true diamonds where both paths go through intermediaries.\n        A direct edge from start to target is canonical, so an additional\n        indirect path (e.g. Purchases\u2192Suppliers direct + Purchases\u2192Products\u2192Suppliers)\n        is not ambiguous and is not flagged.\n        \"\"\"\n        errors: list[SemanticError] = []\n\n        # Build adjacency list from joins (skip secondary joins)\n        adj: dict[str, list[str]] = {}\n        for obj_name, obj in model.data_objects.items():\n            if obj_name not in adj:\n                adj[obj_name] = []\n            for join in obj.joins:\n                if not join.secondary:\n                    adj[obj_name].append(join.join_to)\n\n        reported: set[tuple[str, str]] = set()\n\n        for start in adj:\n            if not adj[start]:\n                continue\n            # BFS from start; track first parent that reached each node\n            direct_neighbors: set[str] = set()\n            first_parent: dict[str, str] = {}\n            queue: deque[tuple[str, str]] = deque()\n            for neighbor in adj[start]:\n                if neighbor == start:\n                    continue\n                direct_neighbors.add(neighbor)\n                if neighbor not in first_parent:\n                    first_parent[neighbor] = start\n                    queue.append((neighbor, start))\n\n            while queue:\n                node, parent = queue.popleft()\n                for neighbor in adj.get(node, []):\n                    if neighbor == start:\n                        continue\n                    if neighbor not in first_parent:\n                        first_parent[neighbor] = node\n                        queue.append((neighbor, node))\n                    elif first_parent[neighbor] != node:\n                        # Skip if target has a direct edge from start \u2014\n                        # the direct join is the canonical path.\n                        if neighbor in direct_neighbors:\n                            continue\n                        pair = (start, neighbor)\n                        if pair not in reported:\n                            reported.add(pair)\n                            errors.append(\n                                SemanticError(\n                                    code=\"MULTIPATH_JOIN\",\n                                    message=(\n                                        f\"Multiple join paths from '{start}' to \"\n                                        f\"'{neighbor}' (via '{first_parent[neighbor]}' \"\n                                        f\"and '{node}'). \"\n                                        f\"Join paths must be unambiguous.\"\n                                    ),\n                                    path=f\"dataObjects.{start}.joins\",\n                                )\n                            )\n\n        return errors\n\n    def _check_measures_resolve(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Ensure measure column references resolve to actual data object columns.\"\"\"\n        errors: list[SemanticError] = []\n        for name, measure in model.measures.items():\n            for i, col_ref in enumerate(measure.columns):\n                obj_name = col_ref.view\n                col_name = col_ref.column\n                if obj_name and obj_name not in model.data_objects:\n                    errors.append(\n                        SemanticError(\n                            code=\"UNKNOWN_DATA_OBJECT\",\n                            message=(\n                                f\"Measure '{name}' column[{i}] references \"\n                                f\"unknown data object '{obj_name}'\"\n                            ),\n                            path=f\"measures.{name}.columns[{i}]\",\n                        )\n                    )\n                elif obj_name and col_name:\n                    obj = model.data_objects[obj_name]\n                    if col_name not in obj.columns:\n                        errors.append(\n                            SemanticError(\n                                code=\"UNKNOWN_COLUMN\",\n                                message=(\n                                    f\"Measure '{name}' column[{i}] references \"\n                                    f\"unknown column '{col_name}' in data object '{obj_name}'\"\n                                ),\n                                path=f\"measures.{name}.columns[{i}]\",\n                            )\n                        )\n        return errors\n\n    def _check_join_targets_exist(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Ensure join targets reference existing data objects.\"\"\"\n        errors: list[SemanticError] = []\n        for obj_name, obj in model.data_objects.items():\n            for i, join in enumerate(obj.joins):\n                if len(join.columns_from) != len(join.columns_to):\n                    errors.append(\n                        SemanticError(\n                            code=\"JOIN_COLUMN_COUNT_MISMATCH\",\n                            message=(\n                                f\"Data object '{obj_name}' join[{i}] has \"\n                                f\"{len(join.columns_from)} columnsFrom and \"\n                                f\"{len(join.columns_to)} columnsTo\"\n                            ),\n                            path=f\"dataObjects.{obj_name}.joins[{i}]\",\n                        )\n                    )\n                if join.join_to not in model.data_objects:\n                    errors.append(\n                        SemanticError(\n                            code=\"UNKNOWN_JOIN_TARGET\",\n                            message=(\n                                f\"Data object '{obj_name}' join[{i}] references \"\n                                f\"unknown data object '{join.join_to}'\"\n                            ),\n                            path=f\"dataObjects.{obj_name}.joins[{i}]\",\n                        )\n                    )\n                else:\n                    # Validate join columns exist\n                    for col_name in join.columns_from:\n                        if col_name not in obj.columns:\n                            errors.append(\n                                SemanticError(\n                                    code=\"UNKNOWN_JOIN_COLUMN\",\n                                    message=(\n                                        f\"Data object '{obj_name}' join[{i}] columnsFrom \"\n                                        f\"references unknown column '{col_name}'\"\n                                    ),\n                                    path=f\"dataObjects.{obj_name}.joins[{i}].columnsFrom\",\n                                )\n                            )\n                    target_obj = model.data_objects[join.join_to]\n                    for col_name in join.columns_to:\n                        if col_name not in target_obj.columns:\n                            errors.append(\n                                SemanticError(\n                                    code=\"UNKNOWN_JOIN_COLUMN\",\n                                    message=(\n                                        f\"Data object '{obj_name}' join[{i}] columnsTo \"\n                                        f\"references unknown column '{col_name}' \"\n                                        f\"in data object '{join.join_to}'\"\n                                    ),\n                                    path=f\"dataObjects.{obj_name}.joins[{i}].columnsTo\",\n                                )\n                            )\n        return errors\n\n    def _check_references_resolve(self, model: SemanticModel) -&gt; list[SemanticError]:\n        \"\"\"Ensure dimension references resolve.\"\"\"\n        errors: list[SemanticError] = []\n        for name, dim in model.dimensions.items():\n            obj_name = dim.view\n            col_name = dim.column\n            if obj_name and obj_name not in model.data_objects:\n                errors.append(\n                    SemanticError(\n                        code=\"UNKNOWN_DATA_OBJECT\",\n                        message=f\"Dimension '{name}' references unknown data object '{obj_name}'\",\n                        path=f\"dimensions.{name}\",\n                    )\n                )\n            elif obj_name and col_name:\n                obj = model.data_objects[obj_name]\n                if col_name not in obj.columns:\n                    errors.append(\n                        SemanticError(\n                            code=\"UNKNOWN_COLUMN\",\n                            message=(\n                                f\"Dimension '{name}' references unknown column \"\n                                f\"'{col_name}' in data object '{obj_name}'\"\n                            ),\n                            path=f\"dimensions.{name}\",\n                        )\n                    )\n        return errors\n</code></pre>"},{"location":"reference/python-api/#orionbelt.parser.validator.SemanticValidator.validate","title":"<code>validate(model)</code>","text":"Source code in <code>src/orionbelt/parser/validator.py</code> <pre><code>def validate(self, model: SemanticModel) -&gt; list[SemanticError]:\n    errors: list[SemanticError] = []\n    errors.extend(self._check_unique_identifiers(model))\n    errors.extend(self._check_unique_column_names(model))\n    errors.extend(self._check_secondary_joins(model))\n    errors.extend(self._check_no_cyclic_joins(model))\n    errors.extend(self._check_no_multipath_joins(model))\n    errors.extend(self._check_measures_resolve(model))\n    errors.extend(self._check_join_targets_exist(model))\n    errors.extend(self._check_references_resolve(model))\n    return errors\n</code></pre>"},{"location":"reference/python-api/#semantic-model","title":"Semantic Model","text":""},{"location":"reference/python-api/#orionbelt.models.semantic.SemanticModel","title":"<code>orionbelt.models.semantic.SemanticModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete semantic model parsed from OBML YAML.</p> Source code in <code>src/orionbelt/models/semantic.py</code> <pre><code>class SemanticModel(BaseModel):\n    \"\"\"Complete semantic model parsed from OBML YAML.\"\"\"\n\n    version: float = 1.0\n    data_objects: dict[str, DataObject] = Field(default={}, alias=\"dataObjects\")\n    dimensions: dict[str, Dimension] = {}\n    measures: dict[str, Measure] = {}\n    metrics: dict[str, Metric] = {}\n    custom_extensions: list[CustomExtension] = Field(\n        default_factory=list, alias=\"customExtensions\"\n    )\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.semantic.DataObject","title":"<code>orionbelt.models.semantic.DataObject</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A database table or view with its columns and joins.</p> Source code in <code>src/orionbelt/models/semantic.py</code> <pre><code>class DataObject(BaseModel):\n    \"\"\"A database table or view with its columns and joins.\"\"\"\n\n    label: str\n    code: str\n    database: str\n    schema_name: str = Field(alias=\"schema\")\n    columns: dict[str, DataObjectColumn] = {}\n    joins: list[DataObjectJoin] = []\n    comment: str | None = None\n    custom_extensions: list[CustomExtension] = Field(\n        default_factory=list, alias=\"customExtensions\"\n    )\n\n    @property\n    def qualified_code(self) -&gt; str:\n        \"\"\"Full qualified table reference: database.schema.code.\"\"\"\n        return f\"{self.database}.{self.schema_name}.{self.code}\"\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.semantic.DataObject.qualified_code","title":"<code>qualified_code</code>  <code>property</code>","text":"<p>Full qualified table reference: database.schema.code.</p>"},{"location":"reference/python-api/#orionbelt.models.semantic.Dimension","title":"<code>orionbelt.models.semantic.Dimension</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A named dimension referencing a data object column.</p> Source code in <code>src/orionbelt/models/semantic.py</code> <pre><code>class Dimension(BaseModel):\n    \"\"\"A named dimension referencing a data object column.\"\"\"\n\n    label: str\n    view: str = Field(alias=\"dataObject\")\n    column: str = \"\"\n    result_type: DataType = Field(DataType.STRING, alias=\"resultType\")\n    time_grain: TimeGrain | None = Field(None, alias=\"timeGrain\")\n    format: str | None = None\n    custom_extensions: list[CustomExtension] = Field(\n        default_factory=list, alias=\"customExtensions\"\n    )\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.semantic.Measure","title":"<code>orionbelt.models.semantic.Measure</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>An aggregation measure with optional expression template.</p> Source code in <code>src/orionbelt/models/semantic.py</code> <pre><code>class Measure(BaseModel):\n    \"\"\"An aggregation measure with optional expression template.\"\"\"\n\n    label: str\n    columns: list[DataColumnRef] = []\n    result_type: DataType = Field(DataType.FLOAT, alias=\"resultType\")\n    aggregation: str\n    expression: str | None = None\n    distinct: bool = False\n    total: bool = False\n    filter: MeasureFilter | None = None\n    format: str | None = None\n    allow_fan_out: bool = Field(False, alias=\"allowFanOut\")\n    delimiter: str | None = None\n    within_group: WithinGroup | None = Field(None, alias=\"withinGroup\")\n    custom_extensions: list[CustomExtension] = Field(\n        default_factory=list, alias=\"customExtensions\"\n    )\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.semantic.Metric","title":"<code>orionbelt.models.semantic.Metric</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A composite metric combining measures via an expression.</p> <p>The expression references measures by name using <code>{[Measure Name]}</code> syntax.</p> Source code in <code>src/orionbelt/models/semantic.py</code> <pre><code>class Metric(BaseModel):\n    \"\"\"A composite metric combining measures via an expression.\n\n    The expression references measures by name using ``{[Measure Name]}`` syntax.\n    \"\"\"\n\n    label: str\n    expression: str\n    format: str | None = None\n    custom_extensions: list[CustomExtension] = Field(\n        default_factory=list, alias=\"customExtensions\"\n    )\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#query-models","title":"Query Models","text":""},{"location":"reference/python-api/#orionbelt.models.query.QueryObject","title":"<code>orionbelt.models.query.QueryObject</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete YAML analytical query.</p> Source code in <code>src/orionbelt/models/query.py</code> <pre><code>class QueryObject(BaseModel):\n    \"\"\"A complete YAML analytical query.\"\"\"\n\n    select: QuerySelect\n    where: list[QueryFilter] = []\n    having: list[QueryFilter] = []\n    order_by: list[QueryOrderBy] = Field([], alias=\"order_by\")\n    limit: int | None = None\n    use_path_names: list[UsePathName] = Field([], alias=\"usePathNames\")\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.query.QuerySelect","title":"<code>orionbelt.models.query.QuerySelect</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The SELECT part of a query: dimensions + measures.</p> Source code in <code>src/orionbelt/models/query.py</code> <pre><code>class QuerySelect(BaseModel):\n    \"\"\"The SELECT part of a query: dimensions + measures.\"\"\"\n\n    dimensions: list[str] = []\n    measures: list[str] = []\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.query.QueryFilter","title":"<code>orionbelt.models.query.QueryFilter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A filter condition in a query.</p> Source code in <code>src/orionbelt/models/query.py</code> <pre><code>class QueryFilter(BaseModel):\n    \"\"\"A filter condition in a query.\"\"\"\n\n    field: str\n    op: FilterOperator\n    value: Any = None\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.query.UsePathName","title":"<code>orionbelt.models.query.UsePathName</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Selects a named secondary join path for a specific (source, target) pair.</p> Source code in <code>src/orionbelt/models/query.py</code> <pre><code>class UsePathName(BaseModel):\n    \"\"\"Selects a named secondary join path for a specific (source, target) pair.\"\"\"\n\n    source: str\n    target: str\n    path_name: str = Field(alias=\"pathName\")\n\n    model_config = {\"populate_by_name\": True}\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.query.DimensionRef","title":"<code>orionbelt.models.query.DimensionRef</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Reference to a dimension, optionally with time grain.</p> <p>Supports notation like \"customer.country\" or \"order.order_date:month\".</p> Source code in <code>src/orionbelt/models/query.py</code> <pre><code>class DimensionRef(BaseModel):\n    \"\"\"Reference to a dimension, optionally with time grain.\n\n    Supports notation like \"customer.country\" or \"order.order_date:month\".\n    \"\"\"\n\n    name: str\n    grain: TimeGrain | None = None\n\n    @classmethod\n    def parse(cls, raw: str) -&gt; DimensionRef:\n        \"\"\"Parse 'name:grain' notation.\"\"\"\n        if \":\" in raw:\n            name, grain_str = raw.rsplit(\":\", 1)\n            return cls(name=name, grain=TimeGrain(grain_str))\n        return cls(name=raw)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.query.DimensionRef.parse","title":"<code>parse(raw)</code>  <code>classmethod</code>","text":"<p>Parse 'name:grain' notation.</p> Source code in <code>src/orionbelt/models/query.py</code> <pre><code>@classmethod\ndef parse(cls, raw: str) -&gt; DimensionRef:\n    \"\"\"Parse 'name:grain' notation.\"\"\"\n    if \":\" in raw:\n        name, grain_str = raw.rsplit(\":\", 1)\n        return cls(name=name, grain=TimeGrain(grain_str))\n    return cls(name=raw)\n</code></pre>"},{"location":"reference/python-api/#error-models","title":"Error Models","text":""},{"location":"reference/python-api/#orionbelt.models.errors.SemanticError","title":"<code>orionbelt.models.errors.SemanticError</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A structured error with optional source position and suggestions.</p> Source code in <code>src/orionbelt/models/errors.py</code> <pre><code>class SemanticError(BaseModel):\n    \"\"\"A structured error with optional source position and suggestions.\"\"\"\n\n    code: str\n    message: str\n    path: str | None = None\n    span: SourceSpan | None = None\n    suggestions: list[str] = []\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.errors.ValidationResult","title":"<code>orionbelt.models.errors.ValidationResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of semantic model validation.</p> Source code in <code>src/orionbelt/models/errors.py</code> <pre><code>class ValidationResult(BaseModel):\n    \"\"\"Result of semantic model validation.\"\"\"\n\n    valid: bool\n    errors: list[SemanticError] = []\n    warnings: list[SemanticError] = []\n</code></pre>"},{"location":"reference/python-api/#orionbelt.models.errors.SourceSpan","title":"<code>orionbelt.models.errors.SourceSpan</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Points to exact location in YAML source for error reporting.</p> Source code in <code>src/orionbelt/models/errors.py</code> <pre><code>class SourceSpan(BaseModel):\n    \"\"\"Points to exact location in YAML source for error reporting.\"\"\"\n\n    file: str\n    line: int\n    column: int\n    end_line: int | None = None\n    end_column: int | None = None\n</code></pre>"},{"location":"reference/python-api/#sql-ast-nodes","title":"SQL AST Nodes","text":""},{"location":"reference/python-api/#orionbelt.ast.nodes.Select","title":"<code>orionbelt.ast.nodes.Select</code>  <code>dataclass</code>","text":"<p>A complete SELECT statement.</p> Source code in <code>src/orionbelt/ast/nodes.py</code> <pre><code>@dataclass(frozen=True)\nclass Select:\n    \"\"\"A complete SELECT statement.\"\"\"\n\n    columns: list[Expr] = field(default_factory=list)\n    from_: From | None = None\n    joins: list[Join] = field(default_factory=list)\n    where: Expr | None = None\n    group_by: list[Expr] = field(default_factory=list)\n    having: Expr | None = None\n    order_by: list[OrderByItem] = field(default_factory=list)\n    limit: int | None = None\n    offset: int | None = None\n    ctes: list[CTE] = field(default_factory=list)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.ast.nodes.ColumnRef","title":"<code>orionbelt.ast.nodes.ColumnRef</code>  <code>dataclass</code>","text":"<p>Reference to a column, optionally qualified by table/alias.</p> Source code in <code>src/orionbelt/ast/nodes.py</code> <pre><code>@dataclass(frozen=True)\nclass ColumnRef:\n    \"\"\"Reference to a column, optionally qualified by table/alias.\"\"\"\n\n    name: str\n    table: str | None = None\n</code></pre>"},{"location":"reference/python-api/#orionbelt.ast.nodes.FunctionCall","title":"<code>orionbelt.ast.nodes.FunctionCall</code>  <code>dataclass</code>","text":"<p>SQL function call, e.g. SUM(col), DATE_TRUNC('month', col).</p> Source code in <code>src/orionbelt/ast/nodes.py</code> <pre><code>@dataclass(frozen=True)\nclass FunctionCall:\n    \"\"\"SQL function call, e.g. SUM(col), DATE_TRUNC('month', col).\"\"\"\n\n    name: str\n    args: list[Expr] = field(default_factory=list)\n    distinct: bool = False\n    order_by: list[OrderByItem] = field(default_factory=list)\n    separator: str | None = None\n</code></pre>"},{"location":"reference/python-api/#orionbelt.ast.nodes.BinaryOp","title":"<code>orionbelt.ast.nodes.BinaryOp</code>  <code>dataclass</code>","text":"<p>Binary operation: left op right.</p> Source code in <code>src/orionbelt/ast/nodes.py</code> <pre><code>@dataclass(frozen=True)\nclass BinaryOp:\n    \"\"\"Binary operation: left op right.\"\"\"\n\n    left: Expr\n    op: str  # +, -, *, /, =, &lt;&gt;, AND, OR, LIKE, etc.\n    right: Expr\n</code></pre>"},{"location":"reference/python-api/#orionbelt.ast.nodes.Literal","title":"<code>orionbelt.ast.nodes.Literal</code>  <code>dataclass</code>","text":"<p>A literal value: number, string, boolean, or NULL.</p> Source code in <code>src/orionbelt/ast/nodes.py</code> <pre><code>@dataclass(frozen=True)\nclass Literal:\n    \"\"\"A literal value: number, string, boolean, or NULL.\"\"\"\n\n    value: str | int | float | bool | None\n\n    @classmethod\n    def string(cls, v: str) -&gt; Literal:\n        return cls(value=v)\n\n    @classmethod\n    def number(cls, v: int | float) -&gt; Literal:\n        return cls(value=v)\n\n    @classmethod\n    def null(cls) -&gt; Literal:\n        return cls(value=None)\n\n    @classmethod\n    def boolean(cls, v: bool) -&gt; Literal:\n        return cls(value=v)\n</code></pre>"},{"location":"reference/python-api/#ast-builder","title":"AST Builder","text":""},{"location":"reference/python-api/#orionbelt.ast.builder.QueryBuilder","title":"<code>orionbelt.ast.builder.QueryBuilder</code>","text":"<p>Fluent builder for ergonomic AST construction.</p> Source code in <code>src/orionbelt/ast/builder.py</code> <pre><code>class QueryBuilder:\n    \"\"\"Fluent builder for ergonomic AST construction.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._columns: list[Expr] = []\n        self._from: From | None = None\n        self._joins: list[Join] = []\n        self._where: Expr | None = None\n        self._group_by: list[Expr] = []\n        self._having: Expr | None = None\n        self._order_by: list[OrderByItem] = []\n        self._limit: int | None = None\n        self._offset: int | None = None\n        self._ctes: list[CTE] = []\n\n    def select(self, *columns: Expr) -&gt; Self:\n        self._columns.extend(columns)\n        return self\n\n    def select_aliased(self, expr: Expr, alias: str) -&gt; Self:\n        self._columns.append(AliasedExpr(expr=expr, alias=alias))\n        return self\n\n    def from_(self, table: str, alias: str | None = None) -&gt; Self:\n        self._from = From(source=table, alias=alias)\n        return self\n\n    def from_subquery(self, subquery: Select, alias: str) -&gt; Self:\n        self._from = From(source=subquery, alias=alias)\n        return self\n\n    def join(\n        self,\n        table: str,\n        on: Expr,\n        join_type: JoinType = JoinType.LEFT,\n        alias: str | None = None,\n    ) -&gt; Self:\n        self._joins.append(Join(join_type=join_type, source=table, alias=alias, on=on))\n        return self\n\n    def where(self, condition: Expr) -&gt; Self:\n        if self._where is None:\n            self._where = condition\n        else:\n            self._where = BinaryOp(left=self._where, op=\"AND\", right=condition)\n        return self\n\n    def group_by(self, *exprs: Expr) -&gt; Self:\n        self._group_by.extend(exprs)\n        return self\n\n    def having(self, condition: Expr) -&gt; Self:\n        if self._having is None:\n            self._having = condition\n        else:\n            self._having = BinaryOp(left=self._having, op=\"AND\", right=condition)\n        return self\n\n    def order_by(self, expr: Expr, desc: bool = False) -&gt; Self:\n        self._order_by.append(OrderByItem(expr=expr, desc=desc))\n        return self\n\n    def limit(self, n: int) -&gt; Self:\n        self._limit = n\n        return self\n\n    def offset(self, n: int) -&gt; Self:\n        self._offset = n\n        return self\n\n    def with_cte(self, name: str, query: Select) -&gt; Self:\n        self._ctes.append(CTE(name=name, query=query))\n        return self\n\n    def build(self) -&gt; Select:\n        return Select(\n            columns=self._columns,\n            from_=self._from,\n            joins=self._joins,\n            where=self._where,\n            group_by=self._group_by,\n            having=self._having,\n            order_by=self._order_by,\n            limit=self._limit,\n            offset=self._offset,\n            ctes=self._ctes,\n        )\n</code></pre>"},{"location":"reference/python-api/#api-schemas","title":"API Schemas","text":""},{"location":"reference/python-api/#orionbelt.api.schemas","title":"<code>orionbelt.api.schemas</code>","text":"<p>API request/response Pydantic schemas.</p>"},{"location":"reference/python-api/#orionbelt.api.schemas.SessionCreateRequest","title":"<code>SessionCreateRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for POST /sessions.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class SessionCreateRequest(BaseModel):\n    \"\"\"Request body for POST /sessions.\"\"\"\n\n    metadata: dict[str, str] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.SessionResponse","title":"<code>SessionResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single session info.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class SessionResponse(BaseModel):\n    \"\"\"Single session info.\"\"\"\n\n    session_id: str\n    created_at: datetime\n    last_accessed_at: datetime\n    model_count: int\n    metadata: dict[str, str] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.SessionListResponse","title":"<code>SessionListResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response for GET /sessions.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class SessionListResponse(BaseModel):\n    \"\"\"Response for GET /sessions.\"\"\"\n\n    sessions: list[SessionResponse]\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.ModelLoadRequest","title":"<code>ModelLoadRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for POST /sessions/{session_id}/models.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class ModelLoadRequest(BaseModel):\n    \"\"\"Request body for POST /sessions/{session_id}/models.\"\"\"\n\n    model_yaml: str = Field(description=\"OBML YAML content\", max_length=5_000_000)\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.ModelLoadResponse","title":"<code>ModelLoadResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response for POST /sessions/{session_id}/models.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class ModelLoadResponse(BaseModel):\n    \"\"\"Response for POST /sessions/{session_id}/models.\"\"\"\n\n    model_id: str\n    data_objects: int\n    dimensions: int\n    measures: int\n    metrics: int\n    warnings: list[str] = []\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.ModelSummaryResponse","title":"<code>ModelSummaryResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Short model summary for listing.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class ModelSummaryResponse(BaseModel):\n    \"\"\"Short model summary for listing.\"\"\"\n\n    model_id: str\n    data_objects: int\n    dimensions: int\n    measures: int\n    metrics: int\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.SessionQueryRequest","title":"<code>SessionQueryRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for POST /sessions/{session_id}/query/sql.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class SessionQueryRequest(BaseModel):\n    \"\"\"Request body for POST /sessions/{session_id}/query/sql.\"\"\"\n\n    model_id: str\n    query: QueryObject\n    dialect: str = Field(default=\"postgres\")\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.QueryCompileResponse","title":"<code>QueryCompileResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response body for POST /query/sql.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class QueryCompileResponse(BaseModel):\n    \"\"\"Response body for POST /query/sql.\"\"\"\n\n    sql: str\n    dialect: str\n    resolved: ResolvedInfoResponse\n    warnings: list[str] = []\n    sql_valid: bool = True\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.ValidateRequest","title":"<code>ValidateRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request body for POST /validate.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class ValidateRequest(BaseModel):\n    \"\"\"Request body for POST /validate.\"\"\"\n\n    model_yaml: str = Field(\n        description=\"YAML semantic model content to validate\", max_length=5_000_000\n    )\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.ValidateResponse","title":"<code>ValidateResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response body for POST /validate.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class ValidateResponse(BaseModel):\n    \"\"\"Response body for POST /validate.\"\"\"\n\n    valid: bool\n    errors: list[ErrorDetail] = []\n    warnings: list[ErrorDetail] = []\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.DialectListResponse","title":"<code>DialectListResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response for GET /dialects.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class DialectListResponse(BaseModel):\n    \"\"\"Response for GET /dialects.\"\"\"\n\n    dialects: list[DialectInfo] = []\n</code></pre>"},{"location":"reference/python-api/#orionbelt.api.schemas.HealthResponse","title":"<code>HealthResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Health check response.</p> Source code in <code>src/orionbelt/api/schemas.py</code> <pre><code>class HealthResponse(BaseModel):\n    \"\"\"Health check response.\"\"\"\n\n    status: str = \"ok\"\n    version: str = \"\"\n</code></pre>"},{"location":"reference/python-api/#settings","title":"Settings","text":""},{"location":"reference/python-api/#orionbelt.settings.Settings","title":"<code>orionbelt.settings.Settings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration for OrionBelt servers (API + MCP).</p> <p>Values are read from environment variables and from a <code>.env</code> file in the working directory.  See <code>.env.example</code> for all options.</p> Source code in <code>src/orionbelt/settings.py</code> <pre><code>class Settings(BaseSettings):\n    \"\"\"Configuration for OrionBelt servers (API + MCP).\n\n    Values are read from environment variables and from a ``.env`` file\n    in the working directory.  See ``.env.example`` for all options.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        extra=\"ignore\",\n    )\n\n    # Shared\n    log_level: str = \"INFO\"\n\n    # REST API\n    api_server_host: str = \"localhost\"\n    api_server_port: int = 8000\n    port: int | None = None  # Cloud Run injects PORT; takes precedence over api_server_port\n\n    @property\n    def effective_port(self) -&gt; int:\n        \"\"\"Return the port to listen on (Cloud Run PORT takes precedence).\"\"\"\n        return self.port if self.port is not None else self.api_server_port\n\n    # MCP\n    mcp_transport: Literal[\"stdio\", \"http\", \"sse\"] = \"stdio\"\n    mcp_server_host: str = \"localhost\"\n    mcp_server_port: int = 9000\n\n    # Sessions\n    session_ttl_seconds: int = 1800  # 30 min inactivity\n    session_cleanup_interval: int = 60  # seconds between cleanup sweeps\n    disable_session_list: bool = False  # hide GET /sessions endpoint\n</code></pre>"},{"location":"reference/python-api/#orionbelt.settings.Settings.effective_port","title":"<code>effective_port</code>  <code>property</code>","text":"<p>Return the port to listen on (Cloud Run PORT takes precedence).</p>"}]}